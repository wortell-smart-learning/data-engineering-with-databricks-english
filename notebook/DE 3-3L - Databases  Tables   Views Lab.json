{
	"name": "DE 3-3L - Databases  Tables   Views Lab",
	"properties": {
		"folder": {
			"name": "03 - Relational Entities on Databricks"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "LakehouseTest",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "4",
				"spark.autotune.trackingId": "f80b53a9-82ef-448e-b796-e0333917489d"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_sparksql",
				"display_name": "sql"
			},
			"language_info": {
				"name": "sql"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/8aa90780-bc26-461d-9db6-6df705873871/resourceGroups/domo-demo/providers/Microsoft.Synapse/workspaces/domodemo/bigDataPools/LakehouseTest",
				"name": "LakehouseTest",
				"type": "Spark",
				"endpoint": "https://domodemo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/LakehouseTest",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.2",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"# Databases, Tables, and Views Lab\n",
					"\n",
					"## Learning Objectives\n",
					"By the end of this lab, you should be able to:\n",
					"- Create and explore interactions between various relational entities, including:\n",
					"  - Databases\n",
					"  - Tables (managed and external)\n",
					"  - Views (views, temp views, and global temp views)\n",
					"\n",
					"**Resources**\n",
					"* <a href=\"https://docs.databricks.com/user-guide/tables.html\" target=\"_blank\">Databases and Tables - Databricks Docs</a>\n",
					"* <a href=\"https://docs.databricks.com/user-guide/tables.html#managed-and-unmanaged-tables\" target=\"_blank\">Managed and Unmanaged Tables</a>\n",
					"* <a href=\"https://docs.databricks.com/user-guide/tables.html#create-a-table-using-the-ui\" target=\"_blank\">Creating a Table with the UI</a>\n",
					"* <a href=\"https://docs.databricks.com/user-guide/tables.html#create-a-local-table\" target=\"_blank\">Create a Local Table</a>\n",
					"* <a href=\"https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#saving-to-persistent-tables\" target=\"_blank\">Saving to Persistent Tables</a>"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"### Getting Started\n",
					"\n",
					"Run the following cell to configure variables and datasets for this lesson."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run ../Includes/Classroom-Setup-03.3L"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"## Overview of the Data\n",
					"\n",
					"The data include multiple entries from a selection of weather stations, including average temperatures recorded in either Fahrenheit or Celsius. The schema for the table:\n",
					"\n",
					"|ColumnName  | DataType| Description|\n",
					"|------------|---------|------------|\n",
					"|NAME        |string   | Station name |\n",
					"|STATION     |string   | Unique ID |\n",
					"|LATITUDE    |float    | Latitude |\n",
					"|LONGITUDE   |float    | Longitude |\n",
					"|ELEVATION   |float    | Elevation |\n",
					"|DATE        |date     | YYYY-MM-DD |\n",
					"|UNIT        |string   | Temperature units |\n",
					"|TAVG        |float    | Average temperature |\n",
					"\n",
					"This data is stored in the Parquet format; preview the data with the query below."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"SELECT * \n",
					"FROM parquet.`${DA.paths.datasets}/weather/StationData-parquet`"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"## Create a Database\n",
					"\n",
					"Create a database in the default location using the **`da.db_name`** variable defined in setup script."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"-- TODO\n",
					"\n",
					"<FILL-IN> ${da.db_name}"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"Run the cell below to check your work."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"assert spark.sql(f\"SHOW DATABASES\").filter(f\"databaseName == '{DA.db_name}'\").count() == 1, \"Database not present\""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"## Change to Your New Database\n",
					"\n",
					"**`USE`** your newly created database."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"-- TODO\n",
					"\n",
					"<FILL-IN> ${da.db_name}"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"Run the cell below to check your work."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"assert spark.sql(f\"SHOW CURRENT DATABASE\").first()[\"namespace\"] == DA.db_name, \"Not using the correct database\""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"## Create a Managed Table\n",
					"Use a CTAS statement to create a managed table named **`weather_managed`**."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"-- TODO\n",
					"\n",
					"<FILL-IN>\n",
					"SELECT * \n",
					"FROM parquet.`${DA.paths.datasets}/weather/StationData-parquet`"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"Run the cell below to check your work."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"assert spark.table(\"weather_managed\"), \"Table named `weather_managed` does not exist\"\n",
					"assert spark.table(\"weather_managed\").count() == 2559, \"Incorrect row count\""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"## Create an External Table\n",
					"\n",
					"Recall that an external table differs from a managed table through specification of a location. Create an external table called **`weather_external`** below."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"-- TODO\n",
					"\n",
					"<FILL-IN>\n",
					"LOCATION \"${da.paths.working_dir}/lab/external\"\n",
					"AS SELECT * \n",
					"FROM parquet.`${DA.paths.datasets}/weather/StationData-parquet`"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"Run the cell below to check your work."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"assert spark.table(\"weather_external\"), \"Table named `weather_external` does not exist\"\n",
					"assert spark.table(\"weather_external\").count() == 2559, \"Incorrect row count\""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"## Examine Table Details\n",
					"Use the SQL command **`DESCRIBE EXTENDED table_name`** to examine the two weather tables."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"DESCRIBE EXTENDED weather_managed"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"DESCRIBE EXTENDED weather_external"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"Run the following helper code to extract and compare the table locations."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"def getTableLocation(tableName):\n",
					"    return spark.sql(f\"DESCRIBE DETAIL {tableName}\").select(\"location\").first()[0]"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"managedTablePath = getTableLocation(\"weather_managed\")\n",
					"externalTablePath = getTableLocation(\"weather_external\")\n",
					"\n",
					"print(f\"\"\"The weather_managed table is saved at: \n",
					"\n",
					"    {managedTablePath}\n",
					"\n",
					"The weather_external table is saved at:\n",
					"\n",
					"    {externalTablePath}\"\"\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"List the contents of these directories to confirm that data exists in both locations."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"files = dbutils.fs.ls(managedTablePath)\n",
					"display(files)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"files = dbutils.fs.ls(externalTablePath)\n",
					"display(files)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"### Check Directory Contents after Dropping Database and All Tables\n",
					"The **`CASCADE`** keyword will accomplish this."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"-- TODO\n",
					"\n",
					"<FILL_IN> ${da.db_name}"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"Run the cell below to check your work."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"assert spark.sql(f\"SHOW DATABASES\").filter(f\"databaseName == '{DA.db_name}'\").count() == 0, \"Database present\""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"With the database dropped, the files will have been deleted as well.\n",
					"\n",
					"Uncomment and run the following cell, which will throw a **`FileNotFoundException`** as your confirmation."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"# files = dbutils.fs.ls(managedTablePath)\n",
					"# display(files)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"files = dbutils.fs.ls(externalTablePath)\n",
					"display(files)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"files = dbutils.fs.ls(DA.paths.working_dir)\n",
					"display(files)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"**This highlights the main differences between managed and external tables.** By default, the files associated with managed tables will be stored to this location on the root DBFS storage linked to the workspace, and will be deleted when a table is dropped.\n",
					"\n",
					"Files for external tables will be persisted in the location provided at table creation, preventing users from inadvertently deleting underlying files. **External tables can easily be migrated to other databases or renamed, but these operations with managed tables will require rewriting ALL underlying files.**"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"## Create a Database with a Specified Path\n",
					"\n",
					"Assuming you dropped your database in the last step, you can use the same **`database`** name."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"CREATE DATABASE ${da.db_name} LOCATION '${da.paths.working_dir}/${da.db_name}';\n",
					"USE ${da.db_name};"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"Recreate your **`weather_managed`** table in this new database and print out the location of this table."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"-- TODO\n",
					"\n",
					"<FILL_IN>"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"getTableLocation(\"weather_managed\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"Run the cell below to check your work."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"assert spark.table(\"weather_managed\"), \"Table named `weather_managed` does not exist\"\n",
					"assert spark.table(\"weather_managed\").count() == 2559, \"Incorrect row count\""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"While here we're using the **`working_dir`** directory created on the DBFS root, _any_ object store can be used as the database directory. **Defining database directories for groups of users can greatly reduce the chances of accidental data exfiltration**."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"## Views and their Scoping\n",
					"\n",
					"In this section, use the provided **`AS`** clause to register:\n",
					"- a view named **`celsius`**\n",
					"- a temporary view named **`celsius_temp`**\n",
					"- a global temp view named **`celsius_global`**\n",
					"\n",
					"Start by creating the first view in the code cell below."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"-- TODO\n",
					"\n",
					"<FILL-IN>\n",
					"AS (SELECT *\n",
					"  FROM weather_managed\n",
					"  WHERE UNIT = \"C\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"Run the cell below to check your work."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"assert spark.table(\"celsius\"), \"Table named `celsius` does not exist\"\n",
					"assert spark.sql(f\"SHOW TABLES\").filter(f\"tableName == 'celsius'\").first()[\"isTemporary\"] == False, \"Table is temporary\""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"Now create a temporary view."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"-- TODO\n",
					"\n",
					"<FILL-IN>\n",
					"AS (SELECT *\n",
					"  FROM weather_managed\n",
					"  WHERE UNIT = \"C\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"Run the cell below to check your work."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"assert spark.table(\"celsius_temp\"), \"Table named `celsius_temp` does not exist\"\n",
					"assert spark.sql(f\"SHOW TABLES\").filter(f\"tableName == 'celsius_temp'\").first()[\"isTemporary\"] == True, \"Table is not temporary\""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"Now register a global temp view."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"-- TODO\n",
					"\n",
					"<FILL-IN>\n",
					"AS (SELECT *\n",
					"  FROM weather_managed\n",
					"  WHERE UNIT = \"C\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"Run the cell below to check your work."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"assert spark.table(\"global_temp.celsius_global\"), \"Global temporary view named `celsius_global` does not exist\""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"Views will be displayed alongside tables when listing from the catalog."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"SHOW TABLES"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"Note the following:\n",
					"- The view is associated with the current database. This view will be available to any user that can access this database and will persist between sessions.\n",
					"- The temp view is not associated with any database. The temp view is ephemeral and is only accessible in the current SparkSession.\n",
					"- The global temp view does not appear in our catalog. **Global temp views will always register to the **`global_temp`** database**. The **`global_temp`** database is ephemeral but tied to the lifetime of the cluster; however, it is only accessible by notebooks attached to the same cluster on which it was created."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"SELECT * FROM global_temp.celsius_global"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"While no job was triggered when defining these views, a job is triggered _each time_ a query is executed against the view."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"## Clean Up\n",
					"Drop the database and all tables to clean up your workspace."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"DROP DATABASE ${da.db_name} CASCADE"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"## Synopsis\n",
					"\n",
					"In this lab we:\n",
					"- Created and deleted databases\n",
					"- Explored behavior of managed and external tables\n",
					"- Learned about the scoping of views"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"Run the following cell to delete the tables and files associated with this lesson."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"DA.cleanup()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
					"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
					"<br/>\n",
					"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
				]
			}
		]
	}
}