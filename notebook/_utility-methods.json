{
	"name": "_utility-methods",
	"properties": {
		"folder": {
			"name": "Includes"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "LakehouseTest",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "739bc5da-9536-473e-af1b-7be082fd33dd"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/8aa90780-bc26-461d-9db6-6df705873871/resourceGroups/domo-demo/providers/Microsoft.Synapse/workspaces/domodemo/bigDataPools/LakehouseTest",
				"name": "LakehouseTest",
				"type": "Spark",
				"endpoint": "https://domodemo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/LakehouseTest",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.2",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"%pip install \\\n",
					"git+https://github.com/databricks-academy/dbacademy-gems@e8183eed9481624f25b34436810cf6666b4438c0 \\\n",
					"git+https://github.com/databricks-academy/dbacademy-rest@bc48bdb21810c4fd69d27154bfff2076cc4d02cc \\\n",
					"git+https://github.com/databricks-academy/dbacademy-helper@e0e819d661e6bf972c6fb1872c1ae1a7d2a74b23 \\\n",
					"--quiet --disable-pip-version-check"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"source": [
					"%run /Includes/_remote_files"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"import time\n",
					"from dbacademy_gems import dbgems\n",
					"from dbacademy_helper import DBAcademyHelper, Paths\n",
					"\n",
					"helper_arguments = {\n",
					"    \"course_code\" : \"dewd\",            # The abreviated version of the course\n",
					"    \"course_name\" : \"data-engineering-with-databricks\",      # The full name of the course, hyphenated\n",
					"    \"data_source_name\" : \"data-engineering-with-databricks\", # Should be the same as the course\n",
					"    \"data_source_version\" : \"v02\",     # New courses would start with 01\n",
					"    \"enable_streaming_support\": True,  # This couse uses stream and thus needs checkpoint directories\n",
					"    \"install_min_time\" : \"5 min\",      # The minimum amount of time to install the datasets (e.g. from Oregon)\n",
					"    \"install_max_time\" : \"15 min\",     # The maximum amount of time to install the datasets (e.g. from India)\n",
					"    \"remote_files\": remote_files,      # The enumerated list of files in the datasets\n",
					"}\n",
					"# Start a timer so we can \n",
					"# benchmark execution duration.\n",
					"setup_start = int(time.time())"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def update_cluster_params(self, params: dict, task_indexes: list):\n",
					"\n",
					"    if not self.is_smoke_test():\n",
					"        return params\n",
					"    \n",
					"    for task_index in task_indexes:\n",
					"        # Need to modify the parameters to run run as a smoke-test.\n",
					"        task = params.get(\"tasks\")[task_index]\n",
					"        del task[\"existing_cluster_id\"]\n",
					"\n",
					"        cluster_params =         {\n",
					"            \"num_workers\": \"0\",\n",
					"            \"spark_version\": self.client.clusters().get_current_spark_version(),\n",
					"            \"spark_conf\": {\n",
					"              \"spark.master\": \"local[*]\"\n",
					"            },\n",
					"        }\n",
					"\n",
					"        instance_pool_id = self.client.clusters().get_current_instance_pool_id()\n",
					"        if instance_pool_id is not None: cluster_params[\"instance_pool_id\"] = self.client.clusters().get_current_instance_pool_id()\n",
					"        else:                            cluster_params[\"node_type_id\"] = self.client.clusters().get_current_node_type_id()\n",
					"\n",
					"        task[\"new_cluster\"] = cluster_params\n",
					"        \n",
					"    return params\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def clone_source_table(self, table_name, source_path, source_name=None):\n",
					"    import time\n",
					"    start = int(time.time())\n",
					"\n",
					"    source_name = table_name if source_name is None else source_name\n",
					"    print(f\"Cloning the {table_name} table from {source_path}/{source_name}\", end=\"...\")\n",
					"    \n",
					"    spark.sql(f\"\"\"\n",
					"        CREATE OR REPLACE TABLE {table_name}\n",
					"        SHALLOW CLONE delta.`{source_path}/{source_name}`\n",
					"        \"\"\")\n",
					"\n",
					"    total = spark.read.table(table_name).count()\n",
					"    print(f\"({int(time.time())-start} seconds / {total:,} records)\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"class DltDataFactory:\n",
					"    def __init__(self, stream_path):\n",
					"        self.stream_path = stream_path\n",
					"        self.source = f\"{DA.paths.datasets}/healthcare/tracker/streaming\"\n",
					"        try:\n",
					"            self.curr_mo = 1 + int(max([x[1].split(\".\")[0] for x in dbutils.fs.ls(self.stream_path)]))\n",
					"        except:\n",
					"            self.curr_mo = 1\n",
					"    \n",
					"    def load(self, continuous=False):\n",
					"        if self.curr_mo > 12:\n",
					"            print(\"Data source exhausted\\n\")\n",
					"        elif continuous == True:\n",
					"            while self.curr_mo <= 12:\n",
					"                curr_file = f\"{self.curr_mo:02}.json\"\n",
					"                target_dir = f\"{self.stream_path}/{curr_file}\"\n",
					"                print(f\"Loading the file {curr_file} to the {target_dir}\")\n",
					"                dbutils.fs.cp(f\"{self.source}/{curr_file}\", target_dir)\n",
					"                self.curr_mo += 1\n",
					"        else:\n",
					"            curr_file = f\"{str(self.curr_mo).zfill(2)}.json\"\n",
					"            target_dir = f\"{self.stream_path}/{curr_file}\"\n",
					"            print(f\"Loading the file {curr_file} to the {target_dir}\")\n",
					"\n",
					"            dbutils.fs.cp(f\"{self.source}/{curr_file}\", target_dir)\n",
					"            self.curr_mo += 1"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def setup_completed(self):\n",
					"    print(f\"\\nSetup completed in {int(time.time())-setup_start} seconds\")\n",
					""
				],
				"execution_count": null
			}
		]
	}
}