{
	"name": "DE 4-3 - Creating Delta Tables",
	"properties": {
		"folder": {
			"name": "04 - ETL with Spark SQL"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "LakehouseTest",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "4c7f82af-f1d6-409d-8965-cc59eb648ed5"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_sparksql",
				"display_name": "sql"
			},
			"language_info": {
				"name": "sql"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/8aa90780-bc26-461d-9db6-6df705873871/resourceGroups/domo-demo/providers/Microsoft.Synapse/workspaces/domodemo/bigDataPools/LakehouseTest",
				"name": "LakehouseTest",
				"type": "Spark",
				"endpoint": "https://domodemo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/LakehouseTest",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.2",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"# Creating Delta Tables\n",
					"\n",
					"After extracting data from external data sources, load data into the Lakehouse to ensure that all of the benefits of the Databricks platform can be fully leveraged.\n",
					"\n",
					"While different organizations may have varying policies for how data is initially loaded into Databricks, we typically recommend that early tables represent a mostly raw version of the data, and that validation and enrichment occur in later stages. This pattern ensures that even if data doesn't match expectations with regards to data types or column names, no data will be dropped, meaning that programmatic or manual intervention can still salvage data in a partially corrupted or invalid state.\n",
					"\n",
					"This lesson will focus primarily on the pattern used to create most tables, **`CREATE TABLE _ AS SELECT`** (CTAS) statements.\n",
					"\n",
					"## Learning Objectives\n",
					"By the end of this lesson, you should be able to:\n",
					"- Use CTAS statements to create Delta Lake tables\n",
					"- Create new tables from existing views or tables\n",
					"- Enrich loaded data with additional metadata\n",
					"- Declare table schema with generated columns and descriptive comments\n",
					"- Set advanced options to control data location, quality enforcement, and partitioning"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"## Run Setup\n",
					"\n",
					"The setup script will create the data and declare necessary values for the rest of this notebook to execute."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run /Includes/Classroom-Setup-04.3"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"\n",
					"## Create Table as Select (CTAS)\n",
					"\n",
					"**`CREATE TABLE AS SELECT`** statements create and populate Delta tables using data retrieved from an input query."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"CREATE OR REPLACE TABLE sales AS\n",
					"SELECT * FROM parquet.`${da.paths.datasets}/ecommerce/raw/sales-historical`;\n",
					"\n",
					"DESCRIBE EXTENDED sales;"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"CTAS statements automatically infer schema information from query results and do **not** support manual schema declaration. \n",
					"\n",
					"This means that CTAS statements are useful for external data ingestion from sources with well-defined schema, such as Parquet files and tables.\n",
					"\n",
					"CTAS statements also do not support specifying additional file options.\n",
					"\n",
					"We can see how this would present significant limitations when trying to ingest data from CSV files."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"CREATE OR REPLACE TABLE sales_unparsed AS\n",
					"SELECT * FROM csv.`${da.paths.datasets}/ecommerce/raw/sales-csv`;\n",
					"\n",
					"SELECT * FROM sales_unparsed;"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"To correctly ingest this data to a Delta Lake table, we'll need to use a reference to the files that allows us to specify options.\n",
					"\n",
					"In the previous lesson, we showed doing this by registering an external table. Here, we'll slightly evolve this syntax to specify the options to a temporary view, and then use this temp view as the source for a CTAS statement to successfully register the Delta table."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"CREATE OR REPLACE TEMP VIEW sales_tmp_vw\n",
					"  (order_id LONG, email STRING, transactions_timestamp LONG, total_item_quantity INTEGER, purchase_revenue_in_usd DOUBLE, unique_items INTEGER, items STRING)\n",
					"USING CSV\n",
					"OPTIONS (\n",
					"  path = \"${da.paths.datasets}/ecommerce/raw/sales-csv\",\n",
					"  header = \"true\",\n",
					"  delimiter = \"|\"\n",
					");\n",
					"\n",
					"CREATE TABLE sales_delta AS\n",
					"  SELECT * FROM sales_tmp_vw;\n",
					"  \n",
					"SELECT * FROM sales_delta"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"## Filtering and Renaming Columns from Existing Tables\n",
					"\n",
					"Simple transformations like changing column names or omitting columns from target tables can be easily accomplished during table creation.\n",
					"\n",
					"The following statement creates a new table containing a subset of columns from the **`sales`** table. \n",
					"\n",
					"Here, we'll presume that we're intentionally leaving out information that potentially identifies the user or that provides itemized purchase details. We'll also rename our fields with the assumption that a downstream system has different naming conventions than our source data."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"CREATE OR REPLACE TABLE purchases AS\n",
					"SELECT order_id AS id, transaction_timestamp, purchase_revenue_in_usd AS price\n",
					"FROM sales;\n",
					"\n",
					"SELECT * FROM purchases"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"Note that we could have accomplished this same goal with a view, as shown below."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"CREATE OR REPLACE VIEW purchases_vw AS\n",
					"SELECT order_id AS id, transaction_timestamp, purchase_revenue_in_usd AS price\n",
					"FROM sales;\n",
					"\n",
					"SELECT * FROM purchases_vw"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"## Declare Schema with Generated Columns\n",
					"\n",
					"As noted previously, CTAS statements do not support schema declaration. We note above that the timestamp column appears to be some variant of a Unix timestamp, which may not be the most useful for our analysts to derive insights. This is a situation where generated columns would be beneficial.\n",
					"\n",
					"Generated columns are a special type of column whose values are automatically generated based on a user-specified function over other columns in the Delta table (introduced in DBR 8.3).\n",
					"\n",
					"The code below demonstrates creating a new table while:\n",
					"1. Specifying column names and types\n",
					"1. Adding a <a href=\"https://docs.databricks.com/delta/delta-batch.html#deltausegeneratedcolumns\" target=\"_blank\">generated column</a> to calculate the date\n",
					"1. Providing a descriptive column comment for the generated column"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"CREATE OR REPLACE TABLE purchase_dates (\n",
					"  id STRING, \n",
					"  transaction_timestamp LONG, \n",
					"  price STRING,\n",
					"  date DATE GENERATED ALWAYS AS (\n",
					"    cast(cast(transaction_timestamp/1e6 AS TIMESTAMP) AS DATE))\n",
					"    COMMENT \"generated based on `transactions_timestamp` column\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"\n",
					"Because **`date`** is a generated column, if we write to **`purchase_dates`** without providing values for the **`date`** column, Delta Lake automatically computes them.\n",
					"\n",
					"**NOTE**: The cell below configures a setting to allow for generating columns when using a Delta Lake **`MERGE`** statement. We'll see more on this syntax later in the course."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"SET spark.databricks.delta.schema.autoMerge.enabled=true; \n",
					"\n",
					"MERGE INTO purchase_dates a\n",
					"USING purchases b\n",
					"ON a.id = b.id\n",
					"WHEN NOT MATCHED THEN\n",
					"  INSERT *"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"We can see below that all dates were computed correctly as data was inserted, although neither our source data or insert query specified the values in this field.\n",
					"\n",
					"As with any Delta Lake source, the query automatically reads the most recent snapshot of the table for any query; you never need to run **`REFRESH TABLE`**."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"SELECT * FROM purchase_dates"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"\n",
					"It's important to note that if a field that would otherwise be generated is included in an insert to a table, this insert will fail if the value provided does not exactly match the value that would be derived by the logic used to define the generated column.\n",
					"\n",
					"We can see this error by uncommenting and running the cell below:"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"-- INSERT INTO purchase_dates VALUES\n",
					"-- (1, 600000000, 42.0, \"2020-06-18\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"## Add a Table Constraint\n",
					"\n",
					"The error message above refers to a **`CHECK constraint`**. Generated columns are a special implementation of check constraints.\n",
					"\n",
					"Because Delta Lake enforces schema on write, Databricks can support standard SQL constraint management clauses to ensure the quality and integrity of data added to a table.\n",
					"\n",
					"Databricks currently support two types of constraints:\n",
					"* <a href=\"https://docs.databricks.com/delta/delta-constraints.html#not-null-constraint\" target=\"_blank\">**`NOT NULL`** constraints</a>\n",
					"* <a href=\"https://docs.databricks.com/delta/delta-constraints.html#check-constraint\" target=\"_blank\">**`CHECK`** constraints</a>\n",
					"\n",
					"In both cases, you must ensure that no data violating the constraint is already in the table prior to defining the constraint. Once a constraint has been added to a table, data violating the constraint will result in write failure.\n",
					"\n",
					"Below, we'll add a **`CHECK`** constraint to the **`date`** column of our table. Note that **`CHECK`** constraints look like standard **`WHERE`** clauses you might use to filter a dataset."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"ALTER TABLE purchase_dates ADD CONSTRAINT valid_date CHECK (date > '2020-01-01');"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"Table constraints are shown in the **`TBLPROPERTIES`** field."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"DESCRIBE EXTENDED purchase_dates"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"## Enrich Tables with Additional Options and Metadata\n",
					"\n",
					"So far we've only scratched the surface as far as the options for enriching Delta Lake tables.\n",
					"\n",
					"Below, we show evolving a CTAS statement to include a number of additional configurations and metadata.\n",
					"\n",
					"Our **`SELECT`** clause leverages two built-in Spark SQL commands useful for file ingestion:\n",
					"* **`current_timestamp()`** records the timestamp when the logic is executed\n",
					"* **`input_file_name()`** records the source data file for each record in the table\n",
					"\n",
					"We also include logic to create a new date column derived from timestamp data in the source.\n",
					"\n",
					"The **`CREATE TABLE`** clause contains several options:\n",
					"* A **`COMMENT`** is added to allow for easier discovery of table contents\n",
					"* A **`LOCATION`** is specified, which will result in an external (rather than managed) table\n",
					"* The table is **`PARTITIONED BY`** a date column; this means that the data from each date will exist within its own directory in the target storage location\n",
					"\n",
					"**NOTE**: Partitioning is shown here primarily to demonstrate syntax and impact. Most Delta Lake tables (especially small-to-medium sized data) will not benefit from partitioning. Because partitioning physically separates data files, this approach can result in a small files problem and prevent file compaction and efficient data skipping. The benefits observed in Hive or HDFS do not translate to Delta Lake, and you should consult with an experienced Delta Lake architect before partitioning tables.\n",
					"\n",
					"**As a best practice, you should default to non-partitioned tables for most use cases when working with Delta Lake.**"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"CREATE OR REPLACE TABLE users_pii\n",
					"COMMENT \"Contains PII\"\n",
					"LOCATION \"${da.paths.working_dir}/tmp/users_pii\"\n",
					"PARTITIONED BY (first_touch_date)\n",
					"AS\n",
					"  SELECT *, \n",
					"    cast(cast(user_first_touch_timestamp/1e6 AS TIMESTAMP) AS DATE) first_touch_date, \n",
					"    current_timestamp() updated,\n",
					"    input_file_name() source_file\n",
					"  FROM parquet.`${da.paths.datasets}/ecommerce/raw/users-historical/`;\n",
					"  \n",
					"SELECT * FROM users_pii;"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"The metadata fields added to the table provide useful information to understand when records were inserted and from where. This can be especially helpful if troubleshooting problems in the source data becomes necessary.\n",
					"\n",
					"All of the comments and properties for a given table can be reviewed using **`DESCRIBE TABLE EXTENDED`**.\n",
					"\n",
					"**NOTE**: Delta Lake automatically adds several table properties on table creation."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"DESCRIBE EXTENDED users_pii"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"Listing the location used for the table reveals that the unique values in the partition column **`first_touch_date`** are used to create data directories."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"files = dbutils.fs.ls(f\"{DA.paths.working_dir}/tmp/users_pii\")\n",
					"display(files)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"## Cloning Delta Lake Tables\n",
					"Delta Lake has two options for efficiently copying Delta Lake tables.\n",
					"\n",
					"**`DEEP CLONE`** fully copies data and metadata from a source table to a target. This copy occurs incrementally, so executing this command again can sync changes from the source to the target location."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"CREATE OR REPLACE TABLE purchases_clone\n",
					"DEEP CLONE purchases"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"Because all the data files must be copied over, this can take quite a while for large datasets.\n",
					"\n",
					"If you wish to create a copy of a table quickly to test out applying changes without the risk of modifying the current table, **`SHALLOW CLONE`** can be a good option. Shallow clones just copy the Delta transaction logs, meaning that the data doesn't move."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"CREATE OR REPLACE TABLE purchases_shallow_clone\n",
					"SHALLOW CLONE purchases"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"In either case, data modifications applied to the cloned version of the table will be tracked and stored separately from the source. Cloning is a great way to set up tables for testing SQL code while still in development."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"## Summary\n",
					"\n",
					"In this notebook, we focused primarily on DDL and syntax for creating Delta Lake tables. In the next notebook, we'll explore options for writing updates to tables."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"Run the following cell to delete the tables and files associated with this lesson."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"DA.cleanup()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
					"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
					"<br/>\n",
					"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
				]
			}
		]
	}
}