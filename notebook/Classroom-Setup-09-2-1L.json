{
	"name": "Classroom-Setup-09-2-1L",
	"properties": {
		"folder": {
			"name": "Includes"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "0e07c44a-652c-48b0-a6d4-d16c1c1cfead"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"%run ./_utility-methods"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def get_pipeline_config(self):\n",
					"    path = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None)\n",
					"    path = \"/\".join(path.split(\"/\")[:-1]) + \"/DE 9.2.3L - DLT Job\"\n",
					"    \n",
					"    pipeline_name = f\"DLT-Job-Lab-92-{DA.username}\"\n",
					"    \n",
					"    return pipeline_name, path\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def print_pipeline_config(self):\n",
					"    pipeline_name, notebook = self.get_pipeline_config()\n",
					"\n",
					"    displayHTML(f\"\"\"<table style=\"width:100%\">\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Pipeline Name:</td>\n",
					"        <td><input type=\"text\" value=\"{pipeline_name}\" style=\"width:100%\"></td></tr>\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Target:</td>\n",
					"        <td><input type=\"text\" value=\"{DA.db_name}\" style=\"width:100%\"></td></tr>\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Storage Location:</td>\n",
					"        <td><input type=\"text\" value=\"{DA.paths.storage_location}\" style=\"width:100%\"></td></tr>\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Notebook Path:</td>\n",
					"        <td><input type=\"text\" value=\"{notebook}\" style=\"width:100%\"></td></tr>\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Datsets Path:</td>\n",
					"        <td><input type=\"text\" value=\"{DA.paths.datasets}\" style=\"width:100%\"></td></tr>\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Source:</td>\n",
					"        <td><input type=\"text\" value=\"{DA.paths.stream_path}\" style=\"width:100%\"></td></tr>\n",
					"    \n",
					"    </table>\"\"\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def create_pipeline(self):\n",
					"    \"Provided by DBAcademy, this function creates the prescribed pipline\"\n",
					"    \n",
					"    pipeline_name, path = self.get_pipeline_config()\n",
					"\n",
					"    # We need to delete the existing pipline so that we can apply updates\n",
					"    # because some attributes are not mutable after creation.\n",
					"    self.client.pipelines().delete_by_name(pipeline_name)\n",
					"    \n",
					"    response = self.client.pipelines().create(\n",
					"        name = pipeline_name, \n",
					"        storage = DA.paths.storage_location, \n",
					"        target = DA.db_name, \n",
					"        notebooks = [path],\n",
					"        configuration = {\n",
					"            \"spark.master\": \"local[*]\",\n",
					"            \"datasets_path\": DA.paths.datasets,\n",
					"            \"source\": DA.paths.stream_path,\n",
					"        },\n",
					"        clusters=[{ \"label\": \"default\", \"num_workers\": 0 }])\n",
					"    \n",
					"    pipeline_id = response.get(\"pipeline_id\")\n",
					"    print(f\"Created pipline \\\"{pipeline_name}\\\" (#{pipeline_id})\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def validate_pipeline_config(self):\n",
					"    \"Provided by DBAcademy, this function validates the configuration of the pipeline\"\n",
					"    import json\n",
					"    \n",
					"    pipeline_name, path = self.get_pipeline_config()\n",
					"\n",
					"    pipeline = self.client.pipelines().get_by_name(pipeline_name)\n",
					"    assert pipeline is not None, f\"The pipline named \\\"{pipeline_name}\\\" doesn't exist. Double check the spelling.\"\n",
					"\n",
					"    spec = pipeline.get(\"spec\")\n",
					"    \n",
					"    storage = spec.get(\"storage\")\n",
					"    assert storage == DA.paths.storage_location, f\"Invalid storage location. Found \\\"{storage}\\\", expected \\\"{DA.paths.storage_location}\\\" \"\n",
					"    \n",
					"    target = spec.get(\"target\")\n",
					"    assert target == DA.db_name, f\"Invalid target. Found \\\"{target}\\\", expected \\\"{DA.db_name}\\\" \"\n",
					"    \n",
					"    libraries = spec.get(\"libraries\")\n",
					"    assert libraries is None or len(libraries) > 0, f\"The notebook path must be specified.\"\n",
					"    assert len(libraries) == 1, f\"More than one library (e.g. notebook) was specified.\"\n",
					"    first_library = libraries[0]\n",
					"    assert first_library.get(\"notebook\") is not None, f\"Incorrect library configuration - expected a notebook.\"\n",
					"    first_library_path = first_library.get(\"notebook\").get(\"path\")\n",
					"    assert first_library_path == path, f\"Invalid notebook path. Found \\\"{first_library_path}\\\", expected \\\"{path}\\\" \"\n",
					"\n",
					"    configuration = spec.get(\"configuration\")\n",
					"    assert configuration is not None, f\"The three configuration parameters were not specified.\"\n",
					"    datasets_path = configuration.get(\"datasets_path\")\n",
					"    assert datasets_path == DA.paths.datasets, f\"Invalid \\\"datasets_path\\\" value. Found \\\"{datasets_path}\\\", expected \\\"{DA.paths.datasets}\\\".\"\n",
					"    spark_master = configuration.get(\"spark.master\")\n",
					"    assert spark_master == f\"local[*]\", f\"Invalid \\\"spark.master\\\" value. Expected \\\"local[*]\\\", found \\\"{spark_master}\\\".\"\n",
					"    stream_source = configuration.get(\"source\")\n",
					"    assert stream_source == DA.paths.stream_path, f\"Invalid \\\"source\\\" value. Expected \\\"{DA.paths.stream_path}\\\", found \\\"{stream_source}\\\".\"\n",
					"    \n",
					"    cluster = spec.get(\"clusters\")[0]\n",
					"    autoscale = cluster.get(\"autoscale\")\n",
					"    assert autoscale is None, f\"Autoscaling should be disabled.\"\n",
					"    \n",
					"    num_workers = cluster.get(\"num_workers\")\n",
					"    assert num_workers == 0, f\"Expected the number of workers to be 0, found {num_workers}.\"\n",
					"\n",
					"    development = spec.get(\"development\")\n",
					"    assert development == True, f\"The pipline mode should be set to \\\"Development\\\".\"\n",
					"    \n",
					"    channel = spec.get(\"channel\")\n",
					"    assert channel is None or channel == \"CURRENT\", f\"Expected the channel to be \\\"Current\\\" but found \\\"{channel}\\\".\"\n",
					"    \n",
					"    photon = spec.get(\"photon\")\n",
					"    assert photon == True, f\"Expected Photon to be enabled.\"\n",
					"    \n",
					"    continuous = spec.get(\"continuous\")\n",
					"    assert continuous == False, f\"Expected the Pipeline mode to be \\\"Triggered\\\", found \\\"Continuous\\\".\"\n",
					"\n",
					"    policy = self.client.cluster_policies.get_by_name(\"Student's DLT-Only Policy\")\n",
					"    if policy is not None:\n",
					"        cluster = { \n",
					"            \"num_workers\": 0,\n",
					"            \"label\": \"default\", \n",
					"            \"policy_id\": policy.get(\"policy_id\")\n",
					"        }\n",
					"        self.client.pipelines.create_or_update(name = pipeline_name,\n",
					"                                               storage = DA.paths.storage_location,\n",
					"                                               target = DA.db_name,\n",
					"                                               notebooks = [path],\n",
					"                                               configuration = {\n",
					"                                                   \"spark.master\": \"local[*]\",\n",
					"                                                   \"datasets_path\": DA.paths.datasets,\n",
					"                                                   \"source\": DA.paths.stream_path,\n",
					"                                               },\n",
					"                                               clusters=[cluster])\n",
					"    print(\"All tests passed!\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def get_job_config(self):\n",
					"    \n",
					"    job_name = f\"Jobs-Lab-92-{DA.username}\"\n",
					"    \n",
					"    notebook_1 = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None)\n",
					"    notebook_1 = \"/\".join(notebook_1.split(\"/\")[:-1]) + \"/DE 9.2.2L - Batch Job\"\n",
					"\n",
					"    notebook_2 = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None)\n",
					"    notebook_2 = \"/\".join(notebook_2.split(\"/\")[:-1]) + \"/DE 9.2.4L - Query Results Job\"\n",
					"\n",
					"    return job_name, notebook_1, notebook_2\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def print_job_config(self):\n",
					"    \n",
					"    job_name, notebook_1, notebook_2 = self.get_job_config()\n",
					"    \n",
					"    displayHTML(f\"\"\"<table style=\"width:100%\">\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Job Name:</td>\n",
					"        <td><input type=\"text\" value=\"{job_name}\" style=\"width:100%\"></td></tr>\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Batch Notebook Path:</td>\n",
					"        <td><input type=\"text\" value=\"{notebook_1}\" style=\"width:100%\"></td></tr>\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Query Notebook Path:</td>\n",
					"        <td><input type=\"text\" value=\"{notebook_2}\" style=\"width:100%\"></td></tr>\n",
					"        \n",
					"    </table>\"\"\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def create_job(self):\n",
					"    \"Provided by DBAcademy, this function creates the prescribed job\"\n",
					"    \n",
					"    pipeline_name, path = self.get_pipeline_config()\n",
					"    job_name, notebook_1, notebook_2 = self.get_job_config()\n",
					"\n",
					"    self.client.jobs.delete_by_name(job_name, success_only=False)\n",
					"    cluster_id = dbgems.get_tags().get(\"clusterId\")\n",
					"    \n",
					"    pipeline = self.client.pipelines().get_by_name(pipeline_name)\n",
					"    pipeline_id = pipeline.get(\"pipeline_id\")\n",
					"    \n",
					"    params = {\n",
					"        \"name\": job_name,\n",
					"        \"tags\": {\n",
					"            \"dbacademy.course\": self.course_name,\n",
					"            \"dbacademy.source\": self.course_name\n",
					"        },\n",
					"        \"email_notifications\": {},\n",
					"        \"timeout_seconds\": 7200,\n",
					"        \"max_concurrent_runs\": 1,\n",
					"        \"format\": \"MULTI_TASK\",\n",
					"        \"tasks\": [\n",
					"            {\n",
					"                \"task_key\": \"Batch-Job\",\n",
					"                \"libraries\": [],\n",
					"                \"notebook_task\": {\n",
					"                    \"notebook_path\": notebook_1,\n",
					"                    \"base_parameters\": []\n",
					"                },\n",
					"                \"existing_cluster_id\": cluster_id\n",
					"            },\n",
					"            {\n",
					"                \"task_key\": \"DLT\",\n",
					"                \"depends_on\": [ { \"task_key\": \"Batch-Job\" } ],\n",
					"                \"pipeline_task\": {\n",
					"                    \"pipeline_id\": pipeline_id\n",
					"                },\n",
					"            },\n",
					"            {\n",
					"                \"task_key\": \"Query-Results\",\n",
					"                \"depends_on\": [ { \"task_key\": \"DLT\" } ],\n",
					"                \"libraries\": [],\n",
					"                \"notebook_task\": {\n",
					"                    \"notebook_path\": notebook_2,\n",
					"                    \"base_parameters\": []\n",
					"                },\n",
					"                \"existing_cluster_id\": cluster_id\n",
					"            },\n",
					"        ],\n",
					"    }\n",
					"    params = self.update_cluster_params(params, [0,2])\n",
					"    \n",
					"    #import json\n",
					"    #print(json.dumps(params, indent=4))\n",
					"    \n",
					"    create_response = self.client.jobs().create(params)\n",
					"    job_id = create_response.get(\"job_id\")\n",
					"    \n",
					"    print(f\"Created job \\\"{job_name}\\\" (#{job_id})\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def validate_job_config(self):\n",
					"    \"Provided by DBAcademy, this function validates the configuration of the job\"\n",
					"    import json\n",
					"    \n",
					"    pipeline_name, job_path = self.get_pipeline_config()\n",
					"    job_name, notebook_1, notebook_2 = self.get_job_config()\n",
					"\n",
					"    job = self.client.jobs.get_by_name(job_name)\n",
					"    assert job is not None, f\"The job named \\\"{job_name}\\\" doesn't exist. Double check the spelling.\"\n",
					"    \n",
					"    settings = job.get(\"settings\")\n",
					"    assert settings.get(\"format\") == \"MULTI_TASK\", f\"Expected three tasks, found 1.\"\n",
					"\n",
					"    tasks = settings.get(\"tasks\", [])\n",
					"    assert len(tasks) == 3, f\"Expected three tasks, found {len(tasks)}.\"\n",
					"\n",
					"    \n",
					"    \n",
					"    # Batch-Job Task\n",
					"    batch_task = tasks[0]\n",
					"    task_name = batch_task.get(\"task_key\", None)\n",
					"    assert task_name == \"Batch-Job\", f\"Expected the first task to have the name \\\"Batch-Job\\\", found \\\"{task_name}\\\"\"\n",
					"    \n",
					"    notebook_path = batch_task.get(\"notebook_task\", {}).get(\"notebook_path\")\n",
					"    assert notebook_path == notebook_1, f\"Invalid Notebook Path for the first task. Found \\\"{notebook_path}\\\", expected \\\"{notebook_1}\\\" \"\n",
					"\n",
					"    if not self.is_smoke_test():\n",
					"        # Don't check the actual_cluster_id when running as a smoke test\n",
					"        \n",
					"        actual_cluster_id = batch_task.get(\"existing_cluster_id\", None)\n",
					"        assert actual_cluster_id is not None, f\"The first task is not configured to use the current All-Purpose cluster\"\n",
					"\n",
					"        expected_cluster_id = dbgems.get_tags().get(\"clusterId\")\n",
					"        if expected_cluster_id != actual_cluster_id:\n",
					"            actual_cluster = self.client.clusters.get(actual_cluster_id).get(\"cluster_name\")\n",
					"            expected_cluster = self.client.clusters.get(expected_cluster_id).get(\"cluster_name\")\n",
					"            assert actual_cluster_id == expected_cluster_id, f\"The first task is not configured to use the current All-Purpose cluster, expected \\\"{expected_cluster}\\\", found \\\"{actual_cluster}\\\"\"\n",
					"\n",
					"    \n",
					"    \n",
					"    # DLT\n",
					"    dlt_task = tasks[1]\n",
					"    task_name = dlt_task.get(\"task_key\", None)\n",
					"    assert task_name == \"DLT\", f\"Expected the second task to have the name \\\"DLT\\\", found \\\"{task_name}\\\"\"\n",
					"\n",
					"    actual_pipeline_id = dlt_task.get(\"pipeline_task\", {}).get(\"pipeline_id\", None)\n",
					"    assert actual_pipeline_id is not None, f\"The second task is not configured to use a Delta Live Tables pipeline\"\n",
					"    \n",
					"    expected_pipeline = self.client.pipelines().get_by_name(pipeline_name)\n",
					"    actual_pipeline = self.client.pipelines().get_by_id(actual_pipeline_id)\n",
					"    actual_name = actual_pipeline.get(\"spec\").get(\"name\", \"Oops\")\n",
					"    assert actual_pipeline_id == expected_pipeline.get(\"pipeline_id\"), f\"The second task is not configured to use the correct pipeline, expected \\\"{pipeline_name}\\\", found \\\"{actual_name}\\\"\"\n",
					"    \n",
					"    depends_on = dlt_task.get(\"depends_on\", [])\n",
					"    assert len(depends_on) > 0, f\"The \\\"DLT\\\" task does not depend on the \\\"Batch-Job\\\" task\"\n",
					"    assert len(depends_on) == 1, f\"The \\\"DLT\\\" task depends on more than just the \\\"Batch-Job\\\" task\"\n",
					"    depends_task_key = depends_on[0].get(\"task_key\")\n",
					"    assert depends_task_key == \"Batch-Job\", f\"The \\\"DLT\\\" task doesn't depend on the \\\"Batch-Job\\\" task, found \\\"{depends_task_key}\\\".\"\n",
					"    \n",
					"    \n",
					"    \n",
					"    # Query Task\n",
					"    query_task = tasks[2] \n",
					"    task_name = query_task.get(\"task_key\", None)\n",
					"    assert task_name == \"Query-Results\", f\"Expected the third task to have the name \\\"Query-Results\\\", found \\\"{task_name}\\\"\"\n",
					"    \n",
					"    notebook_path = query_task.get(\"notebook_task\", {}).get(\"notebook_path\")\n",
					"    assert notebook_path == notebook_2, f\"Invalid Notebook Path for the thrid task. Found \\\"{notebook_path}\\\", expected \\\"{notebook_2}\\\" \"\n",
					"    \n",
					"    depends_on = query_task.get(\"depends_on\", [])\n",
					"    assert len(depends_on) > 0, f\"The \\\"Query-Results\\\" task does not depend on the \\\"DLT\\\" task\"\n",
					"    assert len(depends_on) == 1, f\"The \\\"Query-Results\\\" task depends on more than just the \\\"DLT\\\" task\"\n",
					"    depends_task_key = depends_on[0].get(\"task_key\")\n",
					"    assert depends_task_key == \"DLT\", f\"The \\\"Query-Results\\\" task doesn't depend on the \\\"DLT\\\" task, found \\\"{depends_task_key}\\\".\"\n",
					"\n",
					"    if not self.is_smoke_test():\n",
					"        # Don't check the actual_cluster_id when running as a smoke test\n",
					"        \n",
					"        actual_cluster_id = query_task.get(\"existing_cluster_id\", None)\n",
					"        assert actual_cluster_id is not None, f\"The second task is not configured to use the current All-Purpose cluster\"\n",
					"\n",
					"        expected_cluster_id = dbgems.get_tags().get(\"clusterId\")\n",
					"        if expected_cluster_id != actual_cluster_id:\n",
					"            actual_cluster = self.client.clusters.get(actual_cluster_id).get(\"cluster_name\")\n",
					"            expected_cluster = self.client.clusters.get(expected_cluster_id).get(\"cluster_name\")\n",
					"            assert actual_cluster_id == expected_cluster_id, f\"The second task is not configured to use the current All-Purpose cluster, expected \\\"{expected_cluster}\\\", found \\\"{actual_cluster}\\\"\"\n",
					"\n",
					"    print(\"All tests passed!\")\n",
					"    "
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def start_job(self):\n",
					"    job_name, notebook_1, notebook_2 = self.get_job_config()\n",
					"    job_id = self.client.jobs.get_by_name(job_name).get(\"job_id\")\n",
					"    run_id = self.client.jobs.run_now(job_id).get(\"run_id\")\n",
					"    print(f\"Started job #{job_id}, run #{run_id}\")\n",
					"\n",
					"    self.client.runs.wait_for(run_id)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"DA = DBAcademyHelper(lesson=\"jobs_lab_92\", **helper_arguments)\n",
					"DA.reset_environment()\n",
					"DA.init(install_datasets=True, create_db=True)\n",
					"\n",
					"DA.paths.stream_path = f\"{DA.paths.working_dir}/stream\"\n",
					"DA.paths.storage_location = f\"{DA.paths.working_dir}/storage\"\n",
					"\n",
					"DA.data_factory = DltDataFactory(DA.paths.stream_path)\n",
					"\n",
					"DA.conclude_setup()\n",
					""
				],
				"execution_count": null
			}
		]
	}
}