{
	"name": "DE 4-4 - Writing to Tables",
	"properties": {
		"folder": {
			"name": "04 - ETL with Spark SQL"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "LakehouseTest",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "0342991a-9480-47bb-88db-5b1a5157cb0d"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_sparksql",
				"display_name": "sql"
			},
			"language_info": {
				"name": "sql"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/8aa90780-bc26-461d-9db6-6df705873871/resourceGroups/domo-demo/providers/Microsoft.Synapse/workspaces/domodemo/bigDataPools/LakehouseTest",
				"name": "LakehouseTest",
				"type": "Spark",
				"endpoint": "https://domodemo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/LakehouseTest",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.2",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"# Writing to Delta Tables\n",
					"Delta Lake tables provide ACID compliant updates to tables backed by data files in cloud object storage.\n",
					"\n",
					"In this notebook, we'll explore SQL syntax to process updates with Delta Lake. While many operations are standard SQL, slight variations exist to accommodate Spark and Delta Lake execution.\n",
					"\n",
					"## Learning Objectives\n",
					"By the end of this lesson, you should be able to:\n",
					"- Overwrite data tables using **`INSERT OVERWRITE`**\n",
					"- Append to a table using **`INSERT INTO`**\n",
					"- Append, update, and delete from a table using **`MERGE INTO`**\n",
					"- Ingest data incrementally into tables using **`COPY INTO`**"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"## Run Setup\n",
					"\n",
					"The setup script will create the data and declare necessary values for the rest of this notebook to execute."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run ../Includes/Classroom-Setup-04.4"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"## Complete Overwrites\n",
					"\n",
					"We can use overwrites to atomically replace all of the data in a table. There are multiple benefits to overwriting tables instead of deleting and recreating tables:\n",
					"- Overwriting a table is much faster because it doesn’t need to list the directory recursively or delete any files.\n",
					"- The old version of the table still exists; can easily retrieve the old data using Time Travel.\n",
					"- It’s an atomic operation. Concurrent queries can still read the table while you are deleting the table.\n",
					"- Due to ACID transaction guarantees, if overwriting the table fails, the table will be in its previous state.\n",
					"\n",
					"Spark SQL provides two easy methods to accomplish complete overwrites.\n",
					"\n",
					"Some students may have noticed previous lesson on CTAS statements actually used CRAS statements (to avoid potential errors if a cell was run multiple times).\n",
					"\n",
					"**`CREATE OR REPLACE TABLE`** (CRAS) statements fully replace the contents of a table each time they execute."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"CREATE OR REPLACE TABLE events AS\n",
					"SELECT * FROM parquet.`${da.paths.datasets}/ecommerce/raw/events-historical`"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"Reviewing the table history shows a previous version of this table was replaced."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"DESCRIBE HISTORY events"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"**`INSERT OVERWRITE`** provides a nearly identical outcome as above: data in the target table will be replaced by data from the query. \n",
					"\n",
					"**`INSERT OVERWRITE`**:\n",
					"\n",
					"- Can only overwrite an existing table, not create a new one like our CRAS statement\n",
					"- Can overwrite only with new records that match the current table schema -- and thus can be a \"safer\" technique for overwriting an existing table without disrupting downstream consumers\n",
					"- Can overwrite individual partitions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"INSERT OVERWRITE sales\n",
					"SELECT * FROM parquet.`${da.paths.datasets}/ecommerce/raw/sales-historical/`"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"Note that different metrics are displayed than a CRAS statement; the table history also records the operation differently."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"DESCRIBE HISTORY sales"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"A primary difference here has to do with how Delta Lake enforces schema on write.\n",
					"\n",
					"Whereas a CRAS statement will allow us to completely redefine the contents of our target table, **`INSERT OVERWRITE`** will fail if we try to change our schema (unless we provide optional settings). \n",
					"\n",
					"Uncomment and run the cell below to generate an expected error message."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"-- INSERT OVERWRITE sales\n",
					"-- SELECT *, current_timestamp() FROM parquet.`${da.paths.datasets}/ecommerce/raw/sales-historical`"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"\n",
					"## Append Rows\n",
					"\n",
					"We can use **`INSERT INTO`** to atomically append new rows to an existing Delta table. This allows for incremental updates to existing tables, which is much more efficient than overwriting each time.\n",
					"\n",
					"Append new sale records to the **`sales`** table using **`INSERT INTO`**."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"INSERT INTO sales\n",
					"SELECT * FROM parquet.`${da.paths.datasets}/ecommerce/raw/sales-30m`"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"\n",
					"Note that **`INSERT INTO`** does not have any built-in guarantees to prevent inserting the same records multiple times. Re-executing the above cell would write the same records to the target table, resulting in duplicate records."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"\n",
					"## Merge Updates\n",
					"\n",
					"You can upsert data from a source table, view, or DataFrame into a target Delta table using the **`MERGE`** SQL operation. Delta Lake supports inserts, updates and deletes in **`MERGE`**, and supports extended syntax beyond the SQL standards to facilitate advanced use cases.\n",
					"\n",
					"<strong><code>\n",
					"MERGE INTO target a<br/>\n",
					"USING source b<br/>\n",
					"ON {merge_condition}<br/>\n",
					"WHEN MATCHED THEN {matched_action}<br/>\n",
					"WHEN NOT MATCHED THEN {not_matched_action}<br/>\n",
					"</code></strong>\n",
					"\n",
					"We will use the **`MERGE`** operation to update historic users data with updated emails and new users."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"CREATE OR REPLACE TEMP VIEW users_update AS \n",
					"SELECT *, current_timestamp() AS updated \n",
					"FROM parquet.`${da.paths.datasets}/ecommerce/raw/users-30m`"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"The main benefits of **`MERGE`**:\n",
					"* updates, inserts, and deletes are completed as a single transaction\n",
					"* multiple conditionals can be added in addition to matching fields\n",
					"* provides extensive options for implementing custom logic\n",
					"\n",
					"Below, we'll only update records if the current row has a **`NULL`** email and the new row does not. \n",
					"\n",
					"All unmatched records from the new batch will be inserted."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"MERGE INTO users a\n",
					"USING users_update b\n",
					"ON a.user_id = b.user_id\n",
					"WHEN MATCHED AND a.email IS NULL AND b.email IS NOT NULL THEN\n",
					"  UPDATE SET email = b.email, updated = b.updated\n",
					"WHEN NOT MATCHED THEN INSERT *"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"Note that we explicitly specify the behavior of this function for both the **`MATCHED`** and **`NOT MATCHED`** conditions; the example demonstrated here is just an example of logic that can be applied, rather than indicative of all **`MERGE`** behavior."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"## Insert-Only Merge for Deduplication\n",
					"\n",
					"A common ETL use case is to collect logs or other every-appending datasets into a Delta table through a series of append operations. \n",
					"\n",
					"Many source systems can generate duplicate records. With merge, you can avoid inserting the duplicate records by performing an insert-only merge.\n",
					"\n",
					"This optimized command uses the same **`MERGE`** syntax but only provided a **`WHEN NOT MATCHED`** clause.\n",
					"\n",
					"Below, we use this to confirm that records with the same **`user_id`** and **`event_timestamp`** aren't already in the **`events`** table."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"MERGE INTO events a\n",
					"USING events_update b\n",
					"ON a.user_id = b.user_id AND a.event_timestamp = b.event_timestamp\n",
					"WHEN NOT MATCHED AND b.traffic_source = 'email' THEN \n",
					"  INSERT *"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"## Load Incrementally\n",
					"\n",
					"**`COPY INTO`** provides SQL engineers an idempotent option to incrementally ingest data from external systems.\n",
					"\n",
					"Note that this operation does have some expectations:\n",
					"- Data schema should be consistent\n",
					"- Duplicate records should try to be excluded or handled downstream\n",
					"\n",
					"This operation is potentially much cheaper than full table scans for data that grows predictably.\n",
					"\n",
					"While here we'll show simple execution on a static directory, the real value is in multiple executions over time picking up new files in the source automatically."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"COPY INTO sales\n",
					"FROM \"${da.paths.datasets}/ecommerce/raw/sales-30m\"\n",
					"FILEFORMAT = PARQUET"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"Run the following cell to delete the tables and files associated with this lesson."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"DA.cleanup()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
					"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
					"<br/>\n",
					"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
				]
			}
		]
	}
}