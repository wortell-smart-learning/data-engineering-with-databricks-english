{
	"name": "DE 1-1 - Create and Manage Interactive Clusters",
	"properties": {
		"folder": {
			"name": "01 - Synapse Workspace and Services"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "LakehouseTest",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "daf6dac8-d42b-42ba-808c-734d31ea0e0a"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/8aa90780-bc26-461d-9db6-6df705873871/resourceGroups/domo-demo/providers/Microsoft.Synapse/workspaces/domodemo/bigDataPools/LakehouseTest",
				"name": "LakehouseTest",
				"type": "Spark",
				"endpoint": "https://domodemo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/LakehouseTest",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.2",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"# Create and Manage Interactive Clusters\n",
					"\n",
					"A Spark cluster is a set of computation resources and configurations on which you run data engineering, data science, and data analytics workloads, such as production ETL pipelines, streaming analytics, ad-hoc analytics, and machine learning. You run these workloads as a set of commands in a notebook or as an automated job. \n",
					"\n",
					"Unlike Databricks, Synapse Spark has no distinction between all-purpose clusters and job clusters. \n",
					"\n",
					"## Learning Objectives\n",
					"By the end of this lesson, you should be able to:\n",
					"* Use the Clusters UI to configure and deploy a cluster\n",
					"* Edit, terminate, restart, and delete clusters"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Create a Spark Pool\n",
					"\n",
					"In Synapse, a cluster of computing resources is called a \"Pool\". Like the SQL Pool for Data Warehousing activities, we have the Spark Pool for scalable Big Data workloads such as the Data Lakehouse.\n",
					"Depending on the workspace in which you're currently working, you may or may not have pool creation privileges. \n",
					"\n",
					"Instructions in this section assume that you **do** have pool creation privileges, and that you need to deploy a new pool to execute the lessons in this course.\n",
					"\n",
					"**NOTE**: Check with your instructor or a platform admin to confirm whether or not you should create a new pool or connect to a pool that has already been deployed. Pool policies may impact your options for pool configuration. \n",
					"\n",
					"Steps:\n",
					"1. On the left of the screen, go to the **manage** tab\n",
					"1. Click **Apache Spark Pools**\n",
					"1. Click the **New** button\n",
					"1. For the **Apache Spark pool name**, use your name so that you can find it easily and the instructor can easily identify it if you have problems\n",
					"1. Set the **Node size family** to **Memory Optimized**\n",
					"1. Set the **Node size** to **Small**\n",
					"1. Set **Autoscale** to **disabled**\n",
					"1. Slide the **number of nodes** down to **3**\n",
					"1. Leave **Dynamically allocate executors** to **Disabled**\n",
					"1. Click **Review + create** to review all settings\n",
					"1. Click **Create** to start creating the cluster\n",
					"\n",
					"**NOTE:** Clusters can take several minutes to deploy. Once you have finished deploying a cluster, feel free to continue to explore the cluster creation UI."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Manage Pools\n",
					"\n",
					"Once the pool is created, go back to the **Apache Spark Pools** page inside the **manage** tab to view the cluster.\n",
					"\n",
					"Select a cluster to review the current configuration. \n",
					"\n",
					"Click the **Ellipsis** button. Note that you can modify several settings in an existing pool:\n",
					"\n",
					"* Scale settings (number of nodes, autoscale, dynamic allocation of executors)\n",
					"* Packages\n",
					"* Pause settings (whether the pool should be automatically paused or not)\n",
					"\n",
					"Changing most settings will require running pools to be restarted.\n",
					"\n",
					"**NOTE**: We'll be using our pool in the following lesson. Restarting, terminating, or deleting your pool may put you behind as you wait for new resources to be deployed."
				]
			}
		]
	}
}