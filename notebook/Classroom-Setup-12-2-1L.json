{
	"name": "Classroom-Setup-12-2-1L",
	"properties": {
		"folder": {
			"name": "Includes"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "e99603de-46d1-440f-8dbc-34c07ec19ef7"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"%run /Includes/_utility-methods"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def print_sql(self, rows, sql):\n",
					"    displayHTML(f\"\"\"<body><textarea style=\"width:100%\" rows={rows}> \\n{sql.strip()}</textarea></body>\"\"\")\n",
					"    "
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def generate_daily_patient_avg(self):\n",
					"    sql = f\"SELECT * FROM {DA.db_name}.daily_patient_avg\"\n",
					"    self.print_sql(3, sql)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def generate_visualization_query(self):\n",
					"    sql = f\"\"\"\n",
					"SELECT flow_name, timestamp, int(details:flow_progress:metrics:num_output_rows) num_output_rows\n",
					"FROM {DA.db_name}.dlt_metrics\n",
					"ORDER BY timestamp DESC;\"\"\"\n",
					"    \n",
					"    self.print_sql(5, sql)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"generate_register_dlt_event_metrics_sql_string = \"\"\n",
					"\n",
					"@DBAcademyHelper.monkey_patch\n",
					"def generate_register_dlt_event_metrics_sql(self):\n",
					"    global generate_register_dlt_event_metrics_sql_string\n",
					"    \n",
					"    generate_register_dlt_event_metrics_sql_string = f\"\"\"\n",
					"CREATE TABLE IF NOT EXISTS {DA.db_name}.dlt_events\n",
					"LOCATION '{DA.paths.working_dir}/storage/system/events';\n",
					"\n",
					"CREATE VIEW IF NOT EXISTS {DA.db_name}.dlt_success AS\n",
					"SELECT * FROM {DA.db_name}.dlt_events\n",
					"WHERE details:flow_progress:metrics IS NOT NULL;\n",
					"\n",
					"CREATE VIEW IF NOT EXISTS {DA.db_name}.dlt_metrics AS\n",
					"SELECT timestamp, origin.flow_name, details \n",
					"FROM {DA.db_name}.dlt_success\n",
					"ORDER BY timestamp DESC;\"\"\".strip()\n",
					"    \n",
					"    self.print_sql(13, generate_register_dlt_event_metrics_sql_string)\n",
					"    "
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def get_pipeline_config(self):\n",
					"    notebook = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None)\n",
					"    notebook = \"/\".join(notebook.split(\"/\")[:-1]) + \"/DE 12.2.2L - DLT Task\"\n",
					"    \n",
					"    job_name = f\"Cap-12-{DA.username}\"\n",
					"    \n",
					"    return job_name, notebook\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def print_pipeline_config(self):\n",
					"    \n",
					"    job_name, notebook = self.get_pipeline_config()\n",
					"\n",
					"    displayHTML(f\"\"\"<table style=\"width:100%\">\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Pipeline Name:</td>\n",
					"        <td><input type=\"text\" value=\"{job_name}\" style=\"width:100%\"></td></tr>\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Notebook Path:</td>\n",
					"        <td><input type=\"text\" value=\"{notebook}\" style=\"width:100%\"></td></tr>\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Datsets Path:</td>\n",
					"        <td><input type=\"text\" value=\"{DA.paths.datasets}\" style=\"width:100%\"></td></tr>\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Source:</td>\n",
					"        <td><input type=\"text\" value=\"{DA.paths.stream_path}\" style=\"width:100%\"></td></tr>\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Target:</td>\n",
					"        <td><input type=\"text\" value=\"{DA.db_name}\" style=\"width:100%\"></td></tr>\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Storage Location:</td>\n",
					"        <td><input type=\"text\" value=\"{DA.paths.storage_location}\" style=\"width:100%\"></td></tr>\n",
					"    </table>\"\"\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def create_pipeline(self):\n",
					"    \"Provided by DBAcademy, this function creates the prescribed pipline\"\n",
					"    \n",
					"    pipeline_name, path = self.get_pipeline_config()\n",
					"\n",
					"    # We need to delete the existing pipline so that we can apply updates\n",
					"    # because some attributes are not mutable after creation.\n",
					"    self.client.pipelines().delete_by_name(pipeline_name)\n",
					"    \n",
					"    response = self.client.pipelines().create(\n",
					"        name = pipeline_name, \n",
					"        storage = DA.paths.storage_location, \n",
					"        target = DA.db_name, \n",
					"        notebooks = [path],\n",
					"        continuous = True,\n",
					"        development = self.is_smoke_test(), # When testing, don't use production\n",
					"        configuration = {\n",
					"            \"spark.master\": \"local[*]\",\n",
					"            \"datasets_path\": DA.paths.datasets,\n",
					"            \"source\": DA.paths.stream_path,\n",
					"        },\n",
					"        clusters=[{ \"label\": \"default\", \"num_workers\": 0 }])\n",
					"    \n",
					"    pipeline_id = response.get(\"pipeline_id\")\n",
					"    print(f\"Created pipline {pipeline_id}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def validate_pipeline_config(self):\n",
					"    \"Provided by DBAcademy, this function validates the configuration of the pipeline\"\n",
					"    import json\n",
					"    \n",
					"    pipeline_name, path = self.get_pipeline_config()\n",
					"\n",
					"    pipeline = self.client.pipelines().get_by_name(pipeline_name)\n",
					"    assert pipeline is not None, f\"The pipline named \\\"{pipeline_name}\\\" doesn't exist. Double check the spelling.\"\n",
					"\n",
					"    spec = pipeline.get(\"spec\")\n",
					"    # print(json.dumps(spec, indent=4))\n",
					"    \n",
					"    storage = spec.get(\"storage\")\n",
					"    assert storage == DA.paths.storage_location, f\"Invalid storage location. Found \\\"{storage}\\\", expected \\\"{DA.paths.storage_location}\\\" \"\n",
					"    \n",
					"    target = spec.get(\"target\")\n",
					"    assert target == DA.db_name, f\"Invalid target. Found \\\"{target}\\\", expected \\\"{DA.db_name}\\\" \"\n",
					"    \n",
					"    libraries = spec.get(\"libraries\")\n",
					"    assert libraries is None or len(libraries) > 0, f\"The notebook path must be specified.\"\n",
					"    assert len(libraries) == 1, f\"More than one library (e.g. notebook) was specified.\"\n",
					"    first_library = libraries[0]\n",
					"    assert first_library.get(\"notebook\") is not None, f\"Incorrect library configuration - expected a notebook.\"\n",
					"    first_library_path = first_library.get(\"notebook\").get(\"path\")\n",
					"    assert first_library_path == path, f\"Invalid notebook path. Found \\\"{first_library_path}\\\", expected \\\"{path}\\\" \"\n",
					"\n",
					"    configuration = spec.get(\"configuration\")\n",
					"    assert configuration is not None, f\"The three configuration parameters were not specified.\"\n",
					"    datasets_path = configuration.get(\"datasets_path\")\n",
					"    assert datasets_path == DA.paths.datasets, f\"Invalid \\\"datasets_path\\\" value. Found \\\"{datasets_path}\\\", expected \\\"{DA.paths.datasets}\\\".\"\n",
					"    spark_master = configuration.get(\"spark.master\")\n",
					"    assert spark_master == f\"local[*]\", f\"Invalid \\\"spark.master\\\" value. Expected \\\"local[*]\\\", found \\\"{spark_master}\\\".\"\n",
					"    stream_source = configuration.get(\"source\")\n",
					"    assert stream_source == DA.paths.stream_path, f\"Invalid \\\"source\\\" value. Expected \\\"{DA.paths.stream_path}\\\", found \\\"{stream_source}\\\".\"\n",
					"    \n",
					"    cluster = spec.get(\"clusters\")[0]\n",
					"    autoscale = cluster.get(\"autoscale\")\n",
					"    assert autoscale is None, f\"Autoscaling should be disabled.\"\n",
					"    \n",
					"    num_workers = cluster.get(\"num_workers\")\n",
					"    assert num_workers == 0, f\"Expected the number of workers to be 0, found {num_workers}.\"\n",
					"\n",
					"    development = spec.get(\"development\")\n",
					"    assert development == self.is_smoke_test(), f\"The pipline mode should be set to \\\"Production\\\".\"\n",
					"    \n",
					"    channel = spec.get(\"channel\")\n",
					"    assert channel is None or channel == \"CURRENT\", f\"Expected the channel to be Current but found {channel}.\"\n",
					"    \n",
					"    photon = spec.get(\"photon\")\n",
					"    assert photon == True, f\"Expected Photon to be enabled.\"\n",
					"    \n",
					"    continuous = spec.get(\"continuous\")\n",
					"    assert continuous == True, f\"Expected the Pipeline mode to be \\\"Continuous\\\", found \\\"Triggered\\\".\"\n",
					"\n",
					"    policy = self.client.cluster_policies.get_by_name(\"Student's DLT-Only Policy\")\n",
					"    if policy is not None:\n",
					"        cluster = { \n",
					"            \"num_workers\": 0,\n",
					"            \"label\": \"default\", \n",
					"            \"policy_id\": policy.get(\"policy_id\")\n",
					"        }\n",
					"        self.client.pipelines.create_or_update(name = pipeline_name,\n",
					"                                               storage = DA.paths.storage_location,\n",
					"                                               target = DA.db_name,\n",
					"                                               notebooks = [path],\n",
					"                                               continuous = True,\n",
					"                                               development = False,\n",
					"                                               configuration = {\n",
					"                                                   \"spark.master\": \"local[*]\",\n",
					"                                                   \"datasets_path\": DA.paths.datasets,\n",
					"                                                   \"source\": DA.paths.stream_path,\n",
					"                                               },\n",
					"                                               clusters=[cluster])\n",
					"    print(\"All tests passed!\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def get_job_config(self):\n",
					"    \n",
					"    root = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None)\n",
					"    root = \"/\".join(root.split(\"/\")[:-1])\n",
					"    \n",
					"    notebook = f\"{root}/DE 12.2.3L - Land New Data\"\n",
					"\n",
					"    job_name = f\"Cap-12-{DA.username}\"\n",
					"    \n",
					"    return job_name, notebook"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def print_job_config(self):\n",
					"    \n",
					"    job_name, notebook = self.get_job_config()\n",
					"\n",
					"    displayHTML(f\"\"\"<table style=\"width:100%\">\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Job Name:</td>\n",
					"        <td><input type=\"text\" value=\"{job_name}\" style=\"width:100%\"></td></tr>\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Notebook Path:</td>\n",
					"        <td><input type=\"text\" value=\"{notebook}\" style=\"width:100%\"></td></tr>\n",
					"    </table>\"\"\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def create_job(self):\n",
					"    \"Provided by DBAcademy, this function creates the prescribed job\"\n",
					"    \n",
					"    job_name, notebook = self.get_job_config()\n",
					"\n",
					"    self.client.jobs.delete_by_name(job_name, success_only=False)\n",
					"    cluster_id = dbgems.get_tags().get(\"clusterId\")\n",
					"    \n",
					"    params = {\n",
					"        \"name\": job_name,\n",
					"        \"tags\": {\n",
					"            \"dbacademy.course\": self.course_name,\n",
					"            \"dbacademy.source\": self.course_name\n",
					"        },\n",
					"        \"email_notifications\": {},\n",
					"        \"timeout_seconds\": 7200,\n",
					"        \"max_concurrent_runs\": 1,\n",
					"        \"format\": \"MULTI_TASK\",\n",
					"        \"tasks\": [\n",
					"            {\n",
					"                \"task_key\": \"Land-Data\",\n",
					"                \"libraries\": [],\n",
					"                \"notebook_task\": {\n",
					"                    \"notebook_path\": notebook,\n",
					"                    \"base_parameters\": []\n",
					"                },\n",
					"                \"existing_cluster_id\": cluster_id\n",
					"            },\n",
					"        ],\n",
					"    }\n",
					"    params = self.update_cluster_params(params, [0])\n",
					"    \n",
					"    create_response = self.client.jobs().create(params)\n",
					"    job_id = create_response.get(\"job_id\")\n",
					"    \n",
					"    print(f\"Created job #{job_id}\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def validate_job_config(self):\n",
					"    \"Provided by DBAcademy, this function validates the configuration of the job\"\n",
					"    import json\n",
					"    \n",
					"    pipeline_name, job_path = self.get_pipeline_config()\n",
					"    job_name, notebook = self.get_job_config()\n",
					"\n",
					"    job = self.client.jobs.get_by_name(job_name)\n",
					"    assert job is not None, f\"The job named \\\"{job_name}\\\" doesn't exist. Double check the spelling.\"\n",
					"    \n",
					"    settings = job.get(\"settings\")\n",
					"    assert settings.get(\"format\") == \"SINGLE_TASK\", f\"\"\"Expected only one task.\"\"\"\n",
					"\n",
					"    # Land-Data Task\n",
					"#     task_name = settings.get(\"task_key\", None)\n",
					"#     assert task_name == \"Land-Data\", f\"Expected the first task to have the name \\\"Land-Data\\\", found \\\"{task_name}\\\"\"\n",
					"    \n",
					"    notebook_path = settings.get(\"notebook_task\", {}).get(\"notebook_path\")\n",
					"    assert notebook_path == notebook, f\"Invalid Notebook Path for the first task. Found \\\"{notebook_path}\\\", expected \\\"{notebook}\\\" \"\n",
					"\n",
					"    if not self.is_smoke_test():\n",
					"        # Don't check the actual_cluster_id when running as a smoke test\n",
					"        \n",
					"        actual_cluster_id = settings.get(\"existing_cluster_id\", None)\n",
					"        assert actual_cluster_id is not None, f\"The first task is not configured to use the current All-Purpose cluster\"\n",
					"\n",
					"        expected_cluster_id = dbgems.get_tags().get(\"clusterId\")\n",
					"        if expected_cluster_id != actual_cluster_id:\n",
					"            actual_cluster = self.client.clusters.get(actual_cluster_id).get(\"cluster_name\")\n",
					"            expected_cluster = self.client.clusters.get(expected_cluster_id).get(\"cluster_name\")\n",
					"            assert actual_cluster_id == expected_cluster_id, f\"The first task is not configured to use the current All-Purpose cluster, expected \\\"{expected_cluster}\\\", found \\\"{actual_cluster}\\\"\"\n",
					"\n",
					"    schedule = settings.get(\"schedule\")\n",
					"    if schedule is None:\n",
					"        print(\"WARNING: The job has not been scheduled.\\n\")\n",
					"    else:\n",
					"        pause_status = schedule.get(\"pause_status\")\n",
					"        if pause_status == \"PAUSED\":\n",
					"            print(\"WARNING: The job should not be paused.\\n\")\n",
					"        else:\n",
					"            quartz_cron_expression = schedule.get(\"quartz_cron_expression\")\n",
					"            if \"0/2 * * * ?\" not in quartz_cron_expression:\n",
					"                print(f\"WARNING: Expected the schedule to be \\\"* 0/2 * * * ?\\\" but found \\\"{quartz_cron_expression}\\\".\\n\")\n",
					"    \n",
					"    print(\"All tests passed!\")\n",
					"    "
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def start_job(self):\n",
					"    job_name, notebook = self.get_job_config()\n",
					"    job_id = self.client.jobs.get_by_name(job_name).get(\"job_id\")\n",
					"    run_id = self.client.jobs.run_now(job_id).get(\"run_id\")\n",
					"    print(f\"Started job #{job_id}, run #{run_id}\")\n",
					"\n",
					"    self.client.runs.wait_for(run_id)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"DA = DBAcademyHelper(lesson=\"cap_12\", **helper_arguments)\n",
					"DA.reset_environment()\n",
					"DA.init(install_datasets=True, create_db=True)\n",
					"\n",
					"DA.paths.stream_path = f\"{DA.paths.working_dir}/stream\"\n",
					"DA.paths.storage_location = f\"{DA.paths.working_dir}/storage\"\n",
					"\n",
					"DA.data_factory = DltDataFactory(DA.paths.stream_path)\n",
					"\n",
					"DA.conclude_setup()\n",
					""
				],
				"execution_count": null
			}
		]
	}
}