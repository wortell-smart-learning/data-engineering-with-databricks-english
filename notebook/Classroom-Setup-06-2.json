{
	"name": "Classroom-Setup-06-2",
	"properties": {
		"folder": {
			"name": "Includes"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "3dd58c4f-e3e6-4fad-9ec5-39bae4d87e96"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"%run /Includes/_utility-methods"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def autoload_to_table(data_source, source_format, table_name, checkpoint_directory):\n",
					"    (spark.readStream\n",
					"        .format(\"cloudFiles\")\n",
					"        .option(\"cloudFiles.format\", source_format)\n",
					"        .option(\"cloudFiles.schemaLocation\", checkpoint_directory)\n",
					"        .load(data_source)\n",
					"        .writeStream\n",
					"        .option(\"checkpointLocation\", checkpoint_directory)\n",
					"        .option(\"mergeSchema\", \"true\")\n",
					"        .trigger(availableNow=True)\n",
					"        .table(table_name)\n",
					"        .awaitTermination()\n",
					"    )"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"class DataFactory:\n",
					"    def __init__(self):\n",
					"        self.source = f\"{DA.paths.datasets}/healthcare/tracker/streaming\"\n",
					"        self.userdir = f\"{DA.paths.working_dir}/tracker\"\n",
					"        self.curr_mo = 1\n",
					"    \n",
					"    def load(self, continuous=False):\n",
					"        if self.curr_mo > 12:\n",
					"            print(\"Data source exhausted\\n\")\n",
					"            \n",
					"        elif continuous == True:\n",
					"            while self.curr_mo <= 12:\n",
					"                curr_file = f\"{self.curr_mo:02}.json\"\n",
					"                dbutils.fs.cp(f\"{self.source}/{curr_file}\", f\"{self.userdir}/{curr_file}\")\n",
					"                self.curr_mo += 1\n",
					"                autoload_to_table(self.userdir, \"json\", \"bronze\", f\"{DA.paths.checkpoints}/bronze\")\n",
					"        else:\n",
					"            curr_file = f\"{str(self.curr_mo).zfill(2)}.json\"\n",
					"            target_dir = f\"{self.userdir}/{curr_file}\"\n",
					"            print(f\"Loading the file {curr_file} to the {target_dir}\")\n",
					"            \n",
					"            dbutils.fs.cp(f\"{self.source}/{curr_file}\", f\"{self.userdir}/{curr_file}\")\n",
					"            self.curr_mo += 1\n",
					"            autoload_to_table(self.userdir, \"json\", \"bronze\", f\"{DA.paths.checkpoints}/bronze\")\n",
					"        "
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"DA = DBAcademyHelper(**helper_arguments)\n",
					"DA.reset_environment()\n",
					"DA.init(install_datasets=True, create_db=True)\n",
					"\n",
					"DA.data_factory = DataFactory()\n",
					"print()\n",
					"\n",
					"DA.data_factory.load()\n",
					"DA.conclude_setup()\n",
					"\n",
					"sqlContext.setConf(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)\n",
					""
				],
				"execution_count": null
			}
		]
	}
}