{
	"name": "DE 8-1-1 - DLT UI Walkthrough",
	"properties": {
		"folder": {
			"name": "08 - Delta Live Tables/DE 8.1 - DLT"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "4bd90a4a-4f95-4f5a-a952-d727f9b404c4"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"%md-sandbox\n",
					"\n",
					"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
					"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
					"</div>"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"1fb32f72-2ccc-4206-98d9-907287fc3262\"/>\n",
					"\n",
					"\n",
					"# Using the Delta Live Tables UI\n",
					"\n",
					"This demo will explore the DLT UI. \n",
					"\n",
					"\n",
					"## Learning Objectives\n",
					"By the end of this lesson, you should be able to:\n",
					"* Deploy a DLT pipeline\n",
					"* Explore the resultant DAG\n",
					"* Execute an update of the pipeline\n",
					"* Look at metrics"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"c950ed75-9a93-4340-a82c-e00505222d15\"/>\n",
					"\n",
					"\n",
					"## Run Setup\n",
					"\n",
					"The following cell is configured to reset this demo."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%run ../../Includes/Classroom-Setup-08.1.1"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"0a719ade-b4b5-49b5-89bf-8fc2b0b7d63c\"/>\n",
					"\n",
					"\n",
					"Execute the following cell to print out values that will be used during the following configuration steps."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"DA.print_pipeline_config()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"71b010a3-80be-4909-9b44-6f68029f16c0\"/>\n",
					"\n",
					"\n",
					"## Create and Configure a Pipeline\n",
					"\n",
					"In this section you will create a pipeline using a notebook provided with the courseware. We'll explore the contents of the notebook in the following lesson.\n",
					"\n",
					"1. Click the **Workflows** button on the sidebar.\n",
					"1. Select the **Delta Live Tables** tab.\n",
					"1. Click **Create Pipeline**.\n",
					"1. Leave **Product Edition** as **Advanced**.\n",
					"1. Fill in a **Pipeline Name** - because these names must be unique, we suggest using the **`Pipeline Name`** provided by the cell above.\n",
					"1. For **Notebook Libraries**, use the navigator to locate and select the notebook specified above.\n",
					"   * Even though this document is a standard Databricks Notebook, the SQL syntax is specialized to DLT table declarations.\n",
					"   * We will be exploring the syntax in the exercise that follows.\n",
					"1. Under **Configuration**, add two configuration parameters:\n",
					"   * Click **Add configuration**, set the \"key\" to **spark.master** and the \"value\" to **local[\\*]**.\n",
					"   * Click **Add configuration**, set the \"key\" to **datasets_path** and the \"value\" to the value provided in the cell above.\n",
					"1. In the **Target** field, enter the database name provided in the cell above.<br/>\n",
					"This should follow the pattern **`da_<name>_<hash>_dewd_dlt_demo_81`**\n",
					"   * This field is optional; if not specified, then tables will not be registered to a metastore, but will still be available in the DBFS. Refer to the <a href=\"https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-user-guide.html#publish-tables\" target=\"_blank\">documentation</a> for more information on this option.\n",
					"1. In the **Storage location** field, enter the path provided in the cell above.\n",
					"   * This optional field allows the user to specify a location to store logs, tables, and other information related to pipeline execution. \n",
					"   * If not specified, DLT will automatically generate a directory.\n",
					"1. For **Pipeline Mode**, select **Triggered**\n",
					"   * This field specifies how the pipeline will be run.\n",
					"   * **Triggered** pipelines run once and then shut down until the next manual or scheduled update.\n",
					"   * **Continuous** pipelines run continuously, ingesting new data as it arrives. Choose the mode based on latency and cost requirements.\n",
					"1. For **Pipeline Mode**, select **Triggered**.\n",
					"1. Uncheck the **Enable autoscaling** box.\n",
					"1. Set the number of **`workers`** to **`0`** (zero).\n",
					"   * Along with the **spark.master** config above, this will create a **Single Node** clusters.\n",
					"1. Enable **Photon Acceleration**.\n",
					"\n",
					"The fields **Enable autoscaling**, **Min Workers** and **Max Workers** control the worker configuration for the underlying cluster processing the pipeline. \n",
					"\n",
					"Notice the DBU estimate provided, similar to that provided when configuring interactive clusters.\n",
					"\n",
					"Finally, click **Create**."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"DA.validate_pipeline_config()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"a7e4b2fc-83a1-4509-8269-9a4c5791de21\"/>\n",
					"\n",
					"\n",
					"## Run a Pipeline\n",
					"\n",
					"With a pipeline created, you will now run the pipeline.\n",
					"\n",
					"1. Select **Development** to run the pipeline in development mode. \n",
					"  * Development mode provides for more expeditious iterative development by reusing the cluster (as opposed to creating a new cluster for each run) and disabling retries so that you can readily identify and fix errors.\n",
					"  * Refer to the <a href=\"https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-user-guide.html#optimize-execution\" target=\"_blank\">documentation</a> for more information on this feature.\n",
					"2. Click **Start**.\n",
					"\n",
					"The initial run will take several minutes while a cluster is provisioned. \n",
					"\n",
					"Subsequent runs will be appreciably quicker."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"4b92f93e-7a7f-4169-a1d2-9df3ac440674\"/>\n",
					"\n",
					"\n",
					"## Exploring the DAG\n",
					"\n",
					"As the pipeline completes, the execution flow is graphed. \n",
					"\n",
					"Selecting the tables reviews the details.\n",
					"\n",
					"Select **sales_orders_cleaned**. Notice the results reported in the **Data Quality** section. Because this flow has data expectations declared, those metrics are tracked here. No records are dropped because the constraint is declared in a way that allows violating records to be included in the output. This will be covered in more details in the next exercise."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md-sandbox\n",
					"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
					"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
					"<br/>\n",
					"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
				],
				"execution_count": null
			}
		]
	}
}