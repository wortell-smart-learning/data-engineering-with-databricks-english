{
	"name": "DE 8-2-1L - Lab Instructions",
	"properties": {
		"folder": {
			"name": "08 - Delta Live Tables/DE 8.2 - DLT Lab"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "606f0fb1-f88e-4123-9b63-df503da82713"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
"source": [
 					"\n",
					"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
					"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
					"</div>"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"# Lab: Migrating SQL Notebooks to Delta Live Tables\n",
					"\n",
					"This notebook describes the overall structure for the lab exercise, configures the environment for the lab, provides simulated data streaming, and performs cleanup once you are done. A notebook like this is not typically needed in a production pipeline scenario.\n",
					"\n",
					"## Learning Objectives\n",
					"By the end of this lab, you should be able to:\n",
					"* Convert existing data pipelines to Delta Live Tables"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"## Datasets Used\n",
					"\n",
					"This demo uses simplified artificially generated medical data. The schema of our two datasets is represented below. Note that we will be manipulating these schema during various steps.\n",
					"\n",
					"#### Recordings\n",
					"The main dataset uses heart rate recordings from medical devices delivered in the JSON format. \n",
					"\n",
					"| Field | Type |\n",
					"| --- | --- |\n",
					"| device_id | int |\n",
					"| mrn | long |\n",
					"| time | double |\n",
					"| heartrate | double |\n",
					"\n",
					"#### PII\n",
					"These data will later be joined with a static table of patient information stored in an external system to identify patients by name.\n",
					"\n",
					"| Field | Type |\n",
					"| --- | --- |\n",
					"| mrn | long |\n",
					"| name | string |"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"## Getting Started\n",
					"\n",
					"Begin by running the following cell to configure the lab environment."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%run /Includes/Classroom-Setup-08.2.1L"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"## Land Initial Data\n",
					"Seed the landing zone with more data before proceeding.\n",
					"\n",
					"You will re-run this command to land additional data later."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"DA.data_factory.load()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"Execute the following cell to print out values that will be used during the following configuration steps."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"DA.print_pipeline_config()    "
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"## Create and Configure a Pipeline\n",
					"\n",
					"1. Click the **Workflows** button on the sidebar.\n",
					"1. Select the **Delta Live Tables** tab.\n",
					"1. Click **Create Pipeline**.\n",
					"1. Leave **Product Edition** as **Advanced**.\n",
					"1. Fill in a **Pipeline Name** - because these names must be unique, we suggest using the **Pipeline Name** provided in the cell above.\n",
					"1. For **Notebook Libraries**, use the navigator to locate and select the notebook specified above.\n",
					"1. Under **Configuration**, add three configuration parameters:\n",
					"   * Click **Add configuration**, set the \"key\" to **spark.master** and the \"value\" to **local[\\*]**.\n",
					"   * Click **Add configuration**, set the \"key\" to **datasets_path** and the \"value\" to the value provided in the cell above.\n",
					"   * Click **Add configuration**, set the \"key\" to **source** and the \"value\" to the value provided in the cell above.\n",
					"1. In the **Target** field, enter the database name provided in the cell above.<br/>\n",
					"This should follow the pattern **`da_<name_<hash>_dewd_dlt_lab_82`**\n",
					"1. In the **Storage location** field, enter the path provided in the cell above.\n",
					"1. Enter the location printed next to **`Storage Location`** below in the **Storage Location** field.\n",
					"1. For **Pipeline Mode**, select **Triggered**.\n",
					"1. Uncheck the **Enable autoscaling** box.\n",
					"1. Set the number of **`workers`** to **`0`** (zero).\n",
					"1. Enable **Photon Acceleration**.\n",
					"\n",
					"Finally, click **Create**."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"DA.validate_pipeline_config()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"## Open and Complete DLT Pipeline Notebook\n",
					"\n",
					"You will perform your work in the companion notebook [DE 8.2.2L - Migrating a SQL Pipeline to DLT Lab]($./DE 8.2.2L - Migrating a SQL Pipeline to DLT Lab),<br/>\n",
					"which you will ultimately deploy as a pipeline.\n",
					"\n",
					"Open the Notebook and, following the guidelines provided therein, fill in the cells where prompted to<br/>\n",
					"implement a multi-hop architecture similar to the one we worked with in the previous section."
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"## Run your Pipeline\n",
					"\n",
					"Select **Development** mode, which accelerates the development lifecycle by reusing the same cluster across runs.<br/>\n",
					"It will also turn off automatic retries when jobs fail.\n",
					"\n",
					"Click **Start** to begin the first update to your table.\n",
					"\n",
					"Delta Live Tables will automatically deploy all the necessary infrastructure and resolve the dependencies between all datasets.\n",
					"\n",
					"**NOTE**: The first table update may take several minutes as relationships are resolved and infrastructure deploys."
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"## Troubleshooting Code in Development Mode\n",
					"\n",
					"Don't despair if your pipeline fails the first time. Delta Live Tables is in active development, and error messages are improving all the time.\n",
					"\n",
					"Because relationships between tables are mapped as a DAG, error messages will often indicate that a dataset isn't found.\n",
					"\n",
					"Let's consider our DAG below:\n",
					"\n",
					"<img src=\"https://files.training.databricks.com/images/dlt-dag.png\">\n",
					"\n",
					"If the error message **`Dataset not found: 'recordings_parsed'`** is raised, there may be several culprits:\n",
					"1. The logic defining **`recordings_parsed`** is invalid\n",
					"1. There is an error reading from **`recordings_bronze`**\n",
					"1. A typo exists in either **`recordings_parsed`** or **`recordings_bronze`**\n",
					"\n",
					"The safest way to identify the culprit is to iteratively add table/view definitions back into your DAG starting from your initial ingestion tables. You can simply comment out later table/view definitions and uncomment these between runs."
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 					"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
					"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
					"<br/>\n",
					"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
				],
				"execution_count": null
			}
		]
	}
}