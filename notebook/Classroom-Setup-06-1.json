{
	"name": "Classroom-Setup-06-1",
	"properties": {
		"folder": {
			"name": "Includes"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "dffe6431-823b-436a-809e-d4dc567e0e6f"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"%run ./_utility-methods"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"class DataFactory:\n",
					"    def __init__(self, ):\n",
					"        self.source = f\"{DA.paths.datasets}/healthcare/tracker/streaming\"\n",
					"        self.userdir = f\"{DA.paths.working_dir}/tracker\"\n",
					"        self.curr_mo = 1\n",
					"    \n",
					"    def load(self, continuous=False):\n",
					"        if self.curr_mo > 12:\n",
					"            print(\"Data source exhausted\\n\")\n",
					"            \n",
					"        elif continuous == True:\n",
					"            while self.curr_mo <= 12:\n",
					"                curr_file = f\"{self.curr_mo:02}.json\"\n",
					"                dbutils.fs.cp(self.source + curr_file, self.userdir + curr_file)\n",
					"                self.curr_mo += 1\n",
					"        else:\n",
					"            curr_file = f\"{str(self.curr_mo).zfill(2)}.json\"\n",
					"            target_dir = f\"{self.userdir}/{curr_file}\"\n",
					"            print(f\"Loading the file {curr_file} to the tracker dataset\")\n",
					"            dbutils.fs.cp(f\"{self.source}/{curr_file}\", target_dir)\n",
					"            self.curr_mo += 1"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"DA = DBAcademyHelper(**helper_arguments)\n",
					"DA.reset_environment()\n",
					"DA.init(install_datasets=True, create_db=True)\n",
					"\n",
					"DA.data_factory = DataFactory()\n",
					"print()\n",
					"\n",
					"DA.data_factory.load()\n",
					"DA.conclude_setup()\n",
					"\n",
					"sqlContext.setConf(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)\n",
					""
				],
				"execution_count": null
			}
		]
	}
}