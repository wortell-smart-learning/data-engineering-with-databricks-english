{
	"name": "Classroom-Setup-09-1-1",
	"properties": {
		"folder": {
			"name": "includes"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "b1197750-9320-4f41-aaaf-2ec77110ee65"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"%run ./_utility-methods"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def get_pipeline_config(self):\n",
					"    path = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None)\n",
					"    path = \"/\".join(path.split(\"/\")[:-1]) + \"/DE 9.1.3 - DLT Job\"\n",
					"    \n",
					"    pipeline_name = f\"DLT-Job-Demo-91-{DA.username}\"\n",
					"    \n",
					"    return pipeline_name, path\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def print_pipeline_config(self):\n",
					"    \"Provided by DBAcademy, this function renders the configuration of the pipeline as HTML\"\n",
					"    pipeline_name, path = self.get_pipeline_config()\n",
					"\n",
					"    displayHTML(f\"\"\"<table style=\"width:100%\">\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Pipeline Name:</td>\n",
					"        <td><input type=\"text\" value=\"{pipeline_name}\" style=\"width:100%\"></td></tr>\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Target:</td>\n",
					"        <td><input type=\"text\" value=\"{DA.db_name}\" style=\"width:100%\"></td></tr>\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Storage Location:</td>\n",
					"        <td><input type=\"text\" value=\"{DA.paths.storage_location}\" style=\"width:100%\"></td></tr>\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Notebook Path:</td>\n",
					"        <td><input type=\"text\" value=\"{path}\" style=\"width:100%\"></td></tr>\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Datsets Path:</td>\n",
					"        <td><input type=\"text\" value=\"{DA.paths.datasets}\" style=\"width:100%\"></td></tr>\n",
					"    \n",
					"    </table>\"\"\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def create_pipeline(self):\n",
					"    \"Provided by DBAcademy, this function creates the prescribed pipline\"\n",
					"    \n",
					"    pipeline_name, path = self.get_pipeline_config()\n",
					"\n",
					"    # We need to delete the existing pipline so that we can apply updates\n",
					"    # because some attributes are not mutable after creation.\n",
					"    self.client.pipelines().delete_by_name(pipeline_name)\n",
					"    \n",
					"    response = self.client.pipelines().create(\n",
					"        name = pipeline_name, \n",
					"        storage = DA.paths.storage_location, \n",
					"        target = DA.db_name, \n",
					"        notebooks = [path],\n",
					"        configuration = {\n",
					"            \"spark.master\": \"local[*]\",\n",
					"            \"datasets_path\": DA.paths.datasets,\n",
					"        },\n",
					"        clusters=[{ \"label\": \"default\", \"num_workers\": 0 }])\n",
					"    \n",
					"    pipeline_id = response.get(\"pipeline_id\")\n",
					"    print(f\"Created pipline {pipeline_id}\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def validate_pipeline_config(self):\n",
					"    \"Provided by DBAcademy, this function validates the configuration of the pipeline\"\n",
					"    import json\n",
					"    \n",
					"    pipeline_name, path = self.get_pipeline_config()\n",
					"\n",
					"    pipeline = self.client.pipelines().get_by_name(pipeline_name)\n",
					"    assert pipeline is not None, f\"The pipline named \\\"{pipeline_name}\\\" doesn't exist. Double check the spelling.\"\n",
					"\n",
					"    spec = pipeline.get(\"spec\")\n",
					"    \n",
					"    storage = spec.get(\"storage\")\n",
					"    assert storage == DA.paths.storage_location, f\"Invalid storage location. Found \\\"{storage}\\\", expected \\\"{DA.paths.storage_location}\\\" \"\n",
					"    \n",
					"    target = spec.get(\"target\")\n",
					"    assert target == DA.db_name, f\"Invalid target. Found \\\"{target}\\\", expected \\\"{DA.db_name}\\\" \"\n",
					"    \n",
					"    libraries = spec.get(\"libraries\")\n",
					"    assert libraries is None or len(libraries) > 0, f\"The notebook path must be specified.\"\n",
					"    assert len(libraries) == 1, f\"More than one library (e.g. notebook) was specified.\"\n",
					"    first_library = libraries[0]\n",
					"    assert first_library.get(\"notebook\") is not None, f\"Incorrect library configuration - expected a notebook.\"\n",
					"    first_library_path = first_library.get(\"notebook\").get(\"path\")\n",
					"    assert first_library_path == path, f\"Invalid notebook path. Found \\\"{first_library_path}\\\", expected \\\"{path}\\\" \"\n",
					"\n",
					"    configuration = spec.get(\"configuration\")\n",
					"    assert configuration is not None, f\"The two configuration parameters were not specified.\"\n",
					"    datasets_path = configuration.get(\"datasets_path\")\n",
					"    assert datasets_path == DA.paths.datasets, f\"Invalid datasets_path value. Found \\\"{datasets_path}\\\", expected \\\"{DA.paths.datasets}\\\".\"\n",
					"    spark_master = configuration.get(\"spark.master\")\n",
					"    assert spark_master == f\"local[*]\", f\"Invalid spark.master value. Expected \\\"local[*]\\\", found \\\"{spark_master}\\\".\"\n",
					"    \n",
					"    cluster = spec.get(\"clusters\")[0]\n",
					"    autoscale = cluster.get(\"autoscale\")\n",
					"    assert autoscale is None, f\"Autoscaling should be disabled.\"\n",
					"    \n",
					"    num_workers = cluster.get(\"num_workers\")\n",
					"    assert num_workers == 0, f\"Expected the number of workers to be 0, found {num_workers}.\"\n",
					"\n",
					"    development = spec.get(\"development\")\n",
					"    assert development == True, f\"The pipline mode should be set to \\\"Development\\\".\"\n",
					"    \n",
					"    channel = spec.get(\"channel\")\n",
					"    assert channel is None or channel == \"CURRENT\", f\"Expected the channel to be Current but found {channel}.\"\n",
					"    \n",
					"    photon = spec.get(\"photon\")\n",
					"    assert photon == True, f\"Expected Photon to be enabled.\"\n",
					"    \n",
					"    continuous = spec.get(\"continuous\")\n",
					"    assert continuous == False, f\"Expected the Pipeline mode to be \\\"Triggered\\\", found \\\"Continuous\\\".\"\n",
					"\n",
					"    policy = self.client.cluster_policies.get_by_name(\"Student's DLT-Only Policy\")\n",
					"    if policy is not None:\n",
					"        cluster = { \n",
					"            \"num_workers\": 0,\n",
					"            \"label\": \"default\", \n",
					"            \"policy_id\": policy.get(\"policy_id\")\n",
					"        }\n",
					"        self.client.pipelines.create_or_update(name = pipeline_name,\n",
					"                                               storage = DA.paths.storage_location,\n",
					"                                               target = DA.db_name,\n",
					"                                               notebooks = [path],\n",
					"                                               configuration = {\n",
					"                                                   \"spark.master\": \"local[*]\",\n",
					"                                                   \"datasets_path\": DA.paths.datasets,\n",
					"                                               },\n",
					"                                               clusters=[cluster])\n",
					"    print(\"All tests passed!\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def get_job_config(self):\n",
					"    job_name = f\"Jobs-Demo-91-{DA.username}\"\n",
					"    \n",
					"    notebook = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None)\n",
					"    notebook = \"/\".join(notebook.split(\"/\")[:-1]) + \"/DE 9.1.2 - Reset\"\n",
					"\n",
					"    return job_name, notebook\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def print_job_config_task_reset(self):\n",
					"    \"Provided by DBAcademy, this function renders the configuration of the job as HTML\"\n",
					"    job_name, reset_notebook = self.get_job_config()\n",
					"    \n",
					"    displayHTML(f\"\"\"<table style=\"width:100%\">\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Job Name:</td>\n",
					"        <td><input type=\"text\" value=\"{job_name}\" style=\"width:100%\"></td></tr>\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Reset Notebook Path:</td>\n",
					"        <td><input type=\"text\" value=\"{reset_notebook}\" style=\"width:100%\"></td></tr>\n",
					"\n",
					"    </table>\"\"\")    \n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def create_job_v1(self):\n",
					"    \"Provided by DBAcademy, this function creates the prescribed job\"\n",
					"    \n",
					"    job_name, reset_notebook = self.get_job_config()\n",
					"\n",
					"    self.client.jobs.delete_by_name(job_name, success_only=False)\n",
					"    cluster_id = dbgems.get_tags().get(\"clusterId\")\n",
					"    \n",
					"    params = {\n",
					"        \"name\": job_name,\n",
					"        \"tags\": {\n",
					"            \"dbacademy.course\": self.course_name,\n",
					"            \"dbacademy.source\": self.course_name\n",
					"        },\n",
					"        \"email_notifications\": {},\n",
					"        \"timeout_seconds\": 7200,\n",
					"        \"max_concurrent_runs\": 1,\n",
					"        \"format\": \"MULTI_TASK\",\n",
					"        \"tasks\": [\n",
					"            {\n",
					"                \"task_key\": \"Reset\",\n",
					"                \"libraries\": [],\n",
					"                \"notebook_task\": {\n",
					"                    \"notebook_path\": reset_notebook,\n",
					"                    \"base_parameters\": []\n",
					"                },\n",
					"                \"existing_cluster_id\": cluster_id\n",
					"            },\n",
					"        ],\n",
					"    }\n",
					"    params = self.update_cluster_params(params, [0])\n",
					"    \n",
					"    create_response = self.client.jobs().create(params)\n",
					"    job_id = create_response.get(\"job_id\")\n",
					"    \n",
					"    print(f\"Created job #{job_id}\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def validate_job_v1_config(self):\n",
					"    \"Provided by DBAcademy, this function validates the configuration of the job\"\n",
					"    import json\n",
					"    \n",
					"    job_name, reset_notebook = self.get_job_config()\n",
					"\n",
					"    job = self.client.jobs.get_by_name(job_name)\n",
					"    assert job is not None, f\"The job named \\\"{job_name}\\\" doesn't exist. Double check the spelling.\"\n",
					"\n",
					"    # print(json.dumps(job, indent=4))\n",
					"    \n",
					"    settings = job.get(\"settings\")\n",
					"    \n",
					"    if settings.get(\"format\") == \"SINGLE_TASK\":\n",
					"        notebook_path = settings.get(\"notebook_task\", {}).get(\"notebook_path\")\n",
					"        actual_cluster_id = settings.get(\"existing_cluster_id\", None)\n",
					"        #task_key = settings.get(\"task_key\", None)\n",
					"    else:\n",
					"        tasks = settings.get(\"tasks\", [])\n",
					"        assert len(tasks) == 1, f\"Expected one task, found {len(tasks)}.\"\n",
					"\n",
					"        notebook_path = tasks[0].get(\"notebook_task\", {}).get(\"notebook_path\")\n",
					"        actual_cluster_id = tasks[0].get(\"existing_cluster_id\", None)\n",
					"        \n",
					"        task_key = tasks[0].get(\"task_key\", None)\n",
					"        assert task_key == \"Rest\", f\"Expected the first task to have the name \\\"Reset\\\", found \\\"{task_key}\\\"\"\n",
					"        \n",
					"        \n",
					"    assert notebook_path == reset_notebook, f\"Invalid Notebook Path. Found \\\"{notebook_path}\\\", expected \\\"{reset_notebook}\\\" \"\n",
					"    \n",
					"    if not self.is_smoke_test():\n",
					"        # Don't check the actual_cluster_id when running as a smoke test\n",
					"        \n",
					"        assert actual_cluster_id is not None, f\"The first task is not configured to use the current All-Purpose cluster\"\n",
					"\n",
					"        expected_cluster_id = dbgems.get_tags().get(\"clusterId\")\n",
					"        if expected_cluster_id != actual_cluster_id:\n",
					"            actual_cluster = self.client.clusters.get(actual_cluster_id).get(\"cluster_name\")\n",
					"            expected_cluster = self.client.clusters.get(expected_cluster_id).get(\"cluster_name\")\n",
					"            assert actual_cluster_id == expected_cluster_id, f\"The first task is not configured to use the current All-Purpose cluster, expected \\\"{expected_cluster}\\\", found \\\"{actual_cluster}\\\"\"\n",
					"        \n",
					"    print(\"All tests passed!\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# @DBAcademyHelper.monkey_patch\n",
					"# def print_job_config_task_dlt(self):\n",
					"#     \"Provided by DBAcademy, this function renders the configuration of the job as HTML\"\n",
					"#     pipeline_name, path = self.get_pipeline_config()\n",
					"#     job_name, reset_notebook = self.get_job_config()\n",
					"    \n",
					"#     displayHTML(f\"\"\"<table style=\"width:100%\">\n",
					"#     <tr>\n",
					"#         <td style=\"white-space:nowrap; width:1em\">Job Name:</td>\n",
					"#         <td><input type=\"text\" value=\"{job_name}\" style=\"width:100%\"></td></tr>\n",
					"#     <tr>\n",
					"#         <td style=\"white-space:nowrap; width:1em\">Pipeline Name:</td>\n",
					"#         <td><input type=\"text\" value=\"{pipeline_name}\" style=\"width:100%\"></td></tr>\n",
					"\n",
					"#     </table>\"\"\")    \n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def create_job_v2(self):\n",
					"    \"Provided by DBAcademy, this function creates the prescribed job\"\n",
					"    \n",
					"    pipeline_name, path = self.get_pipeline_config()\n",
					"    job_name, reset_notebook = self.get_job_config()\n",
					"\n",
					"    self.client.jobs.delete_by_name(job_name, success_only=False)\n",
					"    cluster_id = dbgems.get_tags().get(\"clusterId\")\n",
					"    \n",
					"    pipeline = self.client.pipelines().get_by_name(pipeline_name)\n",
					"    pipeline_id = pipeline.get(\"pipeline_id\")\n",
					"    \n",
					"    params = {\n",
					"        \"name\": job_name,\n",
					"        \"tags\": {\n",
					"            \"dbacademy.course\": self.course_name,\n",
					"            \"dbacademy.source\": self.course_name\n",
					"        },\n",
					"        \"email_notifications\": {},\n",
					"        \"timeout_seconds\": 7200,\n",
					"        \"max_concurrent_runs\": 1,\n",
					"        \"format\": \"MULTI_TASK\",\n",
					"        \"tasks\": [\n",
					"            {\n",
					"                \"task_key\": \"Reset\",\n",
					"                \"libraries\": [],\n",
					"                \"notebook_task\": {\n",
					"                    \"notebook_path\": reset_notebook,\n",
					"                    \"base_parameters\": []\n",
					"                },\n",
					"                \"existing_cluster_id\": cluster_id\n",
					"            },\n",
					"            {\n",
					"                \"task_key\": \"DLT\",\n",
					"                \"depends_on\": [ { \"task_key\": \"Reset\" } ],\n",
					"                \"pipeline_task\": {\n",
					"                    \"pipeline_id\": pipeline_id\n",
					"                },\n",
					"            },\n",
					"        ],\n",
					"    }\n",
					"    params = self.update_cluster_params(params, [0])\n",
					"    \n",
					"    create_response = self.client.jobs().create(params)\n",
					"    job_id = create_response.get(\"job_id\")\n",
					"    \n",
					"    print(f\"Created job #{job_id}\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def validate_job_v2_config(self):\n",
					"    \"Provided by DBAcademy, this function validates the configuration of the job\"\n",
					"    import json\n",
					"    \n",
					"    pipeline_name, path = self.get_pipeline_config()\n",
					"    job_name, reset_notebook = self.get_job_config()\n",
					"\n",
					"    job = self.client.jobs.get_by_name(job_name)\n",
					"    assert job is not None, f\"The job named \\\"{job_name}\\\" doesn't exist. Double check the spelling.\"\n",
					"    \n",
					"    settings = job.get(\"settings\")\n",
					"    assert settings.get(\"format\") == \"MULTI_TASK\", f\"Expected two tasks, found 1.\"\n",
					"\n",
					"    tasks = settings.get(\"tasks\", [])\n",
					"    assert len(tasks) == 2, f\"Expected two tasks, found {len(tasks)}.\"\n",
					"    \n",
					"    \n",
					"    # Reset Task\n",
					"    task_name = tasks[0].get(\"task_key\", None)\n",
					"    assert task_name == \"Reset\", f\"Expected the first task to have the name \\\"Reset\\\", found \\\"{task_name}\\\"\"\n",
					"    \n",
					"    notebook_path = tasks[0].get(\"notebook_task\", {}).get(\"notebook_path\")\n",
					"    assert notebook_path == reset_notebook, f\"Invalid Notebook Path for the first task. Found \\\"{notebook_path}\\\", expected \\\"{reset_notebook}\\\" \"\n",
					"\n",
					"    if not self.is_smoke_test():\n",
					"        # Don't check the actual_cluster_id when running as a smoke test\n",
					"        \n",
					"        actual_cluster_id = tasks[0].get(\"existing_cluster_id\", None)\n",
					"        assert actual_cluster_id is not None, f\"The first task is not configured to use the current All-Purpose cluster\"\n",
					"\n",
					"        expected_cluster_id = dbgems.get_tags().get(\"clusterId\")\n",
					"        if expected_cluster_id != actual_cluster_id:\n",
					"            actual_cluster = self.client.clusters.get(actual_cluster_id).get(\"cluster_name\")\n",
					"            expected_cluster = self.client.clusters.get(expected_cluster_id).get(\"cluster_name\")\n",
					"            assert actual_cluster_id == expected_cluster_id, f\"The first task is not configured to use the current All-Purpose cluster, expected \\\"{expected_cluster}\\\", found \\\"{actual_cluster}\\\"\"\n",
					"\n",
					"    \n",
					"    \n",
					"    # Reset DLT\n",
					"    task_name = tasks[1].get(\"task_key\", None)\n",
					"    assert task_name == \"DLT\", f\"Expected the second task to have the name \\\"DLT\\\", found \\\"{task_name}\\\"\"\n",
					"\n",
					"    actual_pipeline_id = tasks[1].get(\"pipeline_task\", {}).get(\"pipeline_id\", None)\n",
					"    assert actual_pipeline_id is not None, f\"The second task is not configured to use a Delta Live Tables pipeline\"\n",
					"    \n",
					"    expected_pipeline = self.client.pipelines().get_by_name(pipeline_name)\n",
					"    actual_pipeline = self.client.pipelines().get_by_id(actual_pipeline_id)\n",
					"    actual_name = actual_pipeline.get(\"spec\").get(\"name\", \"Oops\")\n",
					"    assert actual_pipeline_id == expected_pipeline.get(\"pipeline_id\"), f\"The second task is not configured to use the correct pipeline, expected \\\"{pipeline_name}\\\", found \\\"{actual_name}\\\"\"\n",
					"    \n",
					"    depends_on = tasks[1].get(\"depends_on\", [])\n",
					"    assert len(depends_on) > 0, f\"The \\\"DLT\\\" task does not depend on the \\\"Reset\\\" task\"\n",
					"    assert len(depends_on) == 1, f\"The \\\"DLT\\\" task depends on more than just the \\\"Reset\\\" task\"\n",
					"    depends_task_key = depends_on[0].get(\"task_key\")\n",
					"    assert depends_task_key == \"Reset\", f\"The \\\"DLT\\\" task doesn't depend on the \\\"Reset\\\" task, found {depends_task_key}\"\n",
					"    \n",
					"    print(\"All tests passed!\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def start_job(self):\n",
					"    job_name, reset_notebook = self.get_job_config()\n",
					"    job_id = self.client.jobs.get_by_name(job_name).get(\"job_id\")\n",
					"    run_id = self.client.jobs.run_now(job_id).get(\"run_id\")\n",
					"    print(f\"Started job #{job_id}, run #{run_id}\")\n",
					"\n",
					"    self.client.runs.wait_for(run_id)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# jobs_demo_91 is specifically referenced in the lesson\n",
					"\n",
					"DA = DBAcademyHelper(lesson=\"jobs_demo_91\", **helper_arguments)\n",
					"DA.reset_environment() # First in a series\n",
					"DA.init(install_datasets=True, create_db=True)\n",
					"\n",
					"DA.paths.stream_path = f\"{DA.paths.working_dir}/stream\"\n",
					"DA.paths.storage_location = f\"{DA.paths.working_dir}/storage\"\n",
					"\n",
					"DA.data_factory = DltDataFactory(DA.paths.stream_path)\n",
					"\n",
					"DA.conclude_setup()\n",
					""
				],
				"execution_count": null
			}
		]
	}
}