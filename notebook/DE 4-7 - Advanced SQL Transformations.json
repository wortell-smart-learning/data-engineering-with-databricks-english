{
	"name": "DE 4-7 - Advanced SQL Transformations",
	"properties": {
		"folder": {
			"name": "04 - ETL with Spark SQL"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "5c5bf0d7-8ea0-4a3b-85f7-62a7139f2b87"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_sparksql",
				"display_name": "sql"
			},
			"language_info": {
				"name": "sql"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
"source": [
 					"\n",
					"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
					"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
					"</div>"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"# Advanced SQL Transformations\n",
					"\n",
					"Querying tabular data stored in the data lakehouse with Spark SQL is easy, efficient, and fast.\n",
					"\n",
					"This gets more complicated as the data structure becomes less regular, when many tables need to be used in a single query, or when the shape of data needs to be changed dramatically. This notebook introduces a number of functions present in Spark SQL to help engineers complete even the most complicated transformations.\n",
					"\n",
					"## Learning Objectives\n",
					"By the end of this lesson, you should be able to:\n",
					"- Use **`.`** and **`:`** syntax to query nested data\n",
					"- Work with JSON\n",
					"- Flatten and unpacking arrays and structs\n",
					"- Combine datasets using joins and set operators\n",
					"- Reshape data using pivot tables\n",
					"- Use higher order functions for working with arrays"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"## Run Setup\n",
					"\n",
					"The setup script will create the data and declare necessary values for the rest of this notebook to execute."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%run /Includes/Classroom-Setup-04.7"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"## Interacting with JSON Data\n",
					"\n",
					"The **`events_raw`** table was registered against data representing a Kafka payload.\n",
					"\n",
					"In most cases, Kafka data will be binary-encoded JSON values. We'll cast the **`key`** and **`value`** as strings below to look at these in a human-readable format."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"CREATE OR REPLACE TEMP VIEW events_strings AS\n",
					"  SELECT string(key), string(value) \n",
					"  FROM events_raw;\n",
					"  \n",
					"SELECT * FROM events_strings"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"Spark SQL has built-in functionality to directly interact with JSON data stored as strings. We can use the **`:`** syntax to traverse nested data structures."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"SELECT value:device, value:geo:city \n",
					"FROM events_strings"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"Spark SQL also has the ability to parse JSON objects into struct types (a native Spark type with nested attributes).\n",
					"\n",
					"However, the **`from_json`** function requires a schema. To derive the schema of our current data, we'll start by executing a query we know will return a JSON value with no null fields."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"SELECT value \n",
					"FROM events_strings \n",
					"WHERE value:event_name = \"finalize\" \n",
					"ORDER BY key\n",
					"LIMIT 1"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"Spark SQL also has a **`schema_of_json`** function to derive the JSON schema from an example. Here, we copy and paste an example JSON to the function and chain it into the **`from_json`** function to cast our **`value`** field to a struct type."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"CREATE OR REPLACE TEMP VIEW parsed_events AS\n",
					"  SELECT from_json(value, schema_of_json('{\"device\":\"Linux\",\"ecommerce\":{\"purchase_revenue_in_usd\":1075.5,\"total_item_quantity\":1,\"unique_items\":1},\"event_name\":\"finalize\",\"event_previous_timestamp\":1593879231210816,\"event_timestamp\":1593879335779563,\"geo\":{\"city\":\"Houston\",\"state\":\"TX\"},\"items\":[{\"coupon\":\"NEWBED10\",\"item_id\":\"M_STAN_K\",\"item_name\":\"Standard King Mattress\",\"item_revenue_in_usd\":1075.5,\"price_in_usd\":1195.0,\"quantity\":1}],\"traffic_source\":\"email\",\"user_first_touch_timestamp\":1593454417513109,\"user_id\":\"UA000000106116176\"}')) AS json \n",
					"  FROM events_strings;\n",
					"  \n",
					"SELECT * FROM parsed_events"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"Once a JSON string is unpacked to a struct type, Spark supports **`*`** (star) unpacking to flatten fields into columns."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"CREATE OR REPLACE TEMP VIEW new_events_final AS\n",
					"  SELECT json.* \n",
					"  FROM parsed_events;\n",
					"  \n",
					"SELECT * FROM new_events_final"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"## Explore Data Structures\n",
					"\n",
					"Spark SQL has robust syntax for working with complex and nested data types.\n",
					"\n",
					"Start by looking at the fields in the **`events`** table."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"DESCRIBE events"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"The **`ecommerce`** field is a struct that contains a double and 2 longs.\n",
					"\n",
					"We can interact with the subfields in this field using standard **`.`** syntax similar to how we might traverse nested data in JSON."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"SELECT ecommerce.purchase_revenue_in_usd \n",
					"FROM events\n",
					"WHERE ecommerce.purchase_revenue_in_usd IS NOT NULL"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"## Working with Arrays\n",
					"The **`items`** field in the **`events`** table is an array of structs.\n",
					"\n",
					"Spark SQL has a number of functions specifically to deal with arrays.\n",
					"\n",
					"For example, the **`size`** function provides a count of the number of elements in an array for each row.\n",
					"\n",
					"Let's use this to filter for event records with arrays containing 3 or more items."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"SELECT user_id, event_timestamp, event_name, items\n",
					"FROM events\n",
					"WHERE size(items) > 2"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					" \n",
					"## Explode Arrays\n",
					"\n",
					"The **`explode`** function lets us put each element in an array on its own row.\n",
					"\n",
					"Let's use this to explode event records with 3 or more items into separate rows, one for each item in the array."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"SELECT user_id, event_timestamp, event_name, explode(items) AS item\n",
					"FROM events\n",
					"WHERE size(items) > 2"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					" \n",
					"## Collect Arrays\n",
					"\n",
					"The **`collect_set`** function can collect unique values for a field, including fields within arrays.\n",
					"\n",
					"The **`flatten`** function allows multiple arrays to be combined into a single array.\n",
					"\n",
					"The **`array_distinct`** function removes duplicate elements from an array.\n",
					"\n",
					"Here, we combine these queries to create a simple table that shows the unique collection of actions and the items in a user's cart."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"SELECT user_id,\n",
					"  collect_set(event_name) AS event_history,\n",
					"  array_distinct(flatten(collect_set(items.item_id))) AS cart_history\n",
					"FROM events\n",
					"GROUP BY user_id"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					" \n",
					"## Join Tables\n",
					"\n",
					"Spark SQL supports standard join operations (inner, outer, left, right, anti, cross, semi).\n",
					"\n",
					"Here we chain a join with a lookup table to an **`explode`** operation to grab the standard printed item name."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"CREATE OR REPLACE VIEW sales_enriched AS\n",
					"SELECT *\n",
					"FROM (\n",
					"  SELECT *, explode(items) AS item \n",
					"  FROM sales) a\n",
					"INNER JOIN item_lookup b\n",
					"ON a.item.item_id = b.item_id;\n",
					"\n",
					"SELECT * FROM sales_enriched"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"## Set Operators\n",
					"Spark SQL supports **`UNION`**, **`MINUS`**, and **`INTERSECT`** set operators.\n",
					"\n",
					"**`UNION`** returns the collection of two queries. \n",
					"\n",
					"The query below returns the same results as if we inserted our **`new_events_final`** into the **`events`** table."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"SELECT * FROM events \n",
					"UNION \n",
					"SELECT * FROM new_events_final"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"**`INTERSECT`** returns all rows found in both relations."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"SELECT * FROM events \n",
					"INTERSECT \n",
					"SELECT * FROM new_events_final"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"The above query returns no results because our two datasets have no values in common.\n",
					"\n",
					"**`MINUS`** returns all the rows found in one dataset but not the other; we'll skip executing this here as our previous query demonstrates we have no values in common."
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					" \n",
					"\n",
					"## Pivot Tables\n",
					"The **`PIVOT`** clause is used for data perspective. We can get the aggregated values based on specific column values, which will be turned to multiple columns used in **`SELECT`** clause. The **`PIVOT`** clause can be specified after the table name or subquery.\n",
					"\n",
					"**`SELECT * FROM ()`**: The **`SELECT`** statement inside the parentheses is the input for this table.\n",
					"\n",
					"**`PIVOT`**: The first argument in the clause is an aggregate function and the column to be aggregated. Then, we specify the pivot column in the **`FOR`** subclause. The **`IN`** operator contains the pivot column values. \n",
					"\n",
					"Here we use **`PIVOT`** to create a new **`transactions`** table that flattens out the information contained in the **`sales`** table.\n",
					"\n",
					"This flattened data format can be useful for dashboarding, but also useful for applying machine learning algorithms for inference or prediction."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"CREATE OR REPLACE TABLE transactions AS\n",
					"\n",
					"SELECT * FROM (\n",
					"  SELECT\n",
					"    email,\n",
					"    order_id,\n",
					"    transaction_timestamp,\n",
					"    total_item_quantity,\n",
					"    purchase_revenue_in_usd,\n",
					"    unique_items,\n",
					"    item.item_id AS item_id,\n",
					"    item.quantity AS quantity\n",
					"  FROM sales_enriched\n",
					") PIVOT (\n",
					"  sum(quantity) FOR item_id in (\n",
					"    'P_FOAM_K',\n",
					"    'M_STAN_Q',\n",
					"    'P_FOAM_S',\n",
					"    'M_PREM_Q',\n",
					"    'M_STAN_F',\n",
					"    'M_STAN_T',\n",
					"    'M_PREM_K',\n",
					"    'M_PREM_F',\n",
					"    'M_STAN_K',\n",
					"    'M_PREM_T',\n",
					"    'P_DOWN_S',\n",
					"    'P_DOWN_K'\n",
					"  )\n",
					");\n",
					"\n",
					"SELECT * FROM transactions"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"\n",
					"## Higher Order Functions\n",
					"Higher order functions in Spark SQL allow you to work directly with complex data types. When working with hierarchical data, records are frequently stored as array or map type objects. Higher-order functions allow you to transform data while preserving the original structure.\n",
					"\n",
					"Higher order functions include:\n",
					"- **`FILTER`** filters an array using the given lambda function.\n",
					"- **`EXIST`** tests whether a statement is true for one or more elements in an array. \n",
					"- **`TRANSFORM`** uses the given lambda function to transform all elements in an array.\n",
					"- **`REDUCE`** takes two lambda functions to reduce the elements of an array to a single value by merging the elements into a buffer, and the apply a finishing function on the final buffer."
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"## Filter\n",
					"Remove items that are not king-sized from all records in our **`items`** column. We can use the **`FILTER`** function to create a new column that excludes that value from each array.\n",
					"\n",
					"**`FILTER (items, i -> i.item_id LIKE \"%K\") AS king_items`**\n",
					"\n",
					"In the statement above:\n",
					"- **`FILTER`** : the name of the higher-order function <br>\n",
					"- **`items`** : the name of our input array <br>\n",
					"- **`i`** : the name of the iterator variable. You choose this name and then use it in the lambda function. It iterates over the array, cycling each value into the function one at a time.<br>\n",
					"- **`->`** :  Indicates the start of a function <br>\n",
					"- **`i.item_id LIKE \"%K\"`** : This is the function. Each value is checked to see if it ends with the capital letter K. If it is, it gets filtered into the new column, **`king_items`**"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"-- filter for sales of only king sized items\n",
					"SELECT\n",
					"  order_id,\n",
					"  items,\n",
					"  FILTER (items, i -> i.item_id LIKE \"%K\") AS king_items\n",
					"FROM sales"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"You may write a filter that produces a lot of empty arrays in the created column. When that happens, it can be useful to use a **`WHERE`** clause to show only non-empty array values in the returned column. \n",
					"\n",
					"In this example, we accomplish that by using a subquery (a query within a query). They are useful for performing an operation in multiple steps. In this case, we're using it to create the named column that we will use with a **`WHERE`** clause."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"CREATE OR REPLACE TEMP VIEW king_size_sales AS\n",
					"\n",
					"SELECT order_id, king_items\n",
					"FROM (\n",
					"  SELECT\n",
					"    order_id,\n",
					"    FILTER (items, i -> i.item_id LIKE \"%K\") AS king_items\n",
					"  FROM sales)\n",
					"WHERE size(king_items) > 0;\n",
					"  \n",
					"SELECT * FROM king_size_sales"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"## Transform\n",
					"Built-in functions are designed to operate on a single, simple data type within a cell; they cannot process array values. **`TRANSFORM`** can be particularly useful when you want to apply an existing function to each element in an array. \n",
					"\n",
					"Compute the total revenue from king-sized items per order.\n",
					"\n",
					"**`TRANSFORM(king_items, k -> CAST(k.item_revenue_in_usd * 100 AS INT)) AS item_revenues`**\n",
					"\n",
					"In the statement above, for each value in the input array, we extract the item's revenue value, multiply it by 100, and cast the result to integer. Note that we're using the same kind as references as in the previous command, but we name the iterator with a new variable, **`k`**."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"-- get total revenue from king items per order\n",
					"CREATE OR REPLACE TEMP VIEW king_item_revenues AS\n",
					"\n",
					"SELECT\n",
					"  order_id,\n",
					"  king_items,\n",
					"  TRANSFORM (\n",
					"    king_items,\n",
					"    k -> CAST(k.item_revenue_in_usd * 100 AS INT)\n",
					"  ) AS item_revenues\n",
					"FROM king_size_sales;\n",
					"\n",
					"SELECT * FROM king_item_revenues\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"## Summary\n",
					"Spark SQL offers a comprehensive set of native functionality for interacting with and manipulating highly nested data.\n",
					"\n",
					"While some syntax for this functionality may be unfamiliar to SQL users, leveraging built-in functions like higher order functions can prevent SQL engineers from needing to rely on custom logic when dealing with highly complex data structures."
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					" \n",
					"Run the following cell to delete the tables and files associated with this lesson."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%%pyspark\n",
					"DA.cleanup()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 					"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
					"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
					"<br/>\n",
					"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
				],
				"execution_count": null
			}
		]
	}
}