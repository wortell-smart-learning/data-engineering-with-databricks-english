{
	"name": "DE 4-5L - Extract and Load Data Lab",
	"properties": {
		"folder": {
			"name": "04 - ETL with Spark SQL"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "28cfe6b3-f1f6-482a-9348-8eee98b65fcd"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_sparksql",
				"display_name": "sql"
			},
			"language_info": {
				"name": "sql"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"%md-sandbox\n",
					"\n",
					"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
					"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
					"</div>"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"d2c3cdc2-5fcf-4edc-8101-2964a9355000\"/>\n",
					"\n",
					"\n",
					"# Extract and Load Data Lab\n",
					"\n",
					"In this lab, you will extract and load raw data from JSON files into a Delta table.\n",
					"\n",
					"## Learning Objectives\n",
					"By the end of this lab, you should be able to:\n",
					"- Create an external table to extract data from JSON files\n",
					"- Create an empty Delta table with a provided schema\n",
					"- Insert records from an existing table into a Delta table\n",
					"- Use a CTAS statement to create a Delta table from files"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"e261fd97-ffd7-44b2-b1ca-61b843ee8961\"/>\n",
					"\n",
					"\n",
					"## Run Setup\n",
					"\n",
					"Run the following cell to configure variables and datasets for this lesson."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%run ../Includes/Classroom-Setup-04.5L"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"d7759322-f9b9-4abe-9b30-25f5a7e30d9c\"/>\n",
					"\n",
					"\n",
					"## Overview of the Data\n",
					"\n",
					"We will work with a sample of raw Kafka data written as JSON files. \n",
					"\n",
					"Each file contains all records consumed during a 5-second interval, stored with the full Kafka schema as a multiple-record JSON file. \n",
					"\n",
					"The schema for the table:\n",
					"\n",
					"| field  | type | description |\n",
					"| ------ | ---- | ----------- |\n",
					"| key    | BINARY | The **`user_id`** field is used as the key; this is a unique alphanumeric field that corresponds to session/cookie information |\n",
					"| offset | LONG | This is a unique value, monotonically increasing for each partition |\n",
					"| partition | INTEGER | Our current Kafka implementation uses only 2 partitions (0 and 1) |\n",
					"| timestamp | LONG    | This timestamp is recorded as milliseconds since epoch, and represents the time at which the producer appends a record to a partition |\n",
					"| topic | STRING | While the Kafka service hosts multiple topics, only those records from the **`clickstream`** topic are included here |\n",
					"| value | BINARY | This is the full data payload (to be discussed later), sent as JSON |"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"f2cd70fe-65a1-4dce-b264-c0c7d225640a\"/>\n",
					"\n",
					" \n",
					"## Extract Raw Events From JSON Files\n",
					"To load this data into Delta properly, we first need to extract the JSON data using the correct schema.\n",
					"\n",
					"Create an external table against JSON files located at the filepath provided below. Name this table **`events_json`** and declare the schema above."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"-- TODO\n",
					"<FILL_IN> ${da.paths.datasets}/ecommerce/raw/events-kafka/"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"07ce3850-fdc7-4dea-9335-2a093c2e200c\"/>\n",
					"\n",
					"\n",
					"**NOTE**: We'll use Python to run checks occasionally throughout the lab. The following cell will return an error with a message on what needs to change if you have not followed instructions. No output from cell execution means that you have completed this step."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"assert spark.table(\"events_json\"), \"Table named `events_json` does not exist\"\n",
					"assert spark.table(\"events_json\").columns == ['key', 'offset', 'partition', 'timestamp', 'topic', 'value'], \"Please name the columns in the order provided above\"\n",
					"assert spark.table(\"events_json\").dtypes == [('key', 'binary'), ('offset', 'bigint'), ('partition', 'int'), ('timestamp', 'bigint'), ('topic', 'string'), ('value', 'binary')], \"Please make sure the column types are identical to those provided above\"\n",
					"\n",
					"total = spark.table(\"events_json\").count()\n",
					"assert total == 2252, f\"Expected 2252 records, found {total}\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"ae3b8554-d0e7-4fd7-b25a-27bfbc5f7c13\"/>\n",
					"\n",
					"\n",
					"\n",
					"## Insert Raw Events Into Delta Table\n",
					"Create an empty managed Delta table named **`events_raw`** using the same schema."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"-- TODO\n",
					"<FILL_IN>"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"3d56975b-47ba-4678-ae7b-7c5e4ac20a97\"/>\n",
					"\n",
					"\n",
					"\n",
					"Run the cell below to confirm the table was created correctly."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"assert spark.table(\"events_raw\"), \"Table named `events_raw` does not exist\"\n",
					"assert spark.table(\"events_raw\").columns == ['key', 'offset', 'partition', 'timestamp', 'topic', 'value'], \"Please name the columns in the order provided above\"\n",
					"assert spark.table(\"events_raw\").dtypes == [('key', 'binary'), ('offset', 'bigint'), ('partition', 'int'), ('timestamp', 'bigint'), ('topic', 'string'), ('value', 'binary')], \"Please make sure the column types are identical to those provided above\"\n",
					"assert spark.table(\"events_raw\").count() == 0, \"The table should have 0 records\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"61815a62-6d4f-47fb-98a9-73c39842ac56\"/>\n",
					"\n",
					"\n",
					"\n",
					"Once the extracted data and Delta table are ready, insert the JSON records from the **`events_json`** table into the new **`events_raw`** Delta table."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"-- TODO\n",
					"<FILL_IN>"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"4f545052-31c6-442b-a5e8-4c5892ec912f\"/>\n",
					"\n",
					"\n",
					"Manually review the table contents to ensure data was written as expected."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"-- TODO\n",
					"<FILL_IN>"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"0d66f26b-3df6-4819-9d84-22da9f55aeaa\"/>\n",
					"\n",
					"\n",
					"\n",
					"Run the cell below to confirm the data has been loaded correctly."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"assert spark.table(\"events_raw\").count() == 2252, \"The table should have 2252 records\"\n",
					"assert set(row['timestamp'] for row in spark.table(\"events_raw\").select(\"timestamp\").limit(5).collect()) == {1593880885085, 1593880892303, 1593880889174, 1593880886106, 1593880889725}, \"Make sure you have not modified the data provided\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"e9565088-1762-4f89-a06f-49576a53526a\"/>\n",
					"\n",
					"\n",
					"\n",
					"## Create Delta Table from a Query\n",
					"In addition to new events data, let's also load a small lookup table that provides product details that we'll use later in the course.\n",
					"Use a CTAS statement to create a managed Delta table named **`item_lookup`** that extracts data from the parquet directory provided below."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"-- TODO\n",
					"<FILL_IN> ${da.paths.datasets}/ecommerce/raw/item-lookup"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"9f1ad20f-1238-4a12-ad2a-f10169ed6475\"/>\n",
					"\n",
					"\n",
					"\n",
					"Run the cell below to confirm the lookup table has been loaded correctly."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"assert spark.table(\"item_lookup\").count() == 12, \"The table should have 12 records\"\n",
					"assert set(row['item_id'] for row in spark.table(\"item_lookup\").select(\"item_id\").limit(5).collect()) == {'M_PREM_F', 'M_PREM_K', 'M_PREM_Q', 'M_PREM_T', 'M_STAN_F'}, \"Make sure you have not modified the data provided\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"c24885ea-010e-4b76-9e9d-cc749f10993a\"/>\n",
					"\n",
					" \n",
					"Run the following cell to delete the tables and files associated with this lesson."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"DA.cleanup()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md-sandbox\n",
					"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
					"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
					"<br/>\n",
					"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
				],
				"execution_count": null
			}
		]
	}
}