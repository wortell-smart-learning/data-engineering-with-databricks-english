{
	"name": "DE 4-1 - Querying Files Directly",
	"properties": {
		"folder": {
			"name": "04 - ETL with Spark SQL"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "6d6b1d08-f313-48e4-8e07-1cf210f52df2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_sparksql",
				"display_name": "sql"
			},
			"language_info": {
				"name": "sql"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"%md-sandbox\n",
					"\n",
					"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
					"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
					"</div>"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"ba5cb184-9677-4b79-b000-f42c5fff9044\"/>\n",
					"\n",
					"\n",
					"# Extracting Data Directly from Files\n",
					"\n",
					"In this notebook, you'll learn to extract data directly from files using Spark SQL on Databricks.\n",
					"\n",
					"A number of file formats support this option, but it is most useful for self-describing data formats (such as parquet and JSON).\n",
					"\n",
					"## Learning Objectives\n",
					"By the end of this lesson, you should be able to:\n",
					"- Use Spark SQL to directly query data files\n",
					"- Leverage **`text`** and **`binaryFile`** methods to review raw file contents"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"e9800a3a-c96c-4ce2-a835-b5f058e26ead\"/>\n",
					"\n",
					"\n",
					"## Run Setup\n",
					"\n",
					"The setup script will create the data and declare necessary values for the rest of this notebook to execute."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%run ../Includes/Classroom-Setup-04.1"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"fedca70d-2bf7-415b-8ab9-1691c2366b24\"/>\n",
					"\n",
					"\n",
					"## Data Overview\n",
					"\n",
					"In this example, we'll work with a sample of raw Kafka data written as JSON files. \n",
					"\n",
					"Each file contains all records consumed during a 5-second interval, stored with the full Kafka schema as a multiple-record JSON file.\n",
					"\n",
					"| field | type | description |\n",
					"| --- | --- | --- |\n",
					"| key | BINARY | The **`user_id`** field is used as the key; this is a unique alphanumeric field that corresponds to session/cookie information |\n",
					"| value | BINARY | This is the full data payload (to be discussed later), sent as JSON |\n",
					"| topic | STRING | While the Kafka service hosts multiple topics, only those records from the **`clickstream`** topic are included here |\n",
					"| partition | INTEGER | Our current Kafka implementation uses only 2 partitions (0 and 1) |\n",
					"| offset | LONG | This is a unique value, monotonically increasing for each partition |\n",
					"| timestamp | LONG | This timestamp is recorded as milliseconds since epoch, and represents the time at which the producer appends a record to a partition |"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"00f263ec-293b-4adf-bf4b-b81f04de6e31\"/>\n",
					"\n",
					"\n",
					"Note that our source directory contains many JSON files."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"print(DA.paths.kafka_events)\n",
					"\n",
					"files = dbutils.fs.ls(DA.paths.kafka_events)\n",
					"display(files)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"f4cbde61-2f10-4758-82ca-786e16606d60\"/>\n",
					"\n",
					"\n",
					"Here, we'll be using relative file paths to data that's been written to the DBFS root. \n",
					"\n",
					"Most workflows will require users to access data from external cloud storage locations. \n",
					"\n",
					"In most companies, a workspace administrator will be responsible for configuring access to these storage locations.\n",
					"\n",
					"Instructions for configuring and accessing these locations can be found in the cloud-vendor specific self-paced courses titled \"Cloud Architecture & Systems Integrations\"."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"8e04a0d2-4d79-4547-b2b2-765eefaf6285\"/>\n",
					"\n",
					"\n",
					"## Query a Single File\n",
					"\n",
					"To query the data contained in a single file, execute the query with the following pattern:\n",
					"\n",
					"<strong><code>SELECT * FROM file_format.&#x60;/path/to/file&#x60;</code></strong>\n",
					"\n",
					"Make special note of the use of back-ticks (not single quotes) around the path."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"SELECT * FROM json.`${DA.paths.kafka_events}/001.json`"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"02c296c6-80be-4bd8-99cc-29f2e44e1d2d\"/>\n",
					"\n",
					"\n",
					"Note that our preview displays all 321 rows of our source file."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"01cd5a22-a236-4ef6-bf65-7c40379d7ef9\"/>\n",
					"\n",
					"\n",
					"## Query a Directory of Files\n",
					"\n",
					"Assuming all of the files in a directory have the same format and schema, all files can be queried simultaneously by specifying the directory path rather than an individual file."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"SELECT * FROM json.`${DA.paths.kafka_events}`"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"f36c3a77-4b84-41a0-95ae-a8f999a0f60e\"/>\n",
					"\n",
					"\n",
					"By default, this query will only show the first 1000 rows."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"46590bb8-cf4b-4c3d-a9c6-e431bad4a5e9\"/>\n",
					"\n",
					"\n",
					"## Create References to Files\n",
					"This ability to directly query files and directories means that additional Spark logic can be chained to queries against files.\n",
					"\n",
					"When we create a view from a query against a path, we can reference this view in later queries. Here, we'll create a temporary view, but you can also create a permanent reference with regular view."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"CREATE OR REPLACE TEMP VIEW events_temp_view\n",
					"AS SELECT * FROM json.`${DA.paths.kafka_events}`;\n",
					"\n",
					"SELECT * FROM events_temp_view"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"0a627f4b-ec2c-4002-bf9b-07a788956f03\"/>\n",
					"\n",
					"\n",
					"## Extract Text Files as Raw Strings\n",
					"\n",
					"When working with text-based files (which include JSON, CSV, TSV, and TXT formats), you can use the **`text`** format to load each line of the file as a row with one string column named **`value`**. This can be useful when data sources are prone to corruption and custom text parsing functions will be used to extract value from text fields."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"SELECT * FROM text.`${DA.paths.kafka_events}`"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"ffae0f7a-b956-431d-b1cb-6d2be33b4f6c\"/>\n",
					"\n",
					"\n",
					"## Extract the Raw Bytes and Metadata of a File\n",
					"\n",
					"Some workflows may require working with entire files, such as when dealing with images or unstructured data. Using **`binaryFile`** to query a directory will provide file metadata alongside the binary representation of the file contents.\n",
					"\n",
					"Specifically, the fields created will indicate the **`path`**, **`modificationTime`**, **`length`**, and **`content`**."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"SELECT * FROM binaryFile.`${DA.paths.kafka_events}`"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md <i18n value=\"fa8fcc72-31c0-4825-ae6f-bf194d715f14\"/>\n",
					"\n",
					" \n",
					"Run the following cell to delete the tables and files associated with this lesson."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"DA.cleanup()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md-sandbox\n",
					"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
					"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
					"<br/>\n",
					"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
				],
				"execution_count": null
			}
		]
	}
}