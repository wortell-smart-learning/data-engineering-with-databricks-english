{
	"name": "DE 4-1 - Querying Files Directly",
	"properties": {
		"folder": {
			"name": "04 - ETL with Spark SQL"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "LakehouseTest",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "a6dc63e4-4658-4c94-a60a-8a050917db45"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_sparksql",
				"display_name": "sql"
			},
			"language_info": {
				"name": "sql"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/8aa90780-bc26-461d-9db6-6df705873871/resourceGroups/domo-demo/providers/Microsoft.Synapse/workspaces/domodemo/bigDataPools/LakehouseTest",
				"name": "LakehouseTest",
				"type": "Spark",
				"endpoint": "https://domodemo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/LakehouseTest",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.2",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"# Extracting Data Directly from Files\n",
					"\n",
					"In this notebook, you'll learn to extract data directly from files using Spark SQL on Databricks.\n",
					"\n",
					"A number of file formats support this option, but it is most useful for self-describing data formats (such as parquet and JSON).\n",
					"\n",
					"## Learning Objectives\n",
					"By the end of this lesson, you should be able to:\n",
					"- Use Spark SQL to directly query data files\n",
					"- Leverage **`text`** and **`binaryFile`** methods to review raw file contents"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"## Run Setup\n",
					"\n",
					"The setup script will create the data and declare necessary values for the rest of this notebook to execute."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run ../Includes/Classroom-Setup-04.1"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"## Data Overview\n",
					"\n",
					"In this example, we'll work with a sample of raw Kafka data written as JSON files. \n",
					"\n",
					"Each file contains all records consumed during a 5-second interval, stored with the full Kafka schema as a multiple-record JSON file.\n",
					"\n",
					"| field | type | description |\n",
					"| --- | --- | --- |\n",
					"| key | BINARY | The **`user_id`** field is used as the key; this is a unique alphanumeric field that corresponds to session/cookie information |\n",
					"| value | BINARY | This is the full data payload (to be discussed later), sent as JSON |\n",
					"| topic | STRING | While the Kafka service hosts multiple topics, only those records from the **`clickstream`** topic are included here |\n",
					"| partition | INTEGER | Our current Kafka implementation uses only 2 partitions (0 and 1) |\n",
					"| offset | LONG | This is a unique value, monotonically increasing for each partition |\n",
					"| timestamp | LONG | This timestamp is recorded as milliseconds since epoch, and represents the time at which the producer appends a record to a partition |"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"Note that our source directory contains many JSON files."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"print(DA.paths.kafka_events)\n",
					"\n",
					"files = dbutils.fs.ls(DA.paths.kafka_events)\n",
					"display(files)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"Here, we'll be using relative file paths to data that's been written to the DBFS root. \n",
					"\n",
					"Most workflows will require users to access data from external cloud storage locations. \n",
					"\n",
					"In most companies, a workspace administrator will be responsible for configuring access to these storage locations.\n",
					"\n",
					"Instructions for configuring and accessing these locations can be found in the cloud-vendor specific self-paced courses titled \"Cloud Architecture & Systems Integrations\"."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"## Query a Single File\n",
					"\n",
					"To query the data contained in a single file, execute the query with the following pattern:\n",
					"\n",
					"<strong><code>SELECT * FROM file_format.&#x60;/path/to/file&#x60;</code></strong>\n",
					"\n",
					"Make special note of the use of back-ticks (not single quotes) around the path."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"SELECT * FROM json.`${DA.paths.kafka_events}/001.json`"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"Note that our preview displays all 321 rows of our source file."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"## Query a Directory of Files\n",
					"\n",
					"Assuming all of the files in a directory have the same format and schema, all files can be queried simultaneously by specifying the directory path rather than an individual file."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"SELECT * FROM json.`${DA.paths.kafka_events}`"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"By default, this query will only show the first 1000 rows."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"## Create References to Files\n",
					"This ability to directly query files and directories means that additional Spark logic can be chained to queries against files.\n",
					"\n",
					"When we create a view from a query against a path, we can reference this view in later queries. Here, we'll create a temporary view, but you can also create a permanent reference with regular view."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"CREATE OR REPLACE TEMP VIEW events_temp_view\n",
					"AS SELECT * FROM json.`${DA.paths.kafka_events}`;\n",
					"\n",
					"SELECT * FROM events_temp_view"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"## Extract Text Files as Raw Strings\n",
					"\n",
					"When working with text-based files (which include JSON, CSV, TSV, and TXT formats), you can use the **`text`** format to load each line of the file as a row with one string column named **`value`**. This can be useful when data sources are prone to corruption and custom text parsing functions will be used to extract value from text fields."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"SELECT * FROM text.`${DA.paths.kafka_events}`"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"## Extract the Raw Bytes and Metadata of a File\n",
					"\n",
					"Some workflows may require working with entire files, such as when dealing with images or unstructured data. Using **`binaryFile`** to query a directory will provide file metadata alongside the binary representation of the file contents.\n",
					"\n",
					"Specifically, the fields created will indicate the **`path`**, **`modificationTime`**, **`length`**, and **`content`**."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"SELECT * FROM binaryFile.`${DA.paths.kafka_events}`"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"Run the following cell to delete the tables and files associated with this lesson."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"DA.cleanup()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
					"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
					"<br/>\n",
					"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
				]
			}
		]
	}
}