{
	"name": "Classroom-Setup-08-2-1L",
	"properties": {
		"folder": {
			"name": "includes"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "f19dfaeb-4a29-4e6e-800b-0a8de3eae1e3"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"%run ./_utility-methods"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def get_pipeline_config(self):\n",
					"    path = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None)\n",
					"    path = \"/\".join(path.split(\"/\")[:-1]) + \"/DE 8.2.2L - Migrating a SQL Pipeline to DLT Lab\"\n",
					"    \n",
					"    pipeline_name = f\"DLT-Lab-82L-{DA.username}\"\n",
					"    return pipeline_name, path"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def print_pipeline_config(self):\n",
					"    \"Provided by DBAcademy, this function renders the configuration of the pipeline as HTML\"\n",
					"    pipeline_name, path = self.get_pipeline_config()\n",
					"    \n",
					"    displayHTML(f\"\"\"<table style=\"width:100%\">\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Pipeline Name:</td>\n",
					"        <td><input type=\"text\" value=\"{pipeline_name}\" style=\"width:100%\"></td></tr>\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Target:</td>\n",
					"        <td><input type=\"text\" value=\"{DA.db_name}\" style=\"width:100%\"></td></tr>\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Storage Location:</td>\n",
					"        <td><input type=\"text\" value=\"{DA.paths.working_dir}/storage\" style=\"width:100%\"></td></tr>\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Notebook Path:</td>\n",
					"        <td><input type=\"text\" value=\"{path}\" style=\"width:100%\"></td>\n",
					"    </tr>\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Source:</td>\n",
					"        <td><input type=\"text\" value=\"{DA.paths.stream_path}\" style=\"width:100%\"></td>\n",
					"    <tr>\n",
					"        <td style=\"white-space:nowrap; width:1em\">Datasets Path:</td>\n",
					"        <td><input type=\"text\" value=\"{DA.paths.datasets}\" style=\"width:100%\"></td></tr>\n",
					"    </tr>\n",
					"    </table>\"\"\")\n",
					"    "
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def create_pipeline(self):\n",
					"    \"Provided by DBAcademy, this function creates the prescribed pipline\"\n",
					"    \n",
					"    pipeline_name, path = self.get_pipeline_config()\n",
					"\n",
					"    # We need to delete the existing pipline so that we can apply updates\n",
					"    # because some attributes are not mutable after creation.\n",
					"    self.client.pipelines().delete_by_name(pipeline_name)\n",
					"\n",
					"    response = self.client.pipelines().create(\n",
					"        name = pipeline_name, \n",
					"        storage = DA.paths.storage_location, \n",
					"        target = DA.db_name, \n",
					"        notebooks = [path],\n",
					"        configuration = {\n",
					"            \"source\": DA.paths.stream_path,\n",
					"            \"spark.master\": \"local[*]\",\n",
					"            \"datasets_path\": DA.paths.datasets,\n",
					"        },\n",
					"        clusters=[{ \"label\": \"default\", \"num_workers\": 0 }])\n",
					"\n",
					"    pipeline_id = response.get(\"pipeline_id\")\n",
					"    print(f\"Created pipline {pipeline_id}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def validate_pipeline_config(self):\n",
					"    \"Provided by DBAcademy, this function validates the configuration of the pipeline\"\n",
					"    import json\n",
					"    \n",
					"    pipeline_name, path = self.get_pipeline_config()\n",
					"\n",
					"    pipeline = self.client.pipelines().get_by_name(pipeline_name)\n",
					"    assert pipeline is not None, f\"The pipline named \\\"{pipeline_name}\\\" doesn't exist. Double check the spelling.\"\n",
					"\n",
					"    spec = pipeline.get(\"spec\")\n",
					"    \n",
					"    storage = spec.get(\"storage\")\n",
					"    assert storage == DA.paths.storage_location, f\"Invalid storage location. Found \\\"{storage}\\\", expected \\\"{DA.paths.storage_location}\\\" \"\n",
					"    \n",
					"    target = spec.get(\"target\")\n",
					"    assert target == DA.db_name, f\"Invalid target. Found \\\"{target}\\\", expected \\\"{DA.db_name}\\\" \"\n",
					"    \n",
					"    libraries = spec.get(\"libraries\")\n",
					"    assert libraries is None or len(libraries) > 0, f\"The notebook path must be specified.\"\n",
					"    assert len(libraries) == 1, f\"More than one library (e.g. notebook) was specified.\"\n",
					"    first_library = libraries[0]\n",
					"    assert first_library.get(\"notebook\") is not None, f\"Incorrect library configuration - expected a notebook.\"\n",
					"    first_library_path = first_library.get(\"notebook\").get(\"path\")\n",
					"    assert first_library_path == path, f\"Invalid notebook path. Found \\\"{first_library_path}\\\", expected \\\"{path}\\\" \"\n",
					"\n",
					"    configuration = spec.get(\"configuration\")\n",
					"    assert configuration is not None, f\"The two configuration parameters were not specified.\"\n",
					"    datasets_path = configuration.get(\"datasets_path\")\n",
					"    assert datasets_path == DA.paths.datasets, f\"Invalid datasets_path value. Found \\\"{datasets_path}\\\", expected \\\"{DA.paths.datasets}\\\".\"\n",
					"    spark_master = configuration.get(\"spark.master\")\n",
					"    assert spark_master == f\"local[*]\", f\"Invalid spark.master value. Expected \\\"local[*]\\\", found \\\"{spark_master}\\\".\"\n",
					"    config_source = configuration.get(\"source\")\n",
					"    assert config_source == DA.paths.stream_path, f\"Invalid source value. Expected \\\"{DA.paths.stream_path}\\\", found \\\"{config_source}\\\".\"\n",
					"    \n",
					"    cluster = spec.get(\"clusters\")[0]\n",
					"    autoscale = cluster.get(\"autoscale\")\n",
					"    assert autoscale is None, f\"Autoscaling should be disabled.\"\n",
					"    \n",
					"    num_workers = cluster.get(\"num_workers\")\n",
					"    assert num_workers == 0, f\"Expected the number of workers to be 0, found {num_workers}.\"\n",
					"\n",
					"    development = spec.get(\"development\")\n",
					"    assert development == True, f\"The pipline mode should be set to \\\"Development\\\".\"\n",
					"    \n",
					"    channel = spec.get(\"channel\")\n",
					"    assert channel is None or channel == \"CURRENT\", f\"Expected the channel to be Current but found {channel}.\"\n",
					"    \n",
					"    photon = spec.get(\"photon\")\n",
					"    assert photon == True, f\"Expected Photon to be enabled.\"\n",
					"    \n",
					"    continuous = spec.get(\"continuous\")\n",
					"    assert continuous == False, f\"Expected the Pipeline mode to be \\\"Triggered\\\", found \\\"Continuous\\\".\"\n",
					"\n",
					"    policy = self.client.cluster_policies.get_by_name(\"Student's DLT-Only Policy\")\n",
					"    if policy is not None:\n",
					"        cluster = { \n",
					"            \"num_workers\": 0,\n",
					"            \"label\": \"default\", \n",
					"            \"policy_id\": policy.get(\"policy_id\")\n",
					"        }\n",
					"        self.client.pipelines.create_or_update(name = pipeline_name,\n",
					"                                               storage = DA.paths.storage_location,\n",
					"                                               target = DA.db_name,\n",
					"                                               notebooks = [path],\n",
					"                                               configuration = {\n",
					"                                                   \"source\": DA.paths.stream_path,\n",
					"                                                   \"spark.master\": \"local[*]\",\n",
					"                                                   \"datasets_path\": DA.paths.datasets,\n",
					"                                               },\n",
					"                                               clusters=[cluster])\n",
					"    print(\"All tests passed!\")\n",
					"    "
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"@DBAcademyHelper.monkey_patch\n",
					"def start_pipeline(self):\n",
					"    \"Provided by DBAcademy, this function starts the pipline and then blocks until it has completed, failed or was canceled\"\n",
					"\n",
					"    import time\n",
					"    from dbacademy.dbrest import DBAcademyRestClient\n",
					"    self.client = DBAcademyRestClient()\n",
					"\n",
					"    pipeline_name, path = self.get_pipeline_config()\n",
					"\n",
					"    pipeline = self.client.pipelines().get_by_name(pipeline_name)\n",
					"    pipeline_id = pipeline.get(\"pipeline_id\")\n",
					"    \n",
					"    # Start the pipeline\n",
					"    start = self.client.pipelines().start_by_name(pipeline_name)\n",
					"    update_id = start.get(\"update_id\")\n",
					"\n",
					"    # Get the status and block until it is done\n",
					"    update = self.client.pipelines().get_update_by_id(pipeline_id, update_id)\n",
					"    state = update.get(\"update\").get(\"state\")\n",
					"\n",
					"    done = [\"COMPLETED\", \"FAILED\", \"CANCELED\"]\n",
					"    while state not in done:\n",
					"        duration = 15\n",
					"        time.sleep(duration)\n",
					"        print(f\"Current state is {state}, sleeping {duration} seconds.\")    \n",
					"        update = self.client.pipelines().get_update_by_id(pipeline_id, update_id)\n",
					"        state = update.get(\"update\").get(\"state\")\n",
					"    \n",
					"    print(f\"The final state is {state}.\")    \n",
					"    assert state == \"COMPLETED\", f\"Expected the state to be COMPLETED, found {state}\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"DA = DBAcademyHelper(lesson=\"dlt_lab_82\", **helper_arguments)\n",
					"DA.reset_environment() # First in a series\n",
					"DA.init(install_datasets=True, create_db=True)\n",
					"\n",
					"DA.paths.stream_path = f\"{DA.paths.working_dir}/stream\"\n",
					"DA.paths.storage_location = f\"{DA.paths.working_dir}/storage\"\n",
					"\n",
					"DA.data_factory = DltDataFactory(DA.paths.stream_path)\n",
					"DA.data_factory.load()\n",
					"\n",
					"DA.conclude_setup()\n",
					""
				],
				"execution_count": null
			}
		]
	}
}