{
	"name": "DE 3-1 - Databases and Tables on Databricks",
	"properties": {
		"folder": {
			"name": "03 - Relational Entities on Databricks"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "LakehouseTest",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "4",
				"spark.autotune.trackingId": "9eb5bd13-6d5a-454b-bf84-50484cdb463c"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_sparksql",
				"display_name": "sql"
			},
			"language_info": {
				"name": "sql"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/8aa90780-bc26-461d-9db6-6df705873871/resourceGroups/domo-demo/providers/Microsoft.Synapse/workspaces/domodemo/bigDataPools/LakehouseTest",
				"name": "LakehouseTest",
				"type": "Spark",
				"endpoint": "https://domodemo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/LakehouseTest",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.2",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"# Schemas and Tables on Databricks\n",
					"In this demonstration, you will create and explore schemas and tables.\n",
					"\n",
					"## Learning Objectives\n",
					"By the end of this lesson, you should be able to:\n",
					"* Use Spark SQL DDL to define schemas and tables\n",
					"* Describe how the **`LOCATION`** keyword impacts the default storage directory\n",
					"\n",
					"\n",
					"\n",
					"**Resources**\n",
					"* <a href=\"https://docs.databricks.com/user-guide/tables.html\" target=\"_blank\">Schemas and Tables - Databricks Docs</a>\n",
					"* <a href=\"https://docs.databricks.com/user-guide/tables.html#managed-and-unmanaged-tables\" target=\"_blank\">Managed and Unmanaged Tables</a>\n",
					"* <a href=\"https://docs.databricks.com/user-guide/tables.html#create-a-table-using-the-ui\" target=\"_blank\">Creating a Table with the UI</a>\n",
					"* <a href=\"https://docs.databricks.com/user-guide/tables.html#create-a-local-table\" target=\"_blank\">Create a Local Table</a>\n",
					"* <a href=\"https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#saving-to-persistent-tables\" target=\"_blank\">Saving to Persistent Tables</a>"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"## Lesson Setup\n",
					"The following script clears out previous runs of this demo and configures some Hive variables that will be used in our SQL queries."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run ../Includes/Classroom-Setup-03.1"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"## Using Hive Variables\n",
					"\n",
					"While not a pattern that is generally recommended in Spark SQL, this notebook will use some Hive variables to substitute in string values derived from the account email of the current user.\n",
					"\n",
					"The following cell demonstrates this pattern."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"SELECT \"${da.db_name}\" AS db_name,\n",
					"       \"${da.paths.working_dir}\" AS working_dir"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"Because you may be working in a shared workspace, this course uses variables derived from your username so the schemas don't conflict with other users. Again, consider this use of Hive variables a hack for our lesson environment rather than a good practice for development."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"## Schemas\n",
					"Let's start by creating two schemas:\n",
					"- One with no **`LOCATION`** specified\n",
					"- One with **`LOCATION`** specified"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"CREATE SCHEMA IF NOT EXISTS ${da.db_name}_default_location;\n",
					"CREATE SCHEMA IF NOT EXISTS ${da.db_name}_custom_location LOCATION '${da.paths.working_dir}/_custom_location.db';"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"Note that the location of the first schema is in the default location under **`dbfs:/user/hive/warehouse/`** and that the schema directory is the name of the schema with the **`.db`** extension"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"DESCRIBE SCHEMA EXTENDED ${da.db_name}_default_location;"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"Note that the location of the second schema is in the directory specified after the **`LOCATION`** keyword."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"DESCRIBE SCHEMA EXTENDED ${da.db_name}_custom_location;"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"We will create a table in the schema with default location and insert data. \n",
					"\n",
					"Note that the schema must be provided because there is no data from which to infer the schema."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"USE ${da.db_name}_default_location;\n",
					"\n",
					"CREATE OR REPLACE TABLE managed_table_in_db_with_default_location (width INT, length INT, height INT);\n",
					"INSERT INTO managed_table_in_db_with_default_location \n",
					"VALUES (3, 2, 1);\n",
					"SELECT * FROM managed_table_in_db_with_default_location;"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"We can look at the extended table description to find the location (you'll need to scroll down in the results)."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"DESCRIBE DETAIL managed_table_in_db_with_default_location;"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"By default, managed tables in a schema without the location specified will be created in the **`dbfs:/user/hive/warehouse/<schema_name>.db/`** directory.\n",
					"\n",
					"We can see that, as expected, the data and metadata for our Delta Table are stored in that location."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"hive_root =  f\"dbfs:/user/hive/warehouse\"\n",
					"db_name =    f\"{DA.db_name}_default_location.db\"\n",
					"table_name = f\"managed_table_in_db_with_default_location\"\n",
					"\n",
					"tbl_location = f\"{hive_root}/{db_name}/{table_name}\"\n",
					"print(tbl_location)\n",
					"\n",
					"files = dbutils.fs.ls(tbl_location)\n",
					"display(files)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"Drop the table."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"DROP TABLE managed_table_in_db_with_default_location;"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"Note the table's directory and its log and data files are deleted. Only the schema directory remains."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"\n",
					"db_location = f\"{hive_root}/{db_name}\"\n",
					"print(db_location)\n",
					"dbutils.fs.ls(db_location)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"We now create a table in the schema with custom location and insert data. \n",
					"\n",
					"Note that the schema must be provided because there is no data from which to infer the schema."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"USE ${da.db_name}_custom_location;\n",
					"\n",
					"CREATE OR REPLACE TABLE managed_table_in_db_with_custom_location (width INT, length INT, height INT);\n",
					"INSERT INTO managed_table_in_db_with_custom_location VALUES (3, 2, 1);\n",
					"SELECT * FROM managed_table_in_db_with_custom_location;"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"Again, we'll look at the description to find the table location."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"DESCRIBE DETAIL managed_table_in_db_with_custom_location;"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"As expected, this managed table is created in the path specified with the **`LOCATION`** keyword during schema creation. As such, the data and metadata for the table are persisted in a directory here."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"\n",
					"table_name = f\"managed_table_in_db_with_custom_location\"\n",
					"tbl_location =   f\"{DA.paths.working_dir}/_custom_location.db/{table_name}\"\n",
					"print(tbl_location)\n",
					"\n",
					"files = dbutils.fs.ls(tbl_location)\n",
					"display(files)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"Let's drop the table."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"DROP TABLE managed_table_in_db_with_custom_location;"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"Note the table's folder and the log file and data file are deleted.  \n",
					"  \n",
					"Only the schema location remains"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"\n",
					"db_location =   f\"{DA.paths.working_dir}/_custom_location.db\"\n",
					"print(db_location)\n",
					"\n",
					"dbutils.fs.ls(db_location)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"## Tables\n",
					"We will create an external (unmanaged) table from sample data. \n",
					"\n",
					"The data we are going to use are in CSV format. We want to create a Delta table with a **`LOCATION`** provided in the directory of our choice."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"USE ${da.db_name}_default_location;\n",
					"\n",
					"CREATE OR REPLACE TEMPORARY VIEW temp_delays USING CSV OPTIONS (\n",
					"  path = '${DA.paths.datasets}/flights/departuredelays.csv',\n",
					"  header = \"true\",\n",
					"  mode = \"FAILFAST\" -- abort file parsing with a RuntimeException if any malformed lines are encountered\n",
					");\n",
					"CREATE OR REPLACE TABLE external_table LOCATION '${da.paths.working_dir}/external_table' AS\n",
					"  SELECT * FROM temp_delays;\n",
					"\n",
					"SELECT * FROM external_table;"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"Let's note the location of the table's data in this lesson's working directory."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"DESCRIBE TABLE EXTENDED external_table;"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"Now, we drop the table."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"DROP TABLE external_table;"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"The table definition no longer exists in the metastore, but the underlying data remain intact."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"tbl_path = f\"{DA.paths.working_dir}/external_table\"\n",
					"files = dbutils.fs.ls(tbl_path)\n",
					"display(files)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"## Clean up\n",
					"Drop both schemas."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"DROP SCHEMA ${da.db_name}_default_location CASCADE;\n",
					"DROP SCHEMA ${da.db_name}_custom_location CASCADE;"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					" \n",
					"Run the following cell to delete the tables and files associated with this lesson."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"DA.cleanup()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
					"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
					"<br/>\n",
					"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
				]
			}
		]
	}
}