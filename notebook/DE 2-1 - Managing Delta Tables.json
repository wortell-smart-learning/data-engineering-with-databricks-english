{
	"name": "DE 2-1 - Managing Delta Tables",
	"properties": {
		"folder": {
			"name": "02 - Delta Lake"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "e8303011-05f2-470e-9624-e4e6fedb9291"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_sparksql",
				"display_name": "sql"
			},
			"language_info": {
				"name": "sql"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
 					"\n",
					"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
					"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
					"</div>"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"\n",
					"# Managing Delta Tables\n",
					"\n",
					"If you know any flavor of SQL, you already have much of the knowledge you'll need to work effectively in the data lakehouse.\n",
					"\n",
					"In this notebook, we'll explore basic manipulation of data and tables with SQL on Databricks.\n",
					"\n",
					"Note that Delta Lake is the default format for all tables created with Databricks; if you've been running SQL statements on Databricks, you're likely already working with Delta Lake.\n",
					"\n",
					"## Learning Objectives\n",
					"By the end of this lesson, you should be able to:\n",
					"* Create Delta Lake tables\n",
					"* Query data from Delta Lake tables\n",
					"* Insert, update, and delete records in Delta Lake tables\n",
					"* Write upsert statements with Delta Lake\n",
					"* Drop Delta Lake tables"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"## Run Setup\n",
					"The first thing we're going to do is run a setup script. It will define a username, userhome, and database that is scoped to each user."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%run ../Includes/Classroom-Setup-02.1"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"\n",
					"## Creating a Delta Table\n",
					"\n",
					"There's not much code you need to write to create a table with Delta Lake. There are a number of ways to create Delta Lake tables that we'll see throughout the course. We'll begin with one of the easiest methods: registering an empty Delta Lake table.\n",
					"\n",
					"We need: \n",
					"- A **`CREATE TABLE`** statement\n",
					"- A table name (below we use **`students`**)\n",
					"- A schema\n",
					"\n",
					"**NOTE:** In Databricks Runtime 8.0 and above, Delta Lake is the default format and you donâ€™t need **`USING DELTA`**."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"CREATE TABLE students\n",
					"  (id INT, name STRING, value DOUBLE);"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"\n",
					"If we try to go back and run that cell again...it will error out! This is expected - because the table exists already, we receive an error.\n",
					"\n",
					"We can add in an additional argument, **`IF NOT EXISTS`** which checks if the table exists. This will overcome our error."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"CREATE TABLE IF NOT EXISTS students \n",
					"  (id INT, name STRING, value DOUBLE)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"\n",
					"## Inserting Data\n",
					"Most often, data will be inserted to tables as the result of a query from another source.\n",
					"\n",
					"However, just as in standard SQL, you can also insert values directly, as shown here."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"INSERT INTO students VALUES (1, \"Yve\", 1.0);\n",
					"INSERT INTO students VALUES (2, \"Omar\", 2.5);\n",
					"INSERT INTO students VALUES (3, \"Elia\", 3.3);"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"\n",
					"In the cell above, we completed three separate **`INSERT`** statements. Each of these is processed as a separate transaction with its own ACID guarantees. Most frequently, we'll insert many records in a single transaction."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"INSERT INTO students\n",
					"VALUES \n",
					"  (4, \"Ted\", 4.7),\n",
					"  (5, \"Tiffany\", 5.5),\n",
					"  (6, \"Vini\", 6.3)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"\n",
					"Note that Databricks doesn't have a **`COMMIT`** keyword; transactions run as soon as they're executed, and commit as they succeed."
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"\n",
					"## Querying a Delta Table\n",
					"\n",
					"You probably won't be surprised that querying a Delta Lake table is as easy as using a standard **`SELECT`** statement."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"SELECT * FROM students"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"\n",
					"What may surprise you is that Delta Lake guarantees that any read against a table will **always** return the most recent version of the table, and that you'll never encounter a state of deadlock due to ongoing operations.\n",
					"\n",
					"To repeat: table reads can never conflict with other operations, and the newest version of your data is immediately available to all clients that can query your lakehouse. Because all transaction information is stored in cloud object storage alongside your data files, concurrent reads on Delta Lake tables is limited only by the hard limits of object storage on cloud vendors. (**NOTE**: It's not infinite, but it's at least thousands of reads per second.)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"\n",
					"## Updating Records\n",
					"\n",
					"Updating records provides atomic guarantees as well: we perform a snapshot read of the current version of our table, find all fields that match our **`WHERE`** clause, and then apply the changes as described.\n",
					"\n",
					"Below, we find all students that have a name starting with the letter **T** and add 1 to the number in their **`value`** column."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"UPDATE students \n",
					"SET value = value + 1\n",
					"WHERE name LIKE \"T%\""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"\n",
					"Query the table again to see these changes applied."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"SELECT * FROM students"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"\n",
					"## Deleting Records\n",
					"\n",
					"Deletes are also atomic, so there's no risk of only partially succeeding when removing data from your data lakehouse.\n",
					"\n",
					"A **`DELETE`** statement can remove one or many records, but will always result in a single transaction."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"DELETE FROM students \n",
					"WHERE value > 6"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"\n",
					"## Using Merge\n",
					"\n",
					"Some SQL systems have the concept of an upsert, which allows updates, inserts, and other data manipulations to be run as a single command.\n",
					"\n",
					"Databricks uses the **`MERGE`** keyword to perform this operation.\n",
					"\n",
					"Consider the following temporary view, which contains 4 records that might be output by a Change Data Capture (CDC) feed."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"CREATE OR REPLACE TEMP VIEW updates(id, name, value, type) AS VALUES\n",
					"  (2, \"Omar\", 15.2, \"update\"),\n",
					"  (3, \"\", null, \"delete\"),\n",
					"  (7, \"Blue\", 7.7, \"insert\"),\n",
					"  (11, \"Diya\", 8.8, \"update\");\n",
					"  \n",
					"SELECT * FROM updates;"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"\n",
					"Using the syntax we've seen so far, we could filter from this view by type to write 3 statements, one each to insert, update, and delete records. But this would result in 3 separate transactions; if any of these transactions were to fail, it might leave our data in an invalid state.\n",
					"\n",
					"Instead, we combine these actions into a single atomic transaction, applying all 3 types of changes together.\n",
					"\n",
					"**`MERGE`** statements must have at least one field to match on, and each **`WHEN MATCHED`** or **`WHEN NOT MATCHED`** clause can have any number of additional conditional statements.\n",
					"\n",
					"Here, we match on our **`id`** field and then filter on the **`type`** field to appropriately update, delete, or insert our records."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"MERGE INTO students b\n",
					"USING updates u\n",
					"ON b.id=u.id\n",
					"WHEN MATCHED AND u.type = \"update\"\n",
					"  THEN UPDATE SET *\n",
					"WHEN MATCHED AND u.type = \"delete\"\n",
					"  THEN DELETE\n",
					"WHEN NOT MATCHED AND u.type = \"insert\"\n",
					"  THEN INSERT *"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"\n",
					"Note that only 3 records were impacted by our **`MERGE`** statement; one of the records in our updates table did not have a matching **`id`** in the students table but was marked as an **`update`**. Based on our custom logic, we ignored this record rather than inserting it. \n",
					"\n",
					"How would you modify the above statement to include unmatched records marked **`update`** in the final **`INSERT`** clause?"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"\n",
					"## Dropping a Table\n",
					"\n",
					"Assuming that you have proper permissions on the target table, you can permanently delete data in the lakehouse using a **`DROP TABLE`** command.\n",
					"\n",
					"**NOTE**: Later in the course, we'll discuss Table Access Control Lists (ACLs) and default permissions. In a properly configured lakehouse, users should **not** be able to delete production tables."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"DROP TABLE students"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 
					"\n",
					"\n",
					"\n",
					"Run the following cell to delete the tables and files associated with this lesson."
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"DA.cleanup()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
"source": [
 					"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
					"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
					"<br/>\n",
					"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
				],
				"execution_count": null
			}
		]
	}
}