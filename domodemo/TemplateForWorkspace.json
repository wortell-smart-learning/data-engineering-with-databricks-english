{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "domodemo"
		},
		"domodemo-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'domodemo-WorkspaceDefaultSqlServer'"
		},
		"domodemo-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://domodemo.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/domodemo-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('domodemo-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/domodemo-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('domodemo-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-01-2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "LakehouseTest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "bf7688b8-d651-4a01-8e42-0543b03ffc3b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/8aa90780-bc26-461d-9db6-6df705873871/resourceGroups/domo-demo/providers/Microsoft.Synapse/workspaces/domodemo/bigDataPools/LakehouseTest",
						"name": "LakehouseTest",
						"type": "Spark",
						"endpoint": "https://domodemo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/LakehouseTest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run /Includes/_utility-methods"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def create_demo_tmp_vw(self):\n",
							"    print(\"\\nCreating the temp view \\\"demo_tmp_vw\\\"\")\n",
							"\n",
							"    spark.sql(\"\"\"\n",
							"        CREATE OR REPLACE TEMP VIEW demo_tmp_vw(name, value) AS VALUES\n",
							"        (\"Yi\", 1),\n",
							"        (\"Ali\", 2),\n",
							"        (\"Selina\", 3)\n",
							"        \"\"\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments) # Create the DA object\n",
							"DA.reset_environment()                   # Reset by removing databases and files from other lessons\n",
							"DA.init(install_datasets=True,           # Initialize, install and validate the datasets\n",
							"        create_db=False)                 # Continue initialization, create the user-db\n",
							"\n",
							"DA.create_demo_tmp_vw()                  # Create demo table\n",
							"\n",
							"DA.conclude_setup()                      # Conclude setup by advertising environmental changes\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-02-1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "e69a3036-a9fb-47b3-b98a-a7dd20732b4f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments) # Create the DA object\n",
							"DA.reset_environment()                   # Reset by removing databases and files from other lessons\n",
							"DA.init(install_datasets=True,           # Initialize, install and validate the datasets\n",
							"        create_db=True)                  # Continue initialization, create the user-db\n",
							"DA.conclude_setup()                      # Conclude setup by advertising environmental changes\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-02-2L')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "81dcfc9a-54ff-4878-a742-0e4613f23534"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments) # Create the DA object\n",
							"DA.reset_environment()                   # Reset by removing databases and files from other lessons\n",
							"DA.init(install_datasets=True,           # Initialize, install and validate the datasets\n",
							"        create_db=True)                  # Continue initialization, create the user-db\n",
							"DA.conclude_setup()                      # Conclude setup by advertising environmental changes\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-02-3')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "09080365-adbc-43e9-846d-d944d0d009ca"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments) # Create the DA object\n",
							"DA.reset_environment()                   # Reset by removing databases and files from other lessons\n",
							"DA.init(install_datasets=True,           # Initialize, install and validate the datasets\n",
							"        create_db=True)                  # Continue initialization, create the user-db\n",
							"DA.conclude_setup()                      # Conclude setup by advertising environmental changes\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-02-4L')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "57992eb4-308d-4022-b30f-0e8cfd189b31"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments) # Create the DA object\n",
							"DA.reset_environment()                   # Reset by removing databases and files from other lessons\n",
							"DA.init(install_datasets=True,           # Initialize, install and validate the datasets\n",
							"        create_db=True)                  # Continue initialization, create the user-db\n",
							"DA.conclude_setup()                      # Conclude setup by advertising environmental changes\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-03-1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "e0c9643a-32e5-4d06-b643-761b5d2425cc"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments) # Create the DA object\n",
							"DA.reset_environment()                   # Reset by removing databases and files from other lessons\n",
							"DA.init(install_datasets=True,           # Initialize, install and validate the datasets\n",
							"        create_db=False)                 # Continue initialization, create the user-db\n",
							"DA.conclude_setup()                      # Conclude setup by advertising environmental changes\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-03-2A')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "15d4e2ed-df4c-4d4b-83c8-d908016b2ed0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments) # Create the DA object\n",
							"DA.reset_environment()                   # Reset by removing databases and files from other lessons\n",
							"DA.init(install_datasets=True,           # Initialize, install and validate the datasets\n",
							"        create_db=True)                  # Continue initialization, create the user-db\n",
							"\n",
							"# Clean out the global_temp database.\n",
							"for row in spark.sql(\"SHOW TABLES IN global_temp\").select(\"tableName\").collect():\n",
							"    table_name = row[0]\n",
							"    spark.sql(f\"DROP TABLE global_temp.{table_name}\")\n",
							"\n",
							"DA.conclude_setup()                      # Conclude setup by advertising environmental changes\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-03-2B')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "4007b5ce-5bee-4775-9574-6b9aa886e535"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments) # Create the DA object\n",
							"\n",
							"if dbgems.is_job():\n",
							"    # Only execute this whenunder test.\n",
							"    DA.reset_environment()\n",
							"    DA.init(install_datasets=True, create_db=True)\n",
							"\n",
							"    print(\"Mocking global temp view\")\n",
							"    spark.sql(f\"\"\"USE {DA.db_name}\"\"\")\n",
							"    spark.sql(f\"\"\"CREATE TABLE external_table USING CSV OPTIONS (path='{DA.paths.datasets}/flights/departuredelays.csv', header=\"true\", mode=\"FAILFAST\");\"\"\")\n",
							"    spark.sql(f\"\"\"CREATE OR REPLACE GLOBAL TEMPORARY VIEW global_temp_view_dist_gt_1000 AS SELECT * FROM external_table WHERE distance > 1000;\"\"\")\n",
							"    \n",
							"else:\n",
							"    DA.init(install_datasets=True, create_db=False)\n",
							"    \n",
							"print()\n",
							"\n",
							"DA.conclude_setup()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-03-3L')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "450fb1ff-255c-4c9a-b462-03a5598f843e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments) # Create the DA object\n",
							"DA.reset_environment()                   # Reset by removing databases and files from other lessons\n",
							"DA.init(install_datasets=True,           # Initialize, install and validate the datasets\n",
							"        create_db=False)                 # Continue initialization, create the user-db\n",
							"DA.conclude_setup()                      # Conclude setup by advertising environmental changes\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-04-1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d6d2c20e-7fef-4610-aa2a-f722fc67f30c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments) # Create the DA object\n",
							"DA.reset_environment()                   # Reset by removing databases and files from other lessons\n",
							"DA.init(install_datasets=True,           # Initialize, install and validate the datasets\n",
							"        create_db=True)                  # Continue initialization, create the user-db\n",
							"\n",
							"DA.paths.kafka_events = f\"{DA.paths.datasets}/ecommerce/raw/events-kafka\"\n",
							"\n",
							"DA.conclude_setup()                      # Conclude setup by advertising environmental changes\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-04-2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "bd62e781-6b79-4f8b-957a-0468516bd5c4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments) # Create the DA object\n",
							"DA.reset_environment()                   # Reset by removing databases and files from other lessons\n",
							"DA.init(install_datasets=True,           # Initialize, install and validate the datasets\n",
							"        create_db=True)                  # Continue initialization, create the user-db"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"import os, time, shutil, sqlite3\n",
							"import pandas as pd\n",
							"\n",
							"# Create a user-specific copy of the sales-csv.\n",
							"DA.paths.sales_csv = f\"{DA.paths.working_dir}/sales-csv\"\n",
							"dbutils.fs.cp(f\"{DA.paths.datasets}/ecommerce/raw/sales-csv\", DA.paths.sales_csv, True)\n",
							"\n",
							"start = int(time.time())\n",
							"print(f\"Creating the users table\", end=\"...\")\n",
							"\n",
							"DA.paths.ecommerce_db = f\"{DA.paths.working_dir}/ecommerce.db\"\n",
							"datasource_path = f\"{DA.paths.datasets}/ecommerce/raw/users-historical\"\n",
							"\n",
							"# Create the temp directory and declare the path to the temp db file.\n",
							"db_temp_dir = f\"/tmp/{DA.username}\"\n",
							"dbutils.fs.mkdirs(f\"file:{db_temp_dir}\")\n",
							"db_temp_path = f\"{db_temp_dir}/ecommerce.db\"\n",
							"\n",
							"# Spark => JDBC cannot create the database reliably but Pandas can.\n",
							"conn = sqlite3.connect(db_temp_path) \n",
							"c = conn.cursor()\n",
							"c.execute('CREATE TABLE IF NOT EXISTS users (user_id string, user_first_touch_timestamp decimal(20,0), email string)')\n",
							"conn.commit()\n",
							"df = pd.read_parquet(path = datasource_path.replace(\"dbfs:/\", '/dbfs/'))\n",
							"df.to_sql('users', conn, if_exists='replace', index = False)\n",
							"\n",
							"# Move the temp db to the final location\n",
							"dbutils.fs.mv(f\"file:{db_temp_path}\", DA.paths.ecommerce_db)\n",
							"DA.paths.ecommerce_db = DA.paths.ecommerce_db.replace(\"dbfs:/\", \"/dbfs/\")\n",
							"\n",
							"# Report on the setup time.\n",
							"total = spark.read.parquet(datasource_path).count()\n",
							"print(f\"({int(time.time())-start} seconds / {total:,} records)\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA.conclude_setup()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-04-3')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ba710d84-ae74-46cc-965c-86a624fac0da"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments)\n",
							"DA.reset_environment()\n",
							"DA.init(install_datasets=True, create_db=True)\n",
							"DA.conclude_setup()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-04-4')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "dfa0cfb6-2c99-4af7-a657-01e2321e73f0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments)\n",
							"DA.reset_environment()\n",
							"DA.init(install_datasets=True, create_db=True)\n",
							"\n",
							"print()\n",
							"\n",
							"DA.clone_source_table(\"sales\", f\"{DA.paths.datasets}/ecommerce/delta\", \"sales_hist\")\n",
							"DA.clone_source_table(\"users\", f\"{DA.paths.datasets}/ecommerce/delta\", \"users_hist\")\n",
							"DA.clone_source_table(\"events\", f\"{DA.paths.datasets}/ecommerce/delta\", \"events_hist\")\n",
							"\n",
							"DA.clone_source_table(\"users_update\", f\"{DA.paths.datasets}/ecommerce/delta\")\n",
							"DA.clone_source_table(\"events_update\", f\"{DA.paths.datasets}/ecommerce/delta\")\n",
							"\n",
							"DA.conclude_setup()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-04-5L')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "aa4d1a4c-767f-4cda-aec9-41f5dd88745c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments)\n",
							"DA.reset_environment()\n",
							"DA.init(install_datasets=True, create_db=True)\n",
							"DA.conclude_setup()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-04-6')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a74bd9ae-0177-4053-9d58-a321e3b48078"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# lesson: Writing delta \n",
							"def create_eltwss_users_update():\n",
							"    import time\n",
							"    import pyspark.sql.functions as F\n",
							"    start = int(time.time())\n",
							"    print(f\"Creating the table \\\"users_dirty\\\"\", end=\"...\")\n",
							"\n",
							"    df = spark.createDataFrame(data=[(None, None, None, None), (None, None, None, None), (None, None, None, None)], \n",
							"                               schema=\"user_id: string, user_first_touch_timestamp: long, email:string, updated:timestamp\")\n",
							"    (spark.read\n",
							"          .parquet(f\"{DA.paths.datasets}/ecommerce/raw/users-30m\")\n",
							"          .withColumn(\"updated\", F.current_timestamp())\n",
							"          .union(df)\n",
							"          .write\n",
							"          .mode(\"overwrite\")\n",
							"          .saveAsTable(\"users_dirty\"))\n",
							"    \n",
							"    total = spark.read.table(\"users_dirty\").count()\n",
							"    print(f\"({int(time.time())-start} seconds / {total:,} records)\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments)\n",
							"DA.reset_environment()\n",
							"DA.init(install_datasets=True, create_db=True)\n",
							"\n",
							"print()\n",
							"create_eltwss_users_update()\n",
							"    \n",
							"DA.conclude_setup()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-04-7')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "923b5395-5602-4400-ac9f-d6343174db41"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments)\n",
							"DA.reset_environment()\n",
							"DA.init(install_datasets=True, create_db=True)\n",
							"\n",
							"print()\n",
							"\n",
							"DA.clone_source_table(\"sales\", f\"{DA.paths.datasets}/ecommerce/delta\", \"sales_hist\")\n",
							"DA.clone_source_table(\"events\", f\"{DA.paths.datasets}/ecommerce/delta\", \"events_hist\")\n",
							"\n",
							"DA.clone_source_table(\"events_raw\", f\"{DA.paths.datasets}/ecommerce/delta\")\n",
							"DA.clone_source_table(\"item_lookup\", f\"{DA.paths.datasets}/ecommerce/delta\")\n",
							"    \n",
							"DA.conclude_setup()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-04-8')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7b711815-23bc-4980-a151-8aeccfc15204"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments)\n",
							"DA.reset_environment()\n",
							"DA.init(install_datasets=True, create_db=True)\n",
							"DA.conclude_setup()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-04-9L')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "37717736-3211-4d7e-9fdf-4454a0b443a9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments)\n",
							"DA.reset_environment()\n",
							"DA.init(install_datasets=True, create_db=True)\n",
							"\n",
							"print()\n",
							"DA.clone_source_table(\"events\", f\"{DA.paths.datasets}/ecommerce/delta\")\n",
							"DA.clone_source_table(\"sales\", f\"{DA.paths.datasets}/ecommerce/delta\")\n",
							"DA.clone_source_table(\"users\", f\"{DA.paths.datasets}/ecommerce/delta\")\n",
							"DA.clone_source_table(\"transactions\", f\"{DA.paths.datasets}/ecommerce/delta\")\n",
							"\n",
							"DA.conclude_setup()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-05-3L')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "1ce6b808-01be-40ca-bad7-d50edb94b808"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments)\n",
							"DA.reset_environment()\n",
							"DA.init(install_datasets=True, create_db=False)\n",
							"DA.conclude_setup()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-06-1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "dffe6431-823b-436a-809e-d4dc567e0e6f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"class DataFactory:\n",
							"    def __init__(self, ):\n",
							"        self.source = f\"{DA.paths.datasets}/healthcare/tracker/streaming\"\n",
							"        self.userdir = f\"{DA.paths.working_dir}/tracker\"\n",
							"        self.curr_mo = 1\n",
							"    \n",
							"    def load(self, continuous=False):\n",
							"        if self.curr_mo > 12:\n",
							"            print(\"Data source exhausted\\n\")\n",
							"            \n",
							"        elif continuous == True:\n",
							"            while self.curr_mo <= 12:\n",
							"                curr_file = f\"{self.curr_mo:02}.json\"\n",
							"                dbutils.fs.cp(self.source + curr_file, self.userdir + curr_file)\n",
							"                self.curr_mo += 1\n",
							"        else:\n",
							"            curr_file = f\"{str(self.curr_mo).zfill(2)}.json\"\n",
							"            target_dir = f\"{self.userdir}/{curr_file}\"\n",
							"            print(f\"Loading the file {curr_file} to the tracker dataset\")\n",
							"            dbutils.fs.cp(f\"{self.source}/{curr_file}\", target_dir)\n",
							"            self.curr_mo += 1"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments)\n",
							"DA.reset_environment()\n",
							"DA.init(install_datasets=True, create_db=True)\n",
							"\n",
							"DA.data_factory = DataFactory()\n",
							"print()\n",
							"\n",
							"DA.data_factory.load()\n",
							"DA.conclude_setup()\n",
							"\n",
							"sqlContext.setConf(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-06-2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "3dd58c4f-e3e6-4fad-9ec5-39bae4d87e96"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"def autoload_to_table(data_source, source_format, table_name, checkpoint_directory):\n",
							"    (spark.readStream\n",
							"        .format(\"cloudFiles\")\n",
							"        .option(\"cloudFiles.format\", source_format)\n",
							"        .option(\"cloudFiles.schemaLocation\", checkpoint_directory)\n",
							"        .load(data_source)\n",
							"        .writeStream\n",
							"        .option(\"checkpointLocation\", checkpoint_directory)\n",
							"        .option(\"mergeSchema\", \"true\")\n",
							"        .trigger(availableNow=True)\n",
							"        .table(table_name)\n",
							"        .awaitTermination()\n",
							"    )"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"class DataFactory:\n",
							"    def __init__(self):\n",
							"        self.source = f\"{DA.paths.datasets}/healthcare/tracker/streaming\"\n",
							"        self.userdir = f\"{DA.paths.working_dir}/tracker\"\n",
							"        self.curr_mo = 1\n",
							"    \n",
							"    def load(self, continuous=False):\n",
							"        if self.curr_mo > 12:\n",
							"            print(\"Data source exhausted\\n\")\n",
							"            \n",
							"        elif continuous == True:\n",
							"            while self.curr_mo <= 12:\n",
							"                curr_file = f\"{self.curr_mo:02}.json\"\n",
							"                dbutils.fs.cp(f\"{self.source}/{curr_file}\", f\"{self.userdir}/{curr_file}\")\n",
							"                self.curr_mo += 1\n",
							"                autoload_to_table(self.userdir, \"json\", \"bronze\", f\"{DA.paths.checkpoints}/bronze\")\n",
							"        else:\n",
							"            curr_file = f\"{str(self.curr_mo).zfill(2)}.json\"\n",
							"            target_dir = f\"{self.userdir}/{curr_file}\"\n",
							"            print(f\"Loading the file {curr_file} to the {target_dir}\")\n",
							"            \n",
							"            dbutils.fs.cp(f\"{self.source}/{curr_file}\", f\"{self.userdir}/{curr_file}\")\n",
							"            self.curr_mo += 1\n",
							"            autoload_to_table(self.userdir, \"json\", \"bronze\", f\"{DA.paths.checkpoints}/bronze\")\n",
							"        "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments)\n",
							"DA.reset_environment()\n",
							"DA.init(install_datasets=True, create_db=True)\n",
							"\n",
							"DA.data_factory = DataFactory()\n",
							"print()\n",
							"\n",
							"DA.data_factory.load()\n",
							"DA.conclude_setup()\n",
							"\n",
							"sqlContext.setConf(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-06-3L')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9f89febc-e877-4e76-b248-00fcf3b9b7f3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments)\n",
							"DA.reset_environment()\n",
							"DA.init(install_datasets=True, create_db=True)\n",
							"DA.conclude_setup()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"sqlContext.setConf(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-07-1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "eb55fbc1-0011-463e-ba82-e8fa6342f9a1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"class DataFactory:\n",
							"    def __init__(self):\n",
							"        self.source = f\"{DA.paths.datasets}/healthcare/tracker/streaming\"\n",
							"        self.userdir = DA.paths.data_landing_location\n",
							"        self.curr_mo = 1\n",
							"    \n",
							"    def load(self, continuous=False):\n",
							"        if self.curr_mo > 12:\n",
							"            print(\"Data source exhausted\\n\")\n",
							"        elif continuous == True:\n",
							"            while self.curr_mo <= 12:\n",
							"                curr_file = f\"{self.curr_mo:02}.json\"\n",
							"                target_dir = f\"{self.userdir}/{curr_file}\"\n",
							"                print(f\"Loading the file {curr_file} to the {target_dir}\")\n",
							"                dbutils.fs.cp(f\"{self.source}/{curr_file}\", target_dir)\n",
							"                self.curr_mo += 1\n",
							"        else:\n",
							"            curr_file = f\"{str(self.curr_mo).zfill(2)}.json\"\n",
							"            target_dir = f\"{self.userdir}/{curr_file}\"\n",
							"            print(f\"Loading the file {curr_file} to the {target_dir}\")\n",
							"\n",
							"            dbutils.fs.cp(f\"{self.source}/{curr_file}\", target_dir)\n",
							"            self.curr_mo += 1\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments)\n",
							"DA.reset_environment()\n",
							"DA.init(install_datasets=True, create_db=True)\n",
							"\n",
							"DA.paths.data_landing_location    = f\"{DA.paths.working_dir}/source/tracker\"\n",
							"\n",
							"DA.data_factory = DataFactory()\n",
							"DA.conclude_setup()\n",
							"\n",
							"sqlContext.setConf(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-07-2L')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "6ca27cb3-2a2f-419e-9dfc-8e4cfdf01655"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments)\n",
							"DA.reset_environment()\n",
							"DA.init(install_datasets=True, create_db=True)\n",
							"DA.conclude_setup()\n",
							"\n",
							"sqlContext.setConf(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-08-1-1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d49db479-59a8-42d2-be33-1b64b5b6ca9f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def get_pipeline_config(self):\n",
							"    path = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None)\n",
							"    path = \"/\".join(path.split(\"/\")[:-1]) + \"/DE 8.1.2 - SQL for Delta Live Tables\"\n",
							"    \n",
							"    return f\"DLT-Demo-81-{DA.username}\", path"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def print_pipeline_config(self):\n",
							"    \"Provided by DBAcademy, this function renders the configuration of the pipeline as HTML\"\n",
							"    pipeline_name, path = self.get_pipeline_config()\n",
							"    \n",
							"    displayHTML(f\"\"\"<table style=\"width:100%\">\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Pipeline Name:</td>\n",
							"        <td><input type=\"text\" value=\"{pipeline_name}\" style=\"width:100%\"></td></tr>\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Target:</td>\n",
							"        <td><input type=\"text\" value=\"{DA.db_name}\" style=\"width:100%\"></td></tr>\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Storage Location:</td>\n",
							"        <td><input type=\"text\" value=\"{DA.paths.storage_location}\" style=\"width:100%\"></td></tr>\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Notebook Path:</td>\n",
							"        <td><input type=\"text\" value=\"{path}\" style=\"width:100%\"></td></tr>\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Datasets Path:</td>\n",
							"        <td><input type=\"text\" value=\"{DA.paths.datasets}\" style=\"width:100%\"></td></tr>\n",
							"    </table>\"\"\")\n",
							"    "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def create_pipeline(self):\n",
							"    \"Provided by DBAcademy, this function creates the prescribed pipline\"\n",
							"    \n",
							"    pipeline_name, path = self.get_pipeline_config()\n",
							"\n",
							"    # We need to delete the existing pipline so that we can apply updates\n",
							"    # because some attributes are not mutable after creation.\n",
							"    self.client.pipelines().delete_by_name(pipeline_name)\n",
							"    \n",
							"    response = self.client.pipelines().create(\n",
							"        name = pipeline_name, \n",
							"        storage = DA.paths.storage_location, \n",
							"        target = DA.db_name, \n",
							"        notebooks = [path],\n",
							"        configuration = {\n",
							"            \"spark.master\": \"local[*]\",\n",
							"            \"datasets_path\": DA.paths.datasets,\n",
							"        },\n",
							"        clusters=[{ \"label\": \"default\", \"num_workers\": 0 }])\n",
							"    \n",
							"    pipeline_id = response.get(\"pipeline_id\")\n",
							"    print(f\"Created pipline {pipeline_id}\")\n",
							"    "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def validate_pipeline_config(self):\n",
							"    \"Provided by DBAcademy, this function validates the configuration of the pipeline\"\n",
							"    import json\n",
							"    \n",
							"    pipeline_name, path = self.get_pipeline_config()\n",
							"\n",
							"    pipeline = self.client.pipelines().get_by_name(pipeline_name)\n",
							"    assert pipeline is not None, f\"The pipline named \\\"{pipeline_name}\\\" doesn't exist. Double check the spelling.\"\n",
							"\n",
							"    spec = pipeline.get(\"spec\")\n",
							"    \n",
							"    storage = spec.get(\"storage\")\n",
							"    assert storage == DA.paths.storage_location, f\"Invalid storage location. Found \\\"{storage}\\\", expected \\\"{DA.paths.storage_location}\\\" \"\n",
							"    \n",
							"    target = spec.get(\"target\")\n",
							"    assert target == DA.db_name, f\"Invalid target. Found \\\"{target}\\\", expected \\\"{DA.db_name}\\\" \"\n",
							"    \n",
							"    libraries = spec.get(\"libraries\")\n",
							"    assert libraries is None or len(libraries) > 0, f\"The notebook path must be specified.\"\n",
							"    assert len(libraries) == 1, f\"More than one library (e.g. notebook) was specified.\"\n",
							"    first_library = libraries[0]\n",
							"    assert first_library.get(\"notebook\") is not None, f\"Incorrect library configuration - expected a notebook.\"\n",
							"    first_library_path = first_library.get(\"notebook\").get(\"path\")\n",
							"    assert first_library_path == path, f\"Invalid notebook path. Found \\\"{first_library_path}\\\", expected \\\"{path}\\\" \"\n",
							"\n",
							"    configuration = spec.get(\"configuration\")\n",
							"    assert configuration is not None, f\"The two configuration parameters were not specified.\"\n",
							"    datasets_path = configuration.get(\"datasets_path\")\n",
							"    assert datasets_path == DA.paths.datasets, f\"Invalid datasets_path value. Found \\\"{datasets_path}\\\", expected \\\"{DA.paths.datasets}\\\".\"\n",
							"    spark_master = configuration.get(\"spark.master\")\n",
							"    assert spark_master == f\"local[*]\", f\"Invalid spark.master value. Expected \\\"local[*]\\\", found \\\"{spark_master}\\\".\"\n",
							"    \n",
							"    cluster = spec.get(\"clusters\")[0]\n",
							"    autoscale = cluster.get(\"autoscale\")\n",
							"    assert autoscale is None, f\"Autoscaling should be disabled.\"\n",
							"    \n",
							"    num_workers = cluster.get(\"num_workers\")\n",
							"    assert num_workers == 0, f\"Expected the number of workers to be 0, found {num_workers}.\"\n",
							"\n",
							"    development = spec.get(\"development\")\n",
							"    assert development == True, f\"The pipline mode should be set to \\\"Development\\\".\"\n",
							"    \n",
							"    channel = spec.get(\"channel\")\n",
							"    assert channel is None or channel == \"CURRENT\", f\"Expected the channel to be Current but found {channel}.\"\n",
							"    \n",
							"    photon = spec.get(\"photon\")\n",
							"    assert photon == True, f\"Expected Photon to be enabled.\"\n",
							"    \n",
							"    continuous = spec.get(\"continuous\")\n",
							"    assert continuous == False, f\"Expected the Pipeline mode to be \\\"Triggered\\\", found \\\"Continuous\\\".\"\n",
							"\n",
							"    policy = self.client.cluster_policies.get_by_name(\"Student's DLT-Only Policy\")\n",
							"    if policy is not None:\n",
							"        cluster = { \n",
							"            \"num_workers\": 0,\n",
							"            \"label\": \"default\", \n",
							"            \"policy_id\": policy.get(\"policy_id\")\n",
							"        }\n",
							"        self.client.pipelines.create_or_update(name = pipeline_name,\n",
							"                                               storage = DA.paths.storage_location,\n",
							"                                               target = DA.db_name,\n",
							"                                               notebooks = [path],\n",
							"                                               configuration = {\n",
							"                                                   \"spark.master\": \"local[*]\",\n",
							"                                                   \"datasets_path\": DA.paths.datasets,\n",
							"                                               },\n",
							"                                               clusters=[cluster])\n",
							"    print(\"All tests passed!\")\n",
							"    "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def start_pipeline(self):\n",
							"    \"Provided by DBAcademy, this function starts the pipline and then blocks until it has completed, failed or was canceled\"\n",
							"\n",
							"    import time\n",
							"    from dbacademy.dbrest import DBAcademyRestClient\n",
							"    self.client = DBAcademyRestClient()\n",
							"\n",
							"    pipeline_name, path = self.get_pipeline_config()\n",
							"\n",
							"    pipeline = self.client.pipelines().get_by_name(pipeline_name)\n",
							"    pipeline_id = pipeline.get(\"pipeline_id\")\n",
							"    \n",
							"    # Start the pipeline\n",
							"    start = self.client.pipelines().start_by_name(pipeline_name)\n",
							"    update_id = start.get(\"update_id\")\n",
							"\n",
							"    # Get the status and block until it is done\n",
							"    update = self.client.pipelines().get_update_by_id(pipeline_id, update_id)\n",
							"    state = update.get(\"update\").get(\"state\")\n",
							"\n",
							"    done = [\"COMPLETED\", \"FAILED\", \"CANCELED\"]\n",
							"    while state not in done:\n",
							"        duration = 15\n",
							"        time.sleep(duration)\n",
							"        print(f\"Current state is {state}, sleeping {duration} seconds.\")    \n",
							"        update = self.client.pipelines().get_update_by_id(pipeline_id, update_id)\n",
							"        state = update.get(\"update\").get(\"state\")\n",
							"    \n",
							"    print(f\"The final state is {state}.\")    \n",
							"    assert state == \"COMPLETED\", f\"Expected the state to be COMPLETED, found {state}\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(lesson=\"dlt_demo_81\", **helper_arguments)\n",
							"DA.reset_environment() # First in a series\n",
							"DA.init(install_datasets=True, create_db=True)\n",
							"\n",
							"DA.paths.stream_path = f\"{DA.paths.working_dir}/stream\"\n",
							"DA.paths.storage_location = f\"{DA.paths.working_dir}/storage\"\n",
							"\n",
							"DA.data_factory = DltDataFactory(DA.paths.stream_path)\n",
							"\n",
							"DA.conclude_setup()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-08-1-3')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "3a1e0274-228a-4fa8-9a97-eba8f4638610"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Continues where 8.1.1 picks up, don't remove assets\n",
							"DA = DBAcademyHelper(lesson=\"dlt_demo_81\", **helper_arguments)\n",
							"# DA.reset_environment()  # We don't want to reset the environment\n",
							"DA.init(install_datasets=True, create_db=True)\n",
							"\n",
							"DA.paths.stream_path = f\"{DA.paths.working_dir}/stream\"\n",
							"DA.paths.storage_location = f\"{DA.paths.working_dir}/storage\"\n",
							"\n",
							"DA.data_factory = DltDataFactory(DA.paths.stream_path)\n",
							"\n",
							"DA.conclude_setup()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-08-2-1L')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "f19dfaeb-4a29-4e6e-800b-0a8de3eae1e3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def get_pipeline_config(self):\n",
							"    path = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None)\n",
							"    path = \"/\".join(path.split(\"/\")[:-1]) + \"/DE 8.2.2L - Migrating a SQL Pipeline to DLT Lab\"\n",
							"    \n",
							"    pipeline_name = f\"DLT-Lab-82L-{DA.username}\"\n",
							"    return pipeline_name, path"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def print_pipeline_config(self):\n",
							"    \"Provided by DBAcademy, this function renders the configuration of the pipeline as HTML\"\n",
							"    pipeline_name, path = self.get_pipeline_config()\n",
							"    \n",
							"    displayHTML(f\"\"\"<table style=\"width:100%\">\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Pipeline Name:</td>\n",
							"        <td><input type=\"text\" value=\"{pipeline_name}\" style=\"width:100%\"></td></tr>\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Target:</td>\n",
							"        <td><input type=\"text\" value=\"{DA.db_name}\" style=\"width:100%\"></td></tr>\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Storage Location:</td>\n",
							"        <td><input type=\"text\" value=\"{DA.paths.working_dir}/storage\" style=\"width:100%\"></td></tr>\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Notebook Path:</td>\n",
							"        <td><input type=\"text\" value=\"{path}\" style=\"width:100%\"></td>\n",
							"    </tr>\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Source:</td>\n",
							"        <td><input type=\"text\" value=\"{DA.paths.stream_path}\" style=\"width:100%\"></td>\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Datasets Path:</td>\n",
							"        <td><input type=\"text\" value=\"{DA.paths.datasets}\" style=\"width:100%\"></td></tr>\n",
							"    </tr>\n",
							"    </table>\"\"\")\n",
							"    "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def create_pipeline(self):\n",
							"    \"Provided by DBAcademy, this function creates the prescribed pipline\"\n",
							"    \n",
							"    pipeline_name, path = self.get_pipeline_config()\n",
							"\n",
							"    # We need to delete the existing pipline so that we can apply updates\n",
							"    # because some attributes are not mutable after creation.\n",
							"    self.client.pipelines().delete_by_name(pipeline_name)\n",
							"\n",
							"    response = self.client.pipelines().create(\n",
							"        name = pipeline_name, \n",
							"        storage = DA.paths.storage_location, \n",
							"        target = DA.db_name, \n",
							"        notebooks = [path],\n",
							"        configuration = {\n",
							"            \"source\": DA.paths.stream_path,\n",
							"            \"spark.master\": \"local[*]\",\n",
							"            \"datasets_path\": DA.paths.datasets,\n",
							"        },\n",
							"        clusters=[{ \"label\": \"default\", \"num_workers\": 0 }])\n",
							"\n",
							"    pipeline_id = response.get(\"pipeline_id\")\n",
							"    print(f\"Created pipline {pipeline_id}\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def validate_pipeline_config(self):\n",
							"    \"Provided by DBAcademy, this function validates the configuration of the pipeline\"\n",
							"    import json\n",
							"    \n",
							"    pipeline_name, path = self.get_pipeline_config()\n",
							"\n",
							"    pipeline = self.client.pipelines().get_by_name(pipeline_name)\n",
							"    assert pipeline is not None, f\"The pipline named \\\"{pipeline_name}\\\" doesn't exist. Double check the spelling.\"\n",
							"\n",
							"    spec = pipeline.get(\"spec\")\n",
							"    \n",
							"    storage = spec.get(\"storage\")\n",
							"    assert storage == DA.paths.storage_location, f\"Invalid storage location. Found \\\"{storage}\\\", expected \\\"{DA.paths.storage_location}\\\" \"\n",
							"    \n",
							"    target = spec.get(\"target\")\n",
							"    assert target == DA.db_name, f\"Invalid target. Found \\\"{target}\\\", expected \\\"{DA.db_name}\\\" \"\n",
							"    \n",
							"    libraries = spec.get(\"libraries\")\n",
							"    assert libraries is None or len(libraries) > 0, f\"The notebook path must be specified.\"\n",
							"    assert len(libraries) == 1, f\"More than one library (e.g. notebook) was specified.\"\n",
							"    first_library = libraries[0]\n",
							"    assert first_library.get(\"notebook\") is not None, f\"Incorrect library configuration - expected a notebook.\"\n",
							"    first_library_path = first_library.get(\"notebook\").get(\"path\")\n",
							"    assert first_library_path == path, f\"Invalid notebook path. Found \\\"{first_library_path}\\\", expected \\\"{path}\\\" \"\n",
							"\n",
							"    configuration = spec.get(\"configuration\")\n",
							"    assert configuration is not None, f\"The two configuration parameters were not specified.\"\n",
							"    datasets_path = configuration.get(\"datasets_path\")\n",
							"    assert datasets_path == DA.paths.datasets, f\"Invalid datasets_path value. Found \\\"{datasets_path}\\\", expected \\\"{DA.paths.datasets}\\\".\"\n",
							"    spark_master = configuration.get(\"spark.master\")\n",
							"    assert spark_master == f\"local[*]\", f\"Invalid spark.master value. Expected \\\"local[*]\\\", found \\\"{spark_master}\\\".\"\n",
							"    config_source = configuration.get(\"source\")\n",
							"    assert config_source == DA.paths.stream_path, f\"Invalid source value. Expected \\\"{DA.paths.stream_path}\\\", found \\\"{config_source}\\\".\"\n",
							"    \n",
							"    cluster = spec.get(\"clusters\")[0]\n",
							"    autoscale = cluster.get(\"autoscale\")\n",
							"    assert autoscale is None, f\"Autoscaling should be disabled.\"\n",
							"    \n",
							"    num_workers = cluster.get(\"num_workers\")\n",
							"    assert num_workers == 0, f\"Expected the number of workers to be 0, found {num_workers}.\"\n",
							"\n",
							"    development = spec.get(\"development\")\n",
							"    assert development == True, f\"The pipline mode should be set to \\\"Development\\\".\"\n",
							"    \n",
							"    channel = spec.get(\"channel\")\n",
							"    assert channel is None or channel == \"CURRENT\", f\"Expected the channel to be Current but found {channel}.\"\n",
							"    \n",
							"    photon = spec.get(\"photon\")\n",
							"    assert photon == True, f\"Expected Photon to be enabled.\"\n",
							"    \n",
							"    continuous = spec.get(\"continuous\")\n",
							"    assert continuous == False, f\"Expected the Pipeline mode to be \\\"Triggered\\\", found \\\"Continuous\\\".\"\n",
							"\n",
							"    policy = self.client.cluster_policies.get_by_name(\"Student's DLT-Only Policy\")\n",
							"    if policy is not None:\n",
							"        cluster = { \n",
							"            \"num_workers\": 0,\n",
							"            \"label\": \"default\", \n",
							"            \"policy_id\": policy.get(\"policy_id\")\n",
							"        }\n",
							"        self.client.pipelines.create_or_update(name = pipeline_name,\n",
							"                                               storage = DA.paths.storage_location,\n",
							"                                               target = DA.db_name,\n",
							"                                               notebooks = [path],\n",
							"                                               configuration = {\n",
							"                                                   \"source\": DA.paths.stream_path,\n",
							"                                                   \"spark.master\": \"local[*]\",\n",
							"                                                   \"datasets_path\": DA.paths.datasets,\n",
							"                                               },\n",
							"                                               clusters=[cluster])\n",
							"    print(\"All tests passed!\")\n",
							"    "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def start_pipeline(self):\n",
							"    \"Provided by DBAcademy, this function starts the pipline and then blocks until it has completed, failed or was canceled\"\n",
							"\n",
							"    import time\n",
							"    from dbacademy.dbrest import DBAcademyRestClient\n",
							"    self.client = DBAcademyRestClient()\n",
							"\n",
							"    pipeline_name, path = self.get_pipeline_config()\n",
							"\n",
							"    pipeline = self.client.pipelines().get_by_name(pipeline_name)\n",
							"    pipeline_id = pipeline.get(\"pipeline_id\")\n",
							"    \n",
							"    # Start the pipeline\n",
							"    start = self.client.pipelines().start_by_name(pipeline_name)\n",
							"    update_id = start.get(\"update_id\")\n",
							"\n",
							"    # Get the status and block until it is done\n",
							"    update = self.client.pipelines().get_update_by_id(pipeline_id, update_id)\n",
							"    state = update.get(\"update\").get(\"state\")\n",
							"\n",
							"    done = [\"COMPLETED\", \"FAILED\", \"CANCELED\"]\n",
							"    while state not in done:\n",
							"        duration = 15\n",
							"        time.sleep(duration)\n",
							"        print(f\"Current state is {state}, sleeping {duration} seconds.\")    \n",
							"        update = self.client.pipelines().get_update_by_id(pipeline_id, update_id)\n",
							"        state = update.get(\"update\").get(\"state\")\n",
							"    \n",
							"    print(f\"The final state is {state}.\")    \n",
							"    assert state == \"COMPLETED\", f\"Expected the state to be COMPLETED, found {state}\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(lesson=\"dlt_lab_82\", **helper_arguments)\n",
							"DA.reset_environment() # First in a series\n",
							"DA.init(install_datasets=True, create_db=True)\n",
							"\n",
							"DA.paths.stream_path = f\"{DA.paths.working_dir}/stream\"\n",
							"DA.paths.storage_location = f\"{DA.paths.working_dir}/storage\"\n",
							"\n",
							"DA.data_factory = DltDataFactory(DA.paths.stream_path)\n",
							"DA.data_factory.load()\n",
							"\n",
							"DA.conclude_setup()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-08-2-3L')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "fea8363f-3886-4e64-8e57-8e855ec8aa72"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(lesson=\"dlt_lab_82\", **helper_arguments)\n",
							"# DA.reset_environment()  # We don't want to reset the environment\n",
							"DA.init(install_datasets=True, create_db=True)\n",
							"\n",
							"DA.paths.stream_path = f\"{DA.paths.working_dir}/stream\"\n",
							"DA.paths.storage_location = f\"{DA.paths.working_dir}/storage\"\n",
							"\n",
							"DA.data_factory = DltDataFactory(DA.paths.stream_path)\n",
							"\n",
							"DA.conclude_setup()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-09-1-1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b1197750-9320-4f41-aaaf-2ec77110ee65"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def get_pipeline_config(self):\n",
							"    path = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None)\n",
							"    path = \"/\".join(path.split(\"/\")[:-1]) + \"/DE 9.1.3 - DLT Job\"\n",
							"    \n",
							"    pipeline_name = f\"DLT-Job-Demo-91-{DA.username}\"\n",
							"    \n",
							"    return pipeline_name, path\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def print_pipeline_config(self):\n",
							"    \"Provided by DBAcademy, this function renders the configuration of the pipeline as HTML\"\n",
							"    pipeline_name, path = self.get_pipeline_config()\n",
							"\n",
							"    displayHTML(f\"\"\"<table style=\"width:100%\">\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Pipeline Name:</td>\n",
							"        <td><input type=\"text\" value=\"{pipeline_name}\" style=\"width:100%\"></td></tr>\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Target:</td>\n",
							"        <td><input type=\"text\" value=\"{DA.db_name}\" style=\"width:100%\"></td></tr>\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Storage Location:</td>\n",
							"        <td><input type=\"text\" value=\"{DA.paths.storage_location}\" style=\"width:100%\"></td></tr>\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Notebook Path:</td>\n",
							"        <td><input type=\"text\" value=\"{path}\" style=\"width:100%\"></td></tr>\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Datsets Path:</td>\n",
							"        <td><input type=\"text\" value=\"{DA.paths.datasets}\" style=\"width:100%\"></td></tr>\n",
							"    \n",
							"    </table>\"\"\")\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def create_pipeline(self):\n",
							"    \"Provided by DBAcademy, this function creates the prescribed pipline\"\n",
							"    \n",
							"    pipeline_name, path = self.get_pipeline_config()\n",
							"\n",
							"    # We need to delete the existing pipline so that we can apply updates\n",
							"    # because some attributes are not mutable after creation.\n",
							"    self.client.pipelines().delete_by_name(pipeline_name)\n",
							"    \n",
							"    response = self.client.pipelines().create(\n",
							"        name = pipeline_name, \n",
							"        storage = DA.paths.storage_location, \n",
							"        target = DA.db_name, \n",
							"        notebooks = [path],\n",
							"        configuration = {\n",
							"            \"spark.master\": \"local[*]\",\n",
							"            \"datasets_path\": DA.paths.datasets,\n",
							"        },\n",
							"        clusters=[{ \"label\": \"default\", \"num_workers\": 0 }])\n",
							"    \n",
							"    pipeline_id = response.get(\"pipeline_id\")\n",
							"    print(f\"Created pipline {pipeline_id}\")\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def validate_pipeline_config(self):\n",
							"    \"Provided by DBAcademy, this function validates the configuration of the pipeline\"\n",
							"    import json\n",
							"    \n",
							"    pipeline_name, path = self.get_pipeline_config()\n",
							"\n",
							"    pipeline = self.client.pipelines().get_by_name(pipeline_name)\n",
							"    assert pipeline is not None, f\"The pipline named \\\"{pipeline_name}\\\" doesn't exist. Double check the spelling.\"\n",
							"\n",
							"    spec = pipeline.get(\"spec\")\n",
							"    \n",
							"    storage = spec.get(\"storage\")\n",
							"    assert storage == DA.paths.storage_location, f\"Invalid storage location. Found \\\"{storage}\\\", expected \\\"{DA.paths.storage_location}\\\" \"\n",
							"    \n",
							"    target = spec.get(\"target\")\n",
							"    assert target == DA.db_name, f\"Invalid target. Found \\\"{target}\\\", expected \\\"{DA.db_name}\\\" \"\n",
							"    \n",
							"    libraries = spec.get(\"libraries\")\n",
							"    assert libraries is None or len(libraries) > 0, f\"The notebook path must be specified.\"\n",
							"    assert len(libraries) == 1, f\"More than one library (e.g. notebook) was specified.\"\n",
							"    first_library = libraries[0]\n",
							"    assert first_library.get(\"notebook\") is not None, f\"Incorrect library configuration - expected a notebook.\"\n",
							"    first_library_path = first_library.get(\"notebook\").get(\"path\")\n",
							"    assert first_library_path == path, f\"Invalid notebook path. Found \\\"{first_library_path}\\\", expected \\\"{path}\\\" \"\n",
							"\n",
							"    configuration = spec.get(\"configuration\")\n",
							"    assert configuration is not None, f\"The two configuration parameters were not specified.\"\n",
							"    datasets_path = configuration.get(\"datasets_path\")\n",
							"    assert datasets_path == DA.paths.datasets, f\"Invalid datasets_path value. Found \\\"{datasets_path}\\\", expected \\\"{DA.paths.datasets}\\\".\"\n",
							"    spark_master = configuration.get(\"spark.master\")\n",
							"    assert spark_master == f\"local[*]\", f\"Invalid spark.master value. Expected \\\"local[*]\\\", found \\\"{spark_master}\\\".\"\n",
							"    \n",
							"    cluster = spec.get(\"clusters\")[0]\n",
							"    autoscale = cluster.get(\"autoscale\")\n",
							"    assert autoscale is None, f\"Autoscaling should be disabled.\"\n",
							"    \n",
							"    num_workers = cluster.get(\"num_workers\")\n",
							"    assert num_workers == 0, f\"Expected the number of workers to be 0, found {num_workers}.\"\n",
							"\n",
							"    development = spec.get(\"development\")\n",
							"    assert development == True, f\"The pipline mode should be set to \\\"Development\\\".\"\n",
							"    \n",
							"    channel = spec.get(\"channel\")\n",
							"    assert channel is None or channel == \"CURRENT\", f\"Expected the channel to be Current but found {channel}.\"\n",
							"    \n",
							"    photon = spec.get(\"photon\")\n",
							"    assert photon == True, f\"Expected Photon to be enabled.\"\n",
							"    \n",
							"    continuous = spec.get(\"continuous\")\n",
							"    assert continuous == False, f\"Expected the Pipeline mode to be \\\"Triggered\\\", found \\\"Continuous\\\".\"\n",
							"\n",
							"    policy = self.client.cluster_policies.get_by_name(\"Student's DLT-Only Policy\")\n",
							"    if policy is not None:\n",
							"        cluster = { \n",
							"            \"num_workers\": 0,\n",
							"            \"label\": \"default\", \n",
							"            \"policy_id\": policy.get(\"policy_id\")\n",
							"        }\n",
							"        self.client.pipelines.create_or_update(name = pipeline_name,\n",
							"                                               storage = DA.paths.storage_location,\n",
							"                                               target = DA.db_name,\n",
							"                                               notebooks = [path],\n",
							"                                               configuration = {\n",
							"                                                   \"spark.master\": \"local[*]\",\n",
							"                                                   \"datasets_path\": DA.paths.datasets,\n",
							"                                               },\n",
							"                                               clusters=[cluster])\n",
							"    print(\"All tests passed!\")\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def get_job_config(self):\n",
							"    job_name = f\"Jobs-Demo-91-{DA.username}\"\n",
							"    \n",
							"    notebook = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None)\n",
							"    notebook = \"/\".join(notebook.split(\"/\")[:-1]) + \"/DE 9.1.2 - Reset\"\n",
							"\n",
							"    return job_name, notebook\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def print_job_config_task_reset(self):\n",
							"    \"Provided by DBAcademy, this function renders the configuration of the job as HTML\"\n",
							"    job_name, reset_notebook = self.get_job_config()\n",
							"    \n",
							"    displayHTML(f\"\"\"<table style=\"width:100%\">\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Job Name:</td>\n",
							"        <td><input type=\"text\" value=\"{job_name}\" style=\"width:100%\"></td></tr>\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Reset Notebook Path:</td>\n",
							"        <td><input type=\"text\" value=\"{reset_notebook}\" style=\"width:100%\"></td></tr>\n",
							"\n",
							"    </table>\"\"\")    \n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def create_job_v1(self):\n",
							"    \"Provided by DBAcademy, this function creates the prescribed job\"\n",
							"    \n",
							"    job_name, reset_notebook = self.get_job_config()\n",
							"\n",
							"    self.client.jobs.delete_by_name(job_name, success_only=False)\n",
							"    cluster_id = dbgems.get_tags().get(\"clusterId\")\n",
							"    \n",
							"    params = {\n",
							"        \"name\": job_name,\n",
							"        \"tags\": {\n",
							"            \"dbacademy.course\": self.course_name,\n",
							"            \"dbacademy.source\": self.course_name\n",
							"        },\n",
							"        \"email_notifications\": {},\n",
							"        \"timeout_seconds\": 7200,\n",
							"        \"max_concurrent_runs\": 1,\n",
							"        \"format\": \"MULTI_TASK\",\n",
							"        \"tasks\": [\n",
							"            {\n",
							"                \"task_key\": \"Reset\",\n",
							"                \"libraries\": [],\n",
							"                \"notebook_task\": {\n",
							"                    \"notebook_path\": reset_notebook,\n",
							"                    \"base_parameters\": []\n",
							"                },\n",
							"                \"existing_cluster_id\": cluster_id\n",
							"            },\n",
							"        ],\n",
							"    }\n",
							"    params = self.update_cluster_params(params, [0])\n",
							"    \n",
							"    create_response = self.client.jobs().create(params)\n",
							"    job_id = create_response.get(\"job_id\")\n",
							"    \n",
							"    print(f\"Created job #{job_id}\")\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def validate_job_v1_config(self):\n",
							"    \"Provided by DBAcademy, this function validates the configuration of the job\"\n",
							"    import json\n",
							"    \n",
							"    job_name, reset_notebook = self.get_job_config()\n",
							"\n",
							"    job = self.client.jobs.get_by_name(job_name)\n",
							"    assert job is not None, f\"The job named \\\"{job_name}\\\" doesn't exist. Double check the spelling.\"\n",
							"\n",
							"    # print(json.dumps(job, indent=4))\n",
							"    \n",
							"    settings = job.get(\"settings\")\n",
							"    \n",
							"    if settings.get(\"format\") == \"SINGLE_TASK\":\n",
							"        notebook_path = settings.get(\"notebook_task\", {}).get(\"notebook_path\")\n",
							"        actual_cluster_id = settings.get(\"existing_cluster_id\", None)\n",
							"        #task_key = settings.get(\"task_key\", None)\n",
							"    else:\n",
							"        tasks = settings.get(\"tasks\", [])\n",
							"        assert len(tasks) == 1, f\"Expected one task, found {len(tasks)}.\"\n",
							"\n",
							"        notebook_path = tasks[0].get(\"notebook_task\", {}).get(\"notebook_path\")\n",
							"        actual_cluster_id = tasks[0].get(\"existing_cluster_id\", None)\n",
							"        \n",
							"        task_key = tasks[0].get(\"task_key\", None)\n",
							"        assert task_key == \"Rest\", f\"Expected the first task to have the name \\\"Reset\\\", found \\\"{task_key}\\\"\"\n",
							"        \n",
							"        \n",
							"    assert notebook_path == reset_notebook, f\"Invalid Notebook Path. Found \\\"{notebook_path}\\\", expected \\\"{reset_notebook}\\\" \"\n",
							"    \n",
							"    if not self.is_smoke_test():\n",
							"        # Don't check the actual_cluster_id when running as a smoke test\n",
							"        \n",
							"        assert actual_cluster_id is not None, f\"The first task is not configured to use the current All-Purpose cluster\"\n",
							"\n",
							"        expected_cluster_id = dbgems.get_tags().get(\"clusterId\")\n",
							"        if expected_cluster_id != actual_cluster_id:\n",
							"            actual_cluster = self.client.clusters.get(actual_cluster_id).get(\"cluster_name\")\n",
							"            expected_cluster = self.client.clusters.get(expected_cluster_id).get(\"cluster_name\")\n",
							"            assert actual_cluster_id == expected_cluster_id, f\"The first task is not configured to use the current All-Purpose cluster, expected \\\"{expected_cluster}\\\", found \\\"{actual_cluster}\\\"\"\n",
							"        \n",
							"    print(\"All tests passed!\")\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# @DBAcademyHelper.monkey_patch\n",
							"# def print_job_config_task_dlt(self):\n",
							"#     \"Provided by DBAcademy, this function renders the configuration of the job as HTML\"\n",
							"#     pipeline_name, path = self.get_pipeline_config()\n",
							"#     job_name, reset_notebook = self.get_job_config()\n",
							"    \n",
							"#     displayHTML(f\"\"\"<table style=\"width:100%\">\n",
							"#     <tr>\n",
							"#         <td style=\"white-space:nowrap; width:1em\">Job Name:</td>\n",
							"#         <td><input type=\"text\" value=\"{job_name}\" style=\"width:100%\"></td></tr>\n",
							"#     <tr>\n",
							"#         <td style=\"white-space:nowrap; width:1em\">Pipeline Name:</td>\n",
							"#         <td><input type=\"text\" value=\"{pipeline_name}\" style=\"width:100%\"></td></tr>\n",
							"\n",
							"#     </table>\"\"\")    \n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def create_job_v2(self):\n",
							"    \"Provided by DBAcademy, this function creates the prescribed job\"\n",
							"    \n",
							"    pipeline_name, path = self.get_pipeline_config()\n",
							"    job_name, reset_notebook = self.get_job_config()\n",
							"\n",
							"    self.client.jobs.delete_by_name(job_name, success_only=False)\n",
							"    cluster_id = dbgems.get_tags().get(\"clusterId\")\n",
							"    \n",
							"    pipeline = self.client.pipelines().get_by_name(pipeline_name)\n",
							"    pipeline_id = pipeline.get(\"pipeline_id\")\n",
							"    \n",
							"    params = {\n",
							"        \"name\": job_name,\n",
							"        \"tags\": {\n",
							"            \"dbacademy.course\": self.course_name,\n",
							"            \"dbacademy.source\": self.course_name\n",
							"        },\n",
							"        \"email_notifications\": {},\n",
							"        \"timeout_seconds\": 7200,\n",
							"        \"max_concurrent_runs\": 1,\n",
							"        \"format\": \"MULTI_TASK\",\n",
							"        \"tasks\": [\n",
							"            {\n",
							"                \"task_key\": \"Reset\",\n",
							"                \"libraries\": [],\n",
							"                \"notebook_task\": {\n",
							"                    \"notebook_path\": reset_notebook,\n",
							"                    \"base_parameters\": []\n",
							"                },\n",
							"                \"existing_cluster_id\": cluster_id\n",
							"            },\n",
							"            {\n",
							"                \"task_key\": \"DLT\",\n",
							"                \"depends_on\": [ { \"task_key\": \"Reset\" } ],\n",
							"                \"pipeline_task\": {\n",
							"                    \"pipeline_id\": pipeline_id\n",
							"                },\n",
							"            },\n",
							"        ],\n",
							"    }\n",
							"    params = self.update_cluster_params(params, [0])\n",
							"    \n",
							"    create_response = self.client.jobs().create(params)\n",
							"    job_id = create_response.get(\"job_id\")\n",
							"    \n",
							"    print(f\"Created job #{job_id}\")\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def validate_job_v2_config(self):\n",
							"    \"Provided by DBAcademy, this function validates the configuration of the job\"\n",
							"    import json\n",
							"    \n",
							"    pipeline_name, path = self.get_pipeline_config()\n",
							"    job_name, reset_notebook = self.get_job_config()\n",
							"\n",
							"    job = self.client.jobs.get_by_name(job_name)\n",
							"    assert job is not None, f\"The job named \\\"{job_name}\\\" doesn't exist. Double check the spelling.\"\n",
							"    \n",
							"    settings = job.get(\"settings\")\n",
							"    assert settings.get(\"format\") == \"MULTI_TASK\", f\"Expected two tasks, found 1.\"\n",
							"\n",
							"    tasks = settings.get(\"tasks\", [])\n",
							"    assert len(tasks) == 2, f\"Expected two tasks, found {len(tasks)}.\"\n",
							"    \n",
							"    \n",
							"    # Reset Task\n",
							"    task_name = tasks[0].get(\"task_key\", None)\n",
							"    assert task_name == \"Reset\", f\"Expected the first task to have the name \\\"Reset\\\", found \\\"{task_name}\\\"\"\n",
							"    \n",
							"    notebook_path = tasks[0].get(\"notebook_task\", {}).get(\"notebook_path\")\n",
							"    assert notebook_path == reset_notebook, f\"Invalid Notebook Path for the first task. Found \\\"{notebook_path}\\\", expected \\\"{reset_notebook}\\\" \"\n",
							"\n",
							"    if not self.is_smoke_test():\n",
							"        # Don't check the actual_cluster_id when running as a smoke test\n",
							"        \n",
							"        actual_cluster_id = tasks[0].get(\"existing_cluster_id\", None)\n",
							"        assert actual_cluster_id is not None, f\"The first task is not configured to use the current All-Purpose cluster\"\n",
							"\n",
							"        expected_cluster_id = dbgems.get_tags().get(\"clusterId\")\n",
							"        if expected_cluster_id != actual_cluster_id:\n",
							"            actual_cluster = self.client.clusters.get(actual_cluster_id).get(\"cluster_name\")\n",
							"            expected_cluster = self.client.clusters.get(expected_cluster_id).get(\"cluster_name\")\n",
							"            assert actual_cluster_id == expected_cluster_id, f\"The first task is not configured to use the current All-Purpose cluster, expected \\\"{expected_cluster}\\\", found \\\"{actual_cluster}\\\"\"\n",
							"\n",
							"    \n",
							"    \n",
							"    # Reset DLT\n",
							"    task_name = tasks[1].get(\"task_key\", None)\n",
							"    assert task_name == \"DLT\", f\"Expected the second task to have the name \\\"DLT\\\", found \\\"{task_name}\\\"\"\n",
							"\n",
							"    actual_pipeline_id = tasks[1].get(\"pipeline_task\", {}).get(\"pipeline_id\", None)\n",
							"    assert actual_pipeline_id is not None, f\"The second task is not configured to use a Delta Live Tables pipeline\"\n",
							"    \n",
							"    expected_pipeline = self.client.pipelines().get_by_name(pipeline_name)\n",
							"    actual_pipeline = self.client.pipelines().get_by_id(actual_pipeline_id)\n",
							"    actual_name = actual_pipeline.get(\"spec\").get(\"name\", \"Oops\")\n",
							"    assert actual_pipeline_id == expected_pipeline.get(\"pipeline_id\"), f\"The second task is not configured to use the correct pipeline, expected \\\"{pipeline_name}\\\", found \\\"{actual_name}\\\"\"\n",
							"    \n",
							"    depends_on = tasks[1].get(\"depends_on\", [])\n",
							"    assert len(depends_on) > 0, f\"The \\\"DLT\\\" task does not depend on the \\\"Reset\\\" task\"\n",
							"    assert len(depends_on) == 1, f\"The \\\"DLT\\\" task depends on more than just the \\\"Reset\\\" task\"\n",
							"    depends_task_key = depends_on[0].get(\"task_key\")\n",
							"    assert depends_task_key == \"Reset\", f\"The \\\"DLT\\\" task doesn't depend on the \\\"Reset\\\" task, found {depends_task_key}\"\n",
							"    \n",
							"    print(\"All tests passed!\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def start_job(self):\n",
							"    job_name, reset_notebook = self.get_job_config()\n",
							"    job_id = self.client.jobs.get_by_name(job_name).get(\"job_id\")\n",
							"    run_id = self.client.jobs.run_now(job_id).get(\"run_id\")\n",
							"    print(f\"Started job #{job_id}, run #{run_id}\")\n",
							"\n",
							"    self.client.runs.wait_for(run_id)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# jobs_demo_91 is specifically referenced in the lesson\n",
							"\n",
							"DA = DBAcademyHelper(lesson=\"jobs_demo_91\", **helper_arguments)\n",
							"DA.reset_environment() # First in a series\n",
							"DA.init(install_datasets=True, create_db=True)\n",
							"\n",
							"DA.paths.stream_path = f\"{DA.paths.working_dir}/stream\"\n",
							"DA.paths.storage_location = f\"{DA.paths.working_dir}/storage\"\n",
							"\n",
							"DA.data_factory = DltDataFactory(DA.paths.stream_path)\n",
							"\n",
							"DA.conclude_setup()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-09-1-2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "6869f9ad-500b-4069-b74e-e5121080aa56"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(lesson=\"jobs_demo_91\", **helper_arguments)\n",
							"DA.reset_environment() # Second in series, but requires reset\n",
							"DA.init(install_datasets=True, create_db=True)\n",
							"\n",
							"DA.paths.stream_path = f\"{DA.paths.working_dir}/stream\"\n",
							"DA.data_factory = DltDataFactory(DA.paths.stream_path)\n",
							"\n",
							"DA.conclude_setup()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-09-1-3')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "95f4800e-f1bb-4d96-8c40-eff255d1cf45"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(lesson=\"jobs_demo_91\", **helper_arguments)\n",
							"# DA.reset_environment() # We don't want to reset the environment\n",
							"DA.init(install_datasets=True, create_db=True)\n",
							"DA.conclude_setup()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-09-2-1L')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0e07c44a-652c-48b0-a6d4-d16c1c1cfead"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def get_pipeline_config(self):\n",
							"    path = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None)\n",
							"    path = \"/\".join(path.split(\"/\")[:-1]) + \"/DE 9.2.3L - DLT Job\"\n",
							"    \n",
							"    pipeline_name = f\"DLT-Job-Lab-92-{DA.username}\"\n",
							"    \n",
							"    return pipeline_name, path\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def print_pipeline_config(self):\n",
							"    pipeline_name, notebook = self.get_pipeline_config()\n",
							"\n",
							"    displayHTML(f\"\"\"<table style=\"width:100%\">\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Pipeline Name:</td>\n",
							"        <td><input type=\"text\" value=\"{pipeline_name}\" style=\"width:100%\"></td></tr>\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Target:</td>\n",
							"        <td><input type=\"text\" value=\"{DA.db_name}\" style=\"width:100%\"></td></tr>\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Storage Location:</td>\n",
							"        <td><input type=\"text\" value=\"{DA.paths.storage_location}\" style=\"width:100%\"></td></tr>\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Notebook Path:</td>\n",
							"        <td><input type=\"text\" value=\"{notebook}\" style=\"width:100%\"></td></tr>\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Datsets Path:</td>\n",
							"        <td><input type=\"text\" value=\"{DA.paths.datasets}\" style=\"width:100%\"></td></tr>\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Source:</td>\n",
							"        <td><input type=\"text\" value=\"{DA.paths.stream_path}\" style=\"width:100%\"></td></tr>\n",
							"    \n",
							"    </table>\"\"\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def create_pipeline(self):\n",
							"    \"Provided by DBAcademy, this function creates the prescribed pipline\"\n",
							"    \n",
							"    pipeline_name, path = self.get_pipeline_config()\n",
							"\n",
							"    # We need to delete the existing pipline so that we can apply updates\n",
							"    # because some attributes are not mutable after creation.\n",
							"    self.client.pipelines().delete_by_name(pipeline_name)\n",
							"    \n",
							"    response = self.client.pipelines().create(\n",
							"        name = pipeline_name, \n",
							"        storage = DA.paths.storage_location, \n",
							"        target = DA.db_name, \n",
							"        notebooks = [path],\n",
							"        configuration = {\n",
							"            \"spark.master\": \"local[*]\",\n",
							"            \"datasets_path\": DA.paths.datasets,\n",
							"            \"source\": DA.paths.stream_path,\n",
							"        },\n",
							"        clusters=[{ \"label\": \"default\", \"num_workers\": 0 }])\n",
							"    \n",
							"    pipeline_id = response.get(\"pipeline_id\")\n",
							"    print(f\"Created pipline \\\"{pipeline_name}\\\" (#{pipeline_id})\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def validate_pipeline_config(self):\n",
							"    \"Provided by DBAcademy, this function validates the configuration of the pipeline\"\n",
							"    import json\n",
							"    \n",
							"    pipeline_name, path = self.get_pipeline_config()\n",
							"\n",
							"    pipeline = self.client.pipelines().get_by_name(pipeline_name)\n",
							"    assert pipeline is not None, f\"The pipline named \\\"{pipeline_name}\\\" doesn't exist. Double check the spelling.\"\n",
							"\n",
							"    spec = pipeline.get(\"spec\")\n",
							"    \n",
							"    storage = spec.get(\"storage\")\n",
							"    assert storage == DA.paths.storage_location, f\"Invalid storage location. Found \\\"{storage}\\\", expected \\\"{DA.paths.storage_location}\\\" \"\n",
							"    \n",
							"    target = spec.get(\"target\")\n",
							"    assert target == DA.db_name, f\"Invalid target. Found \\\"{target}\\\", expected \\\"{DA.db_name}\\\" \"\n",
							"    \n",
							"    libraries = spec.get(\"libraries\")\n",
							"    assert libraries is None or len(libraries) > 0, f\"The notebook path must be specified.\"\n",
							"    assert len(libraries) == 1, f\"More than one library (e.g. notebook) was specified.\"\n",
							"    first_library = libraries[0]\n",
							"    assert first_library.get(\"notebook\") is not None, f\"Incorrect library configuration - expected a notebook.\"\n",
							"    first_library_path = first_library.get(\"notebook\").get(\"path\")\n",
							"    assert first_library_path == path, f\"Invalid notebook path. Found \\\"{first_library_path}\\\", expected \\\"{path}\\\" \"\n",
							"\n",
							"    configuration = spec.get(\"configuration\")\n",
							"    assert configuration is not None, f\"The three configuration parameters were not specified.\"\n",
							"    datasets_path = configuration.get(\"datasets_path\")\n",
							"    assert datasets_path == DA.paths.datasets, f\"Invalid \\\"datasets_path\\\" value. Found \\\"{datasets_path}\\\", expected \\\"{DA.paths.datasets}\\\".\"\n",
							"    spark_master = configuration.get(\"spark.master\")\n",
							"    assert spark_master == f\"local[*]\", f\"Invalid \\\"spark.master\\\" value. Expected \\\"local[*]\\\", found \\\"{spark_master}\\\".\"\n",
							"    stream_source = configuration.get(\"source\")\n",
							"    assert stream_source == DA.paths.stream_path, f\"Invalid \\\"source\\\" value. Expected \\\"{DA.paths.stream_path}\\\", found \\\"{stream_source}\\\".\"\n",
							"    \n",
							"    cluster = spec.get(\"clusters\")[0]\n",
							"    autoscale = cluster.get(\"autoscale\")\n",
							"    assert autoscale is None, f\"Autoscaling should be disabled.\"\n",
							"    \n",
							"    num_workers = cluster.get(\"num_workers\")\n",
							"    assert num_workers == 0, f\"Expected the number of workers to be 0, found {num_workers}.\"\n",
							"\n",
							"    development = spec.get(\"development\")\n",
							"    assert development == True, f\"The pipline mode should be set to \\\"Development\\\".\"\n",
							"    \n",
							"    channel = spec.get(\"channel\")\n",
							"    assert channel is None or channel == \"CURRENT\", f\"Expected the channel to be \\\"Current\\\" but found \\\"{channel}\\\".\"\n",
							"    \n",
							"    photon = spec.get(\"photon\")\n",
							"    assert photon == True, f\"Expected Photon to be enabled.\"\n",
							"    \n",
							"    continuous = spec.get(\"continuous\")\n",
							"    assert continuous == False, f\"Expected the Pipeline mode to be \\\"Triggered\\\", found \\\"Continuous\\\".\"\n",
							"\n",
							"    policy = self.client.cluster_policies.get_by_name(\"Student's DLT-Only Policy\")\n",
							"    if policy is not None:\n",
							"        cluster = { \n",
							"            \"num_workers\": 0,\n",
							"            \"label\": \"default\", \n",
							"            \"policy_id\": policy.get(\"policy_id\")\n",
							"        }\n",
							"        self.client.pipelines.create_or_update(name = pipeline_name,\n",
							"                                               storage = DA.paths.storage_location,\n",
							"                                               target = DA.db_name,\n",
							"                                               notebooks = [path],\n",
							"                                               configuration = {\n",
							"                                                   \"spark.master\": \"local[*]\",\n",
							"                                                   \"datasets_path\": DA.paths.datasets,\n",
							"                                                   \"source\": DA.paths.stream_path,\n",
							"                                               },\n",
							"                                               clusters=[cluster])\n",
							"    print(\"All tests passed!\")\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def get_job_config(self):\n",
							"    \n",
							"    job_name = f\"Jobs-Lab-92-{DA.username}\"\n",
							"    \n",
							"    notebook_1 = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None)\n",
							"    notebook_1 = \"/\".join(notebook_1.split(\"/\")[:-1]) + \"/DE 9.2.2L - Batch Job\"\n",
							"\n",
							"    notebook_2 = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None)\n",
							"    notebook_2 = \"/\".join(notebook_2.split(\"/\")[:-1]) + \"/DE 9.2.4L - Query Results Job\"\n",
							"\n",
							"    return job_name, notebook_1, notebook_2\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def print_job_config(self):\n",
							"    \n",
							"    job_name, notebook_1, notebook_2 = self.get_job_config()\n",
							"    \n",
							"    displayHTML(f\"\"\"<table style=\"width:100%\">\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Job Name:</td>\n",
							"        <td><input type=\"text\" value=\"{job_name}\" style=\"width:100%\"></td></tr>\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Batch Notebook Path:</td>\n",
							"        <td><input type=\"text\" value=\"{notebook_1}\" style=\"width:100%\"></td></tr>\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Query Notebook Path:</td>\n",
							"        <td><input type=\"text\" value=\"{notebook_2}\" style=\"width:100%\"></td></tr>\n",
							"        \n",
							"    </table>\"\"\")\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def create_job(self):\n",
							"    \"Provided by DBAcademy, this function creates the prescribed job\"\n",
							"    \n",
							"    pipeline_name, path = self.get_pipeline_config()\n",
							"    job_name, notebook_1, notebook_2 = self.get_job_config()\n",
							"\n",
							"    self.client.jobs.delete_by_name(job_name, success_only=False)\n",
							"    cluster_id = dbgems.get_tags().get(\"clusterId\")\n",
							"    \n",
							"    pipeline = self.client.pipelines().get_by_name(pipeline_name)\n",
							"    pipeline_id = pipeline.get(\"pipeline_id\")\n",
							"    \n",
							"    params = {\n",
							"        \"name\": job_name,\n",
							"        \"tags\": {\n",
							"            \"dbacademy.course\": self.course_name,\n",
							"            \"dbacademy.source\": self.course_name\n",
							"        },\n",
							"        \"email_notifications\": {},\n",
							"        \"timeout_seconds\": 7200,\n",
							"        \"max_concurrent_runs\": 1,\n",
							"        \"format\": \"MULTI_TASK\",\n",
							"        \"tasks\": [\n",
							"            {\n",
							"                \"task_key\": \"Batch-Job\",\n",
							"                \"libraries\": [],\n",
							"                \"notebook_task\": {\n",
							"                    \"notebook_path\": notebook_1,\n",
							"                    \"base_parameters\": []\n",
							"                },\n",
							"                \"existing_cluster_id\": cluster_id\n",
							"            },\n",
							"            {\n",
							"                \"task_key\": \"DLT\",\n",
							"                \"depends_on\": [ { \"task_key\": \"Batch-Job\" } ],\n",
							"                \"pipeline_task\": {\n",
							"                    \"pipeline_id\": pipeline_id\n",
							"                },\n",
							"            },\n",
							"            {\n",
							"                \"task_key\": \"Query-Results\",\n",
							"                \"depends_on\": [ { \"task_key\": \"DLT\" } ],\n",
							"                \"libraries\": [],\n",
							"                \"notebook_task\": {\n",
							"                    \"notebook_path\": notebook_2,\n",
							"                    \"base_parameters\": []\n",
							"                },\n",
							"                \"existing_cluster_id\": cluster_id\n",
							"            },\n",
							"        ],\n",
							"    }\n",
							"    params = self.update_cluster_params(params, [0,2])\n",
							"    \n",
							"    #import json\n",
							"    #print(json.dumps(params, indent=4))\n",
							"    \n",
							"    create_response = self.client.jobs().create(params)\n",
							"    job_id = create_response.get(\"job_id\")\n",
							"    \n",
							"    print(f\"Created job \\\"{job_name}\\\" (#{job_id})\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def validate_job_config(self):\n",
							"    \"Provided by DBAcademy, this function validates the configuration of the job\"\n",
							"    import json\n",
							"    \n",
							"    pipeline_name, job_path = self.get_pipeline_config()\n",
							"    job_name, notebook_1, notebook_2 = self.get_job_config()\n",
							"\n",
							"    job = self.client.jobs.get_by_name(job_name)\n",
							"    assert job is not None, f\"The job named \\\"{job_name}\\\" doesn't exist. Double check the spelling.\"\n",
							"    \n",
							"    settings = job.get(\"settings\")\n",
							"    assert settings.get(\"format\") == \"MULTI_TASK\", f\"Expected three tasks, found 1.\"\n",
							"\n",
							"    tasks = settings.get(\"tasks\", [])\n",
							"    assert len(tasks) == 3, f\"Expected three tasks, found {len(tasks)}.\"\n",
							"\n",
							"    \n",
							"    \n",
							"    # Batch-Job Task\n",
							"    batch_task = tasks[0]\n",
							"    task_name = batch_task.get(\"task_key\", None)\n",
							"    assert task_name == \"Batch-Job\", f\"Expected the first task to have the name \\\"Batch-Job\\\", found \\\"{task_name}\\\"\"\n",
							"    \n",
							"    notebook_path = batch_task.get(\"notebook_task\", {}).get(\"notebook_path\")\n",
							"    assert notebook_path == notebook_1, f\"Invalid Notebook Path for the first task. Found \\\"{notebook_path}\\\", expected \\\"{notebook_1}\\\" \"\n",
							"\n",
							"    if not self.is_smoke_test():\n",
							"        # Don't check the actual_cluster_id when running as a smoke test\n",
							"        \n",
							"        actual_cluster_id = batch_task.get(\"existing_cluster_id\", None)\n",
							"        assert actual_cluster_id is not None, f\"The first task is not configured to use the current All-Purpose cluster\"\n",
							"\n",
							"        expected_cluster_id = dbgems.get_tags().get(\"clusterId\")\n",
							"        if expected_cluster_id != actual_cluster_id:\n",
							"            actual_cluster = self.client.clusters.get(actual_cluster_id).get(\"cluster_name\")\n",
							"            expected_cluster = self.client.clusters.get(expected_cluster_id).get(\"cluster_name\")\n",
							"            assert actual_cluster_id == expected_cluster_id, f\"The first task is not configured to use the current All-Purpose cluster, expected \\\"{expected_cluster}\\\", found \\\"{actual_cluster}\\\"\"\n",
							"\n",
							"    \n",
							"    \n",
							"    # DLT\n",
							"    dlt_task = tasks[1]\n",
							"    task_name = dlt_task.get(\"task_key\", None)\n",
							"    assert task_name == \"DLT\", f\"Expected the second task to have the name \\\"DLT\\\", found \\\"{task_name}\\\"\"\n",
							"\n",
							"    actual_pipeline_id = dlt_task.get(\"pipeline_task\", {}).get(\"pipeline_id\", None)\n",
							"    assert actual_pipeline_id is not None, f\"The second task is not configured to use a Delta Live Tables pipeline\"\n",
							"    \n",
							"    expected_pipeline = self.client.pipelines().get_by_name(pipeline_name)\n",
							"    actual_pipeline = self.client.pipelines().get_by_id(actual_pipeline_id)\n",
							"    actual_name = actual_pipeline.get(\"spec\").get(\"name\", \"Oops\")\n",
							"    assert actual_pipeline_id == expected_pipeline.get(\"pipeline_id\"), f\"The second task is not configured to use the correct pipeline, expected \\\"{pipeline_name}\\\", found \\\"{actual_name}\\\"\"\n",
							"    \n",
							"    depends_on = dlt_task.get(\"depends_on\", [])\n",
							"    assert len(depends_on) > 0, f\"The \\\"DLT\\\" task does not depend on the \\\"Batch-Job\\\" task\"\n",
							"    assert len(depends_on) == 1, f\"The \\\"DLT\\\" task depends on more than just the \\\"Batch-Job\\\" task\"\n",
							"    depends_task_key = depends_on[0].get(\"task_key\")\n",
							"    assert depends_task_key == \"Batch-Job\", f\"The \\\"DLT\\\" task doesn't depend on the \\\"Batch-Job\\\" task, found \\\"{depends_task_key}\\\".\"\n",
							"    \n",
							"    \n",
							"    \n",
							"    # Query Task\n",
							"    query_task = tasks[2] \n",
							"    task_name = query_task.get(\"task_key\", None)\n",
							"    assert task_name == \"Query-Results\", f\"Expected the third task to have the name \\\"Query-Results\\\", found \\\"{task_name}\\\"\"\n",
							"    \n",
							"    notebook_path = query_task.get(\"notebook_task\", {}).get(\"notebook_path\")\n",
							"    assert notebook_path == notebook_2, f\"Invalid Notebook Path for the thrid task. Found \\\"{notebook_path}\\\", expected \\\"{notebook_2}\\\" \"\n",
							"    \n",
							"    depends_on = query_task.get(\"depends_on\", [])\n",
							"    assert len(depends_on) > 0, f\"The \\\"Query-Results\\\" task does not depend on the \\\"DLT\\\" task\"\n",
							"    assert len(depends_on) == 1, f\"The \\\"Query-Results\\\" task depends on more than just the \\\"DLT\\\" task\"\n",
							"    depends_task_key = depends_on[0].get(\"task_key\")\n",
							"    assert depends_task_key == \"DLT\", f\"The \\\"Query-Results\\\" task doesn't depend on the \\\"DLT\\\" task, found \\\"{depends_task_key}\\\".\"\n",
							"\n",
							"    if not self.is_smoke_test():\n",
							"        # Don't check the actual_cluster_id when running as a smoke test\n",
							"        \n",
							"        actual_cluster_id = query_task.get(\"existing_cluster_id\", None)\n",
							"        assert actual_cluster_id is not None, f\"The second task is not configured to use the current All-Purpose cluster\"\n",
							"\n",
							"        expected_cluster_id = dbgems.get_tags().get(\"clusterId\")\n",
							"        if expected_cluster_id != actual_cluster_id:\n",
							"            actual_cluster = self.client.clusters.get(actual_cluster_id).get(\"cluster_name\")\n",
							"            expected_cluster = self.client.clusters.get(expected_cluster_id).get(\"cluster_name\")\n",
							"            assert actual_cluster_id == expected_cluster_id, f\"The second task is not configured to use the current All-Purpose cluster, expected \\\"{expected_cluster}\\\", found \\\"{actual_cluster}\\\"\"\n",
							"\n",
							"    print(\"All tests passed!\")\n",
							"    "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def start_job(self):\n",
							"    job_name, notebook_1, notebook_2 = self.get_job_config()\n",
							"    job_id = self.client.jobs.get_by_name(job_name).get(\"job_id\")\n",
							"    run_id = self.client.jobs.run_now(job_id).get(\"run_id\")\n",
							"    print(f\"Started job #{job_id}, run #{run_id}\")\n",
							"\n",
							"    self.client.runs.wait_for(run_id)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(lesson=\"jobs_lab_92\", **helper_arguments)\n",
							"DA.reset_environment()\n",
							"DA.init(install_datasets=True, create_db=True)\n",
							"\n",
							"DA.paths.stream_path = f\"{DA.paths.working_dir}/stream\"\n",
							"DA.paths.storage_location = f\"{DA.paths.working_dir}/storage\"\n",
							"\n",
							"DA.data_factory = DltDataFactory(DA.paths.stream_path)\n",
							"\n",
							"DA.conclude_setup()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-09-2-2L')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "2a1956e1-b2a9-4097-9dc3-b10c112c84e5"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(lesson=\"jobs_lab_92\", **helper_arguments)\n",
							"# DA.reset_environment() # We don't want to reset the environment\n",
							"DA.init(install_datasets=True, create_db=False)\n",
							"\n",
							"DA.paths.stream_path = f\"{DA.paths.working_dir}/stream\"\n",
							"DA.data_factory = DltDataFactory(DA.paths.stream_path)\n",
							"\n",
							"DA.conclude_setup()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-09-2-3L')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "fe517444-7820-411c-a1af-9e54b1b92902"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(lesson=\"jobs_lab_92\", **helper_arguments)\n",
							"# DA.reset_environment() # We don't want to reset the environment\n",
							"DA.init(install_datasets=True, create_db=False)\n",
							"DA.conclude_setup()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-09-2-4L')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "74f4e6c8-acdd-47dd-aef9-64297eb22f1d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(lesson=\"jobs_lab_92\", **helper_arguments)\n",
							"# DA.reset_environment() # We don't want to reset the environment\n",
							"DA.init(install_datasets=True, create_db=False)\n",
							"DA.conclude_setup()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-11-1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d510ecd2-d235-4635-94bb-4d2708647e67"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def print_sql(self, rows, sql):\n",
							"    html = f\"\"\"<textarea style=\"width:100%\" rows=\"{rows}\"> \\n{sql.strip()}</textarea>\"\"\"\n",
							"    displayHTML(html)\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def generate_users_table(self):\n",
							"    self.print_sql(20, f\"\"\"\n",
							"CREATE DATABASE IF NOT EXISTS {DA.db_name}\n",
							"LOCATION '{DA.paths.user_db}';\n",
							"\n",
							"USE {DA.db_name};\n",
							"\n",
							"CREATE TABLE users (id INT, name STRING, value DOUBLE, state STRING);\n",
							"\n",
							"INSERT INTO users\n",
							"VALUES (1, \"Yve\", 1.0, \"CA\"),\n",
							"       (2, \"Omar\", 2.5, \"NY\"),\n",
							"       (3, \"Elia\", 3.3, \"OH\"),\n",
							"       (4, \"Rebecca\", 4.7, \"TX\"),\n",
							"       (5, \"Ameena\", 5.3, \"CA\"),\n",
							"       (6, \"Ling\", 6.6, \"NY\"),\n",
							"       (7, \"Pedro\", 7.1, \"KY\");\n",
							"\n",
							"CREATE VIEW ny_users_vw\n",
							"AS SELECT * FROM users WHERE state = 'NY';\n",
							"\"\"\")\n",
							"    "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def generate_create_database_with_grants(self):\n",
							"    self.print_sql(7, f\"\"\"\n",
							"CREATE DATABASE {DA.db_name}_derivative;\n",
							"\n",
							"GRANT USAGE, READ_METADATA, CREATE, MODIFY, SELECT ON DATABASE `{DA.db_name}_derivative` TO `users`;\n",
							"\n",
							"SHOW GRANT ON DATABASE `{DA.db_name}_derivative`;\"\"\")    \n",
							"    "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments)\n",
							"DA.reset_environment()\n",
							"DA.init(install_datasets=True, create_db=False)\n",
							"DA.conclude_setup()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-11-2L')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "dace2a2f-6d89-474c-bad1-8b194068712f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# The lesson name is specifically named \"acls_lab\" as it is a significantly user-facing - JDP\n",
							"DA = DBAcademyHelper(lesson=\"acls_lab\", **helper_arguments)\n",
							"DA.reset_environment() # Not sequenced, but \"acls_lab\" is directly referenced in the prose\n",
							"DA.init(install_datasets=True, create_db=False)\n",
							"DA.conclude_setup()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def print_sql(self, rows, sql):\n",
							"    displayHTML(f\"\"\"<body><textarea style=\"width:100%\" rows={rows}> \\n{sql.strip()}</textarea></body>\"\"\")\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def generate_query(self):\n",
							"    import re\n",
							"    import random\n",
							"\n",
							"    self.print_sql(23, f\"\"\"\n",
							"CREATE DATABASE IF NOT EXISTS {DA.db_name}\n",
							"LOCATION '{DA.paths.user_db}';\n",
							"\n",
							"USE {DA.db_name};\n",
							"    \n",
							"CREATE TABLE beans \n",
							"(name STRING, color STRING, grams FLOAT, delicious BOOLEAN); \n",
							"\n",
							"INSERT INTO beans\n",
							"VALUES ('black', 'black', {random.uniform(0, 5000):.2f}, {random.choice([\"true\", \"false\"])}),\n",
							"       ('lentils', 'brown', {random.uniform(0, 5000):.2f}, {random.choice([\"true\", \"false\"])}),\n",
							"       ('jelly', 'rainbow', {random.uniform(0, 5000):.2f}, {random.choice([\"true\", \"false\"])}),\n",
							"       ('pinto', 'brown', {random.uniform(0, 5000):.2f}, {random.choice([\"true\", \"false\"])}),\n",
							"       ('green', 'green', {random.uniform(0, 5000):.2f}, {random.choice([\"true\", \"false\"])}),\n",
							"       ('beanbag chair', 'white', {random.uniform(0, 5000):.2f}, {random.choice([\"true\", \"false\"])}),\n",
							"       ('lentils', 'green', {random.uniform(0, 5000):.2f}, {random.choice([\"true\", \"false\"])}),\n",
							"       ('kidney', 'red', {random.uniform(0, 5000):.2f}, {random.choice([\"true\", \"false\"])}),\n",
							"       ('castor', 'brown', {random.uniform(0, 5000):.2f}, {random.choice([\"true\", \"false\"])});\n",
							"\n",
							"CREATE VIEW tasty_beans\n",
							"AS SELECT * FROM beans WHERE delicious = true;\n",
							"    \"\"\")\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def generate_confirmation_query(self, username):\n",
							"    import re\n",
							"    # clean_username = re.sub(\"[^a-zA-Z0-9]\", \"_\", username)\n",
							"    database = DA.db_name #.replace(DA.clean_username, clean_username)\n",
							"    \n",
							"    self.print_sql(11, f\"\"\"\n",
							"USE {database};\n",
							"\n",
							"SELECT * FROM beans;\n",
							"SELECT * FROM tasty_beans;\n",
							"SELECT * FROM beans MINUS SELECT * FROM tasty_beans;\n",
							"\n",
							"UPDATE beans\n",
							"SET color = 'pink'\n",
							"WHERE name = 'black'\n",
							"\"\"\")\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def generate_union_query(self):\n",
							"    self.print_sql(6, f\"\"\"\n",
							"USE {DA.db_name};\n",
							"\n",
							"SELECT * FROM beans\n",
							"UNION ALL TABLE beans;\"\"\")\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def generate_derivative_view(self):\n",
							"    self.print_sql(7, f\"\"\"\n",
							"USE {DA.db_name};\n",
							"\n",
							"CREATE VIEW our_beans \n",
							"AS SELECT * FROM beans\n",
							"UNION ALL TABLE beans;\n",
							"\"\"\")\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def get_their_db(self, their_username):\n",
							"    import re\n",
							"    db_name_prefix = self.to_database_name(username=their_username, course_code=self.course_code)\n",
							"    \n",
							"#     da_name, da_hash = self.get_username_hash(their_username)\n",
							"#     db_name_prefix = f\"da-{da_name}@{da_hash}-{self.course_code}\"         # Composite all the values to create the \"dirty\" database name\n",
							"#     while \"__\" in db_name_prefix: \n",
							"#         db_name_prefix = self.db_name_prefix.replace(\"__\", \"_\")           # Replace all double underscores with single underscores\n",
							"\n",
							"    if DA.lesson is None: \n",
							"      # No lesson, database name is the same as prefix\n",
							"      return db_name_prefix                        \n",
							"    else:\n",
							"      # Database name includes the lesson name\n",
							"      return f\"{db_name_prefix}_{DA.clean_lesson}\" \n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def generate_partner_view(self, their_username):\n",
							"    self.print_sql(7, f\"\"\"\n",
							"USE {self.get_their_db(their_username)};\n",
							"\n",
							"SELECT name, color, delicious, sum(grams)\n",
							"FROM our_beans\n",
							"GROUP BY name, color, delicious;\"\"\")\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def generate_delete_query(self, their_username):\n",
							"    \n",
							"    self.print_sql(5, f\"\"\"\n",
							"USE {self.get_their_db(their_username)};\n",
							"\n",
							"DELETE FROM beans\n",
							"    \"\"\")\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-12-1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0761854e-e362-456f-bc89-ad82c92856ab"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments)\n",
							"DA.reset_environment()\n",
							"DA.init(install_datasets=True, create_db=False)\n",
							"DA.conclude_setup()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"def print_sql(rows, sql):\n",
							"    html = f\"<textarea style=\\\"width:100%\\\" rows={rows}> \\n{sql.strip()}</textarea>\"\n",
							"    displayHTML(html)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"def _generate_config():\n",
							"    print_sql(33, f\"\"\"\n",
							"CREATE DATABASE IF NOT EXISTS {DA.db_name}\n",
							"LOCATION '{DA.paths.working_dir}';\n",
							"\n",
							"USE {DA.db_name};\n",
							"\n",
							"CREATE TABLE user_ping \n",
							"(user_id STRING, ping INTEGER, time TIMESTAMP); \n",
							"\n",
							"CREATE TABLE user_ids (user_id STRING);\n",
							"\n",
							"INSERT INTO user_ids VALUES\n",
							"(\"potato_luver\"),\n",
							"(\"beanbag_lyfe\"),\n",
							"(\"default_username\"),\n",
							"(\"the_king\"),\n",
							"(\"n00b\"),\n",
							"(\"frodo\"),\n",
							"(\"data_the_kid\"),\n",
							"(\"el_matador\"),\n",
							"(\"the_wiz\");\n",
							"\n",
							"CREATE FUNCTION get_ping()\n",
							"    RETURNS INT\n",
							"    RETURN int(rand() * 250);\n",
							"    \n",
							"CREATE FUNCTION is_active()\n",
							"    RETURNS BOOLEAN\n",
							"    RETURN CASE \n",
							"        WHEN rand() > .25 THEN true\n",
							"        ELSE false\n",
							"        END;\n",
							"\"\"\")\n",
							"    \n",
							"DA.generate_config = _generate_config    "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"def _generate_load():\n",
							"    print_sql(12, f\"\"\"\n",
							"USE {DA.db_name};\n",
							"\n",
							"INSERT INTO user_ping\n",
							"SELECT *, \n",
							"  get_ping() ping, \n",
							"  current_timestamp() time\n",
							"FROM user_ids\n",
							"WHERE is_active()=true;\n",
							"\n",
							"SELECT * FROM user_ping;\n",
							"\"\"\")\n",
							"\n",
							"DA.generate_load = _generate_load"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"def _generate_user_counts():\n",
							"    print_sql(10, f\"\"\"\n",
							"USE {DA.db_name};\n",
							"\n",
							"SELECT user_id, count(*) total_records\n",
							"FROM user_ping\n",
							"GROUP BY user_id\n",
							"ORDER BY \n",
							"  total_records DESC,\n",
							"  user_id ASC;\n",
							"\"\"\")\n",
							"\n",
							"DA.generate_user_counts = _generate_user_counts"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"def _generate_avg_ping():\n",
							"    print_sql(10, f\"\"\"\n",
							"USE {DA.db_name};\n",
							"\n",
							"SELECT user_id, window.end end_time, mean(ping) avg_ping\n",
							"FROM user_ping\n",
							"GROUP BY user_id, window(time, '3 minutes')\n",
							"ORDER BY\n",
							"  end_time DESC,\n",
							"  user_id ASC;\n",
							"\"\"\")\n",
							"\n",
							"DA.generate_avg_ping = _generate_avg_ping"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"def _generate_summary():\n",
							"    print_sql(8, f\"\"\"\n",
							"USE {DA.db_name};\n",
							"\n",
							"SELECT user_id, min(time) first_seen, max(time) last_seen, count(*) total_records, avg(ping) total_avg_ping\n",
							"FROM user_ping\n",
							"GROUP BY user_id\n",
							"ORDER BY user_id ASC;\n",
							"\"\"\")\n",
							"    \n",
							"DA.generate_summary = _generate_summary\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-12-2-1L')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "e99603de-46d1-440f-8dbc-34c07ec19ef7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def print_sql(self, rows, sql):\n",
							"    displayHTML(f\"\"\"<body><textarea style=\"width:100%\" rows={rows}> \\n{sql.strip()}</textarea></body>\"\"\")\n",
							"    "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def generate_daily_patient_avg(self):\n",
							"    sql = f\"SELECT * FROM {DA.db_name}.daily_patient_avg\"\n",
							"    self.print_sql(3, sql)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def generate_visualization_query(self):\n",
							"    sql = f\"\"\"\n",
							"SELECT flow_name, timestamp, int(details:flow_progress:metrics:num_output_rows) num_output_rows\n",
							"FROM {DA.db_name}.dlt_metrics\n",
							"ORDER BY timestamp DESC;\"\"\"\n",
							"    \n",
							"    self.print_sql(5, sql)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"generate_register_dlt_event_metrics_sql_string = \"\"\n",
							"\n",
							"@DBAcademyHelper.monkey_patch\n",
							"def generate_register_dlt_event_metrics_sql(self):\n",
							"    global generate_register_dlt_event_metrics_sql_string\n",
							"    \n",
							"    generate_register_dlt_event_metrics_sql_string = f\"\"\"\n",
							"CREATE TABLE IF NOT EXISTS {DA.db_name}.dlt_events\n",
							"LOCATION '{DA.paths.working_dir}/storage/system/events';\n",
							"\n",
							"CREATE VIEW IF NOT EXISTS {DA.db_name}.dlt_success AS\n",
							"SELECT * FROM {DA.db_name}.dlt_events\n",
							"WHERE details:flow_progress:metrics IS NOT NULL;\n",
							"\n",
							"CREATE VIEW IF NOT EXISTS {DA.db_name}.dlt_metrics AS\n",
							"SELECT timestamp, origin.flow_name, details \n",
							"FROM {DA.db_name}.dlt_success\n",
							"ORDER BY timestamp DESC;\"\"\".strip()\n",
							"    \n",
							"    self.print_sql(13, generate_register_dlt_event_metrics_sql_string)\n",
							"    "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def get_pipeline_config(self):\n",
							"    notebook = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None)\n",
							"    notebook = \"/\".join(notebook.split(\"/\")[:-1]) + \"/DE 12.2.2L - DLT Task\"\n",
							"    \n",
							"    job_name = f\"Cap-12-{DA.username}\"\n",
							"    \n",
							"    return job_name, notebook\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def print_pipeline_config(self):\n",
							"    \n",
							"    job_name, notebook = self.get_pipeline_config()\n",
							"\n",
							"    displayHTML(f\"\"\"<table style=\"width:100%\">\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Pipeline Name:</td>\n",
							"        <td><input type=\"text\" value=\"{job_name}\" style=\"width:100%\"></td></tr>\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Notebook Path:</td>\n",
							"        <td><input type=\"text\" value=\"{notebook}\" style=\"width:100%\"></td></tr>\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Datsets Path:</td>\n",
							"        <td><input type=\"text\" value=\"{DA.paths.datasets}\" style=\"width:100%\"></td></tr>\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Source:</td>\n",
							"        <td><input type=\"text\" value=\"{DA.paths.stream_path}\" style=\"width:100%\"></td></tr>\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Target:</td>\n",
							"        <td><input type=\"text\" value=\"{DA.db_name}\" style=\"width:100%\"></td></tr>\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Storage Location:</td>\n",
							"        <td><input type=\"text\" value=\"{DA.paths.storage_location}\" style=\"width:100%\"></td></tr>\n",
							"    </table>\"\"\")\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def create_pipeline(self):\n",
							"    \"Provided by DBAcademy, this function creates the prescribed pipline\"\n",
							"    \n",
							"    pipeline_name, path = self.get_pipeline_config()\n",
							"\n",
							"    # We need to delete the existing pipline so that we can apply updates\n",
							"    # because some attributes are not mutable after creation.\n",
							"    self.client.pipelines().delete_by_name(pipeline_name)\n",
							"    \n",
							"    response = self.client.pipelines().create(\n",
							"        name = pipeline_name, \n",
							"        storage = DA.paths.storage_location, \n",
							"        target = DA.db_name, \n",
							"        notebooks = [path],\n",
							"        continuous = True,\n",
							"        development = self.is_smoke_test(), # When testing, don't use production\n",
							"        configuration = {\n",
							"            \"spark.master\": \"local[*]\",\n",
							"            \"datasets_path\": DA.paths.datasets,\n",
							"            \"source\": DA.paths.stream_path,\n",
							"        },\n",
							"        clusters=[{ \"label\": \"default\", \"num_workers\": 0 }])\n",
							"    \n",
							"    pipeline_id = response.get(\"pipeline_id\")\n",
							"    print(f\"Created pipline {pipeline_id}\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def validate_pipeline_config(self):\n",
							"    \"Provided by DBAcademy, this function validates the configuration of the pipeline\"\n",
							"    import json\n",
							"    \n",
							"    pipeline_name, path = self.get_pipeline_config()\n",
							"\n",
							"    pipeline = self.client.pipelines().get_by_name(pipeline_name)\n",
							"    assert pipeline is not None, f\"The pipline named \\\"{pipeline_name}\\\" doesn't exist. Double check the spelling.\"\n",
							"\n",
							"    spec = pipeline.get(\"spec\")\n",
							"    # print(json.dumps(spec, indent=4))\n",
							"    \n",
							"    storage = spec.get(\"storage\")\n",
							"    assert storage == DA.paths.storage_location, f\"Invalid storage location. Found \\\"{storage}\\\", expected \\\"{DA.paths.storage_location}\\\" \"\n",
							"    \n",
							"    target = spec.get(\"target\")\n",
							"    assert target == DA.db_name, f\"Invalid target. Found \\\"{target}\\\", expected \\\"{DA.db_name}\\\" \"\n",
							"    \n",
							"    libraries = spec.get(\"libraries\")\n",
							"    assert libraries is None or len(libraries) > 0, f\"The notebook path must be specified.\"\n",
							"    assert len(libraries) == 1, f\"More than one library (e.g. notebook) was specified.\"\n",
							"    first_library = libraries[0]\n",
							"    assert first_library.get(\"notebook\") is not None, f\"Incorrect library configuration - expected a notebook.\"\n",
							"    first_library_path = first_library.get(\"notebook\").get(\"path\")\n",
							"    assert first_library_path == path, f\"Invalid notebook path. Found \\\"{first_library_path}\\\", expected \\\"{path}\\\" \"\n",
							"\n",
							"    configuration = spec.get(\"configuration\")\n",
							"    assert configuration is not None, f\"The three configuration parameters were not specified.\"\n",
							"    datasets_path = configuration.get(\"datasets_path\")\n",
							"    assert datasets_path == DA.paths.datasets, f\"Invalid \\\"datasets_path\\\" value. Found \\\"{datasets_path}\\\", expected \\\"{DA.paths.datasets}\\\".\"\n",
							"    spark_master = configuration.get(\"spark.master\")\n",
							"    assert spark_master == f\"local[*]\", f\"Invalid \\\"spark.master\\\" value. Expected \\\"local[*]\\\", found \\\"{spark_master}\\\".\"\n",
							"    stream_source = configuration.get(\"source\")\n",
							"    assert stream_source == DA.paths.stream_path, f\"Invalid \\\"source\\\" value. Expected \\\"{DA.paths.stream_path}\\\", found \\\"{stream_source}\\\".\"\n",
							"    \n",
							"    cluster = spec.get(\"clusters\")[0]\n",
							"    autoscale = cluster.get(\"autoscale\")\n",
							"    assert autoscale is None, f\"Autoscaling should be disabled.\"\n",
							"    \n",
							"    num_workers = cluster.get(\"num_workers\")\n",
							"    assert num_workers == 0, f\"Expected the number of workers to be 0, found {num_workers}.\"\n",
							"\n",
							"    development = spec.get(\"development\")\n",
							"    assert development == self.is_smoke_test(), f\"The pipline mode should be set to \\\"Production\\\".\"\n",
							"    \n",
							"    channel = spec.get(\"channel\")\n",
							"    assert channel is None or channel == \"CURRENT\", f\"Expected the channel to be Current but found {channel}.\"\n",
							"    \n",
							"    photon = spec.get(\"photon\")\n",
							"    assert photon == True, f\"Expected Photon to be enabled.\"\n",
							"    \n",
							"    continuous = spec.get(\"continuous\")\n",
							"    assert continuous == True, f\"Expected the Pipeline mode to be \\\"Continuous\\\", found \\\"Triggered\\\".\"\n",
							"\n",
							"    policy = self.client.cluster_policies.get_by_name(\"Student's DLT-Only Policy\")\n",
							"    if policy is not None:\n",
							"        cluster = { \n",
							"            \"num_workers\": 0,\n",
							"            \"label\": \"default\", \n",
							"            \"policy_id\": policy.get(\"policy_id\")\n",
							"        }\n",
							"        self.client.pipelines.create_or_update(name = pipeline_name,\n",
							"                                               storage = DA.paths.storage_location,\n",
							"                                               target = DA.db_name,\n",
							"                                               notebooks = [path],\n",
							"                                               continuous = True,\n",
							"                                               development = False,\n",
							"                                               configuration = {\n",
							"                                                   \"spark.master\": \"local[*]\",\n",
							"                                                   \"datasets_path\": DA.paths.datasets,\n",
							"                                                   \"source\": DA.paths.stream_path,\n",
							"                                               },\n",
							"                                               clusters=[cluster])\n",
							"    print(\"All tests passed!\")\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def get_job_config(self):\n",
							"    \n",
							"    root = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None)\n",
							"    root = \"/\".join(root.split(\"/\")[:-1])\n",
							"    \n",
							"    notebook = f\"{root}/DE 12.2.3L - Land New Data\"\n",
							"\n",
							"    job_name = f\"Cap-12-{DA.username}\"\n",
							"    \n",
							"    return job_name, notebook"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def print_job_config(self):\n",
							"    \n",
							"    job_name, notebook = self.get_job_config()\n",
							"\n",
							"    displayHTML(f\"\"\"<table style=\"width:100%\">\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Job Name:</td>\n",
							"        <td><input type=\"text\" value=\"{job_name}\" style=\"width:100%\"></td></tr>\n",
							"    <tr>\n",
							"        <td style=\"white-space:nowrap; width:1em\">Notebook Path:</td>\n",
							"        <td><input type=\"text\" value=\"{notebook}\" style=\"width:100%\"></td></tr>\n",
							"    </table>\"\"\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def create_job(self):\n",
							"    \"Provided by DBAcademy, this function creates the prescribed job\"\n",
							"    \n",
							"    job_name, notebook = self.get_job_config()\n",
							"\n",
							"    self.client.jobs.delete_by_name(job_name, success_only=False)\n",
							"    cluster_id = dbgems.get_tags().get(\"clusterId\")\n",
							"    \n",
							"    params = {\n",
							"        \"name\": job_name,\n",
							"        \"tags\": {\n",
							"            \"dbacademy.course\": self.course_name,\n",
							"            \"dbacademy.source\": self.course_name\n",
							"        },\n",
							"        \"email_notifications\": {},\n",
							"        \"timeout_seconds\": 7200,\n",
							"        \"max_concurrent_runs\": 1,\n",
							"        \"format\": \"MULTI_TASK\",\n",
							"        \"tasks\": [\n",
							"            {\n",
							"                \"task_key\": \"Land-Data\",\n",
							"                \"libraries\": [],\n",
							"                \"notebook_task\": {\n",
							"                    \"notebook_path\": notebook,\n",
							"                    \"base_parameters\": []\n",
							"                },\n",
							"                \"existing_cluster_id\": cluster_id\n",
							"            },\n",
							"        ],\n",
							"    }\n",
							"    params = self.update_cluster_params(params, [0])\n",
							"    \n",
							"    create_response = self.client.jobs().create(params)\n",
							"    job_id = create_response.get(\"job_id\")\n",
							"    \n",
							"    print(f\"Created job #{job_id}\")\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def validate_job_config(self):\n",
							"    \"Provided by DBAcademy, this function validates the configuration of the job\"\n",
							"    import json\n",
							"    \n",
							"    pipeline_name, job_path = self.get_pipeline_config()\n",
							"    job_name, notebook = self.get_job_config()\n",
							"\n",
							"    job = self.client.jobs.get_by_name(job_name)\n",
							"    assert job is not None, f\"The job named \\\"{job_name}\\\" doesn't exist. Double check the spelling.\"\n",
							"    \n",
							"    settings = job.get(\"settings\")\n",
							"    assert settings.get(\"format\") == \"SINGLE_TASK\", f\"\"\"Expected only one task.\"\"\"\n",
							"\n",
							"    # Land-Data Task\n",
							"#     task_name = settings.get(\"task_key\", None)\n",
							"#     assert task_name == \"Land-Data\", f\"Expected the first task to have the name \\\"Land-Data\\\", found \\\"{task_name}\\\"\"\n",
							"    \n",
							"    notebook_path = settings.get(\"notebook_task\", {}).get(\"notebook_path\")\n",
							"    assert notebook_path == notebook, f\"Invalid Notebook Path for the first task. Found \\\"{notebook_path}\\\", expected \\\"{notebook}\\\" \"\n",
							"\n",
							"    if not self.is_smoke_test():\n",
							"        # Don't check the actual_cluster_id when running as a smoke test\n",
							"        \n",
							"        actual_cluster_id = settings.get(\"existing_cluster_id\", None)\n",
							"        assert actual_cluster_id is not None, f\"The first task is not configured to use the current All-Purpose cluster\"\n",
							"\n",
							"        expected_cluster_id = dbgems.get_tags().get(\"clusterId\")\n",
							"        if expected_cluster_id != actual_cluster_id:\n",
							"            actual_cluster = self.client.clusters.get(actual_cluster_id).get(\"cluster_name\")\n",
							"            expected_cluster = self.client.clusters.get(expected_cluster_id).get(\"cluster_name\")\n",
							"            assert actual_cluster_id == expected_cluster_id, f\"The first task is not configured to use the current All-Purpose cluster, expected \\\"{expected_cluster}\\\", found \\\"{actual_cluster}\\\"\"\n",
							"\n",
							"    schedule = settings.get(\"schedule\")\n",
							"    if schedule is None:\n",
							"        print(\"WARNING: The job has not been scheduled.\\n\")\n",
							"    else:\n",
							"        pause_status = schedule.get(\"pause_status\")\n",
							"        if pause_status == \"PAUSED\":\n",
							"            print(\"WARNING: The job should not be paused.\\n\")\n",
							"        else:\n",
							"            quartz_cron_expression = schedule.get(\"quartz_cron_expression\")\n",
							"            if \"0/2 * * * ?\" not in quartz_cron_expression:\n",
							"                print(f\"WARNING: Expected the schedule to be \\\"* 0/2 * * * ?\\\" but found \\\"{quartz_cron_expression}\\\".\\n\")\n",
							"    \n",
							"    print(\"All tests passed!\")\n",
							"    "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def start_job(self):\n",
							"    job_name, notebook = self.get_job_config()\n",
							"    job_id = self.client.jobs.get_by_name(job_name).get(\"job_id\")\n",
							"    run_id = self.client.jobs.run_now(job_id).get(\"run_id\")\n",
							"    print(f\"Started job #{job_id}, run #{run_id}\")\n",
							"\n",
							"    self.client.runs.wait_for(run_id)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(lesson=\"cap_12\", **helper_arguments)\n",
							"DA.reset_environment()\n",
							"DA.init(install_datasets=True, create_db=True)\n",
							"\n",
							"DA.paths.stream_path = f\"{DA.paths.working_dir}/stream\"\n",
							"DA.paths.storage_location = f\"{DA.paths.working_dir}/storage\"\n",
							"\n",
							"DA.data_factory = DltDataFactory(DA.paths.stream_path)\n",
							"\n",
							"DA.conclude_setup()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-12-2-3L')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "dda922dd-7ed4-43ca-a76e-47cb085b71be"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(lesson=\"cap_12\", **helper_arguments)\n",
							"# DA.reset_environment() # We don't want to reset the environment\n",
							"DA.init(install_datasets=True, create_db=False)\n",
							"\n",
							"DA.paths.stream_path = f\"{DA.paths.working_dir}/stream\"\n",
							"DA.data_factory = DltDataFactory(DA.paths.stream_path)\n",
							"\n",
							"DA.conclude_setup()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Classroom-Setup-12-2-4L')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "6e01341c-67ac-41a9-9526-302582db2425"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(lesson=\"cap_12\", **helper_arguments)\n",
							"# DA.reset_environment() # We don't want to reset the environment\n",
							"DA.init(install_datasets=True, create_db=False)\n",
							"DA.conclude_setup()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Configure-Permissions')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "31247b28-66a0-4bca-adbe-b7a69444a47a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"GRANT CREATE ON CATALOG TO users\")\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 1-1 - Create and Manage Interactive Clusters')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "01 - Synapse Workspace and Services"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "LakehouseTest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "daf6dac8-d42b-42ba-808c-734d31ea0e0a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/8aa90780-bc26-461d-9db6-6df705873871/resourceGroups/domo-demo/providers/Microsoft.Synapse/workspaces/domodemo/bigDataPools/LakehouseTest",
						"name": "LakehouseTest",
						"type": "Spark",
						"endpoint": "https://domodemo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/LakehouseTest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Create and Manage Interactive Clusters\n",
							"\n",
							"A Spark cluster is a set of computation resources and configurations on which you run data engineering, data science, and data analytics workloads, such as production ETL pipelines, streaming analytics, ad-hoc analytics, and machine learning. You run these workloads as a set of commands in a notebook or as an automated job. \n",
							"\n",
							"Unlike Databricks, Synapse Spark has no distinction between all-purpose clusters and job clusters. \n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lesson, you should be able to:\n",
							"* Use the Clusters UI to configure and deploy a cluster\n",
							"* Edit, terminate, restart, and delete clusters"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Create a Spark Pool\n",
							"\n",
							"In Synapse, a cluster of computing resources is called a \"Pool\". Like the SQL Pool for Data Warehousing activities, we have the Spark Pool for scalable Big Data workloads such as the Data Lakehouse.\n",
							"Depending on the workspace in which you're currently working, you may or may not have pool creation privileges. \n",
							"\n",
							"Instructions in this section assume that you **do** have pool creation privileges, and that you need to deploy a new pool to execute the lessons in this course.\n",
							"\n",
							"**NOTE**: Check with your instructor or a platform admin to confirm whether or not you should create a new pool or connect to a pool that has already been deployed. Pool policies may impact your options for pool configuration. \n",
							"\n",
							"Steps:\n",
							"1. On the left of the screen, go to the **manage** tab\n",
							"1. Click **Apache Spark Pools**\n",
							"1. Click the **New** button\n",
							"1. For the **Apache Spark pool name**, use your name so that you can find it easily and the instructor can easily identify it if you have problems\n",
							"1. Set the **Node size family** to **Memory Optimized**\n",
							"1. Set the **Node size** to **Small**\n",
							"1. Set **Autoscale** to **disabled**\n",
							"1. Slide the **number of nodes** down to **3**\n",
							"1. Leave **Dynamically allocate executors** to **Disabled**\n",
							"1. Click **Review + create** to review all settings\n",
							"1. Click **Create** to start creating the cluster\n",
							"\n",
							"**NOTE:** Clusters can take several minutes to deploy. Once you have finished deploying a cluster, feel free to continue to explore the cluster creation UI."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Manage Pools\n",
							"\n",
							"Once the pool is created, go back to the **Apache Spark Pools** page inside the **manage** tab to view the cluster.\n",
							"\n",
							"Select a cluster to review the current configuration. \n",
							"\n",
							"Click the **Ellipsis** button. Note that you can modify several settings in an existing pool:\n",
							"\n",
							"* Scale settings (number of nodes, autoscale, dynamic allocation of executors)\n",
							"* Packages\n",
							"* Pause settings (whether the pool should be automatically paused or not)\n",
							"\n",
							"Changing most settings will require running pools to be restarted.\n",
							"\n",
							"**NOTE**: We'll be using our pool in the following lesson. Restarting, terminating, or deleting your pool may put you behind as you wait for new resources to be deployed."
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 1-2 - Notebook Basics')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "01 - Synapse Workspace and Services"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "LakehouseTest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "fc596c79-e23a-4d13-a5bf-9b1a42d31f37"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/8aa90780-bc26-461d-9db6-6df705873871/resourceGroups/domo-demo/providers/Microsoft.Synapse/workspaces/domodemo/bigDataPools/LakehouseTest",
						"name": "LakehouseTest",
						"type": "Spark",
						"endpoint": "https://domodemo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/LakehouseTest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Notebook Basics\r\n",
							"\r\n",
							"Notebooks are the primary means of developing and executing code interactively on Databricks. This lesson provides a basic introduction to working with Databricks notebooks.\r\n",
							"\r\n",
							"If you've previously used Databricks notebooks but this is your first time executing a notebook in Databricks Repos, you'll notice that basic functionality is the same. In the next lesson, we'll review some of the functionality that Databricks Repos adds to notebooks.\r\n",
							"\r\n",
							"## Learning Objectives\r\n",
							"By the end of this lesson, you should be able to:\r\n",
							"* Attach a notebook to a cluster\r\n",
							"* Execute a cell in a notebook\r\n",
							"* Set the language for a notebook\r\n",
							"* Describe and use magic commands\r\n",
							"* Create and run a SQL cell\r\n",
							"* Create and run a Python cell\r\n",
							"* Create a markdown cell\r\n",
							"* Export a Databricks notebook\r\n",
							"* Export a collection of Databricks notebooks"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Attach to a Cluster\r\n",
							"\r\n",
							"In the previous lesson, you should have either deployed a cluster or identified a cluster that an admin has configured for you to use.\r\n",
							"\r\n",
							"Directly below the name of this notebook at the top of your screen, use the drop-down list to connect this notebook to your cluster.\r\n",
							"\r\n",
							"**NOTE**: Deploying a cluster can take several minutes. A green arrow will appear to the right of the cluster name once resources have been deployed. If your cluster has a solid gray circle to the left, you will need to follow instructions to <a href=\"https://docs.databricks.com/clusters/clusters-manage.html#start-a-cluster\" target=\"_blank\">start a cluster</a>.\r\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Notebooks Basics\r\n",
							"\r\n",
							"Notebooks provide cell-by-cell execution of code. Multiple languages can be mixed in a notebook. Users can add plots, images, and markdown text to enhance their code.\r\n",
							"\r\n",
							"Throughout this course, our notebooks are designed as learning instruments. Notebooks can be easily deployed as production code with Databricks, as well as providing a robust toolset for data exploration, reporting, and dashboarding.\r\n",
							"\r\n",
							"### Running a Cell\r\n",
							"* Run the cell below using one of the following options:\r\n",
							"  * **CTRL+ENTER** or **CTRL+RETURN**\r\n",
							"  * **SHIFT+ENTER** or **SHIFT+RETURN** to run the cell and move to the next one\r\n",
							"  * Using **Run Cell**, **Run All Above** or **Run All Below** as seen here\r\n",
							"  ![Run all below](https://files.training.databricks.com/images/notebook-cell-run-cmd.png)"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"print(\"I'm running Python!\")"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**NOTE**: Cell-by-cell code execution means that cells can be executed multiple times or out of order. Unless explicitly instructed, you should always assume that the notebooks in this course are intended to be run one cell at a time from top to bottom. If you encounter an error, make sure you read the text before and after a cell to ensure that the error wasn't an intentional learning moment before you try to troubleshoot. Most errors can be resolved by either running earlier cells in a notebook that were missed or re-executing the entire notebook from the top.\r\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Setting the Default Notebook Language\r\n",
							"\r\n",
							"The cell above executes a Python command, because our current default language for the notebook is set to Python.\r\n",
							"\r\n",
							"Synapse notebooks support the following languages:\r\n",
							"\r\n",
							"* PySpark (Python)\r\n",
							"* Spark SQL (SQL)\r\n",
							"* Spark (Scala)\r\n",
							"* .Net Spark (C#)\r\n",
							"\r\n",
							"A language can be selected when a notebook is created, but this can be changed at any time.\r\n",
							"\r\n",
							"The default language appears directly to the right of the notebook title at the top of the page. Throughout this course, we'll use a blend of SQL and Python notebooks.\r\n",
							"\r\n",
							"We'll change the default language for this notebook to SQL.\r\n",
							"\r\n",
							"Steps:\r\n",
							"* Click on the **PySpark (Python)** next to the notebook title at the top of the screen\r\n",
							"* In the dropdown that appears, select **Spark SQL**\r\n",
							"\r\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Create and Run a SQL Cell\r\n",
							"\r\n",
							"* Highlight this cell and press the **B** button on the keyboard to create a new cell below\r\n",
							"* Copy the following code into the cell below and then run the cell\r\n",
							"\r\n",
							"**`%%sql`**  \r\n",
							"**`SELECT \"I'm running SQL!\"`**\r\n",
							"\r\n",
							"**NOTE**: There are a number of different methods for adding, moving, and deleting cells including GUI options and keyboard shortcuts. Refer to the <a href=\"https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-development-using-notebooks\" target=\"_blank\">docs</a> for details.\r\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Magic Commands\r\n",
							"* Magic commands are specific to the Databricks notebooks\r\n",
							"* They are very similar to magic commands found in comparable notebook products\r\n",
							"* These are built-in commands that provide the same outcome regardless of the notebook's language\r\n",
							"* A double percent (%%) symbol at the start of a cell identifies a magic command\r\n",
							"  * You can only have one magic command per cell\r\n",
							"  * A magic command must be the first thing in a cell"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Language Magics\r\n",
							"Language magic commands allow for the execution of code in languages other than the notebook's default. In this course, we'll see the following language magics:\r\n",
							"* **`%%pyspark`** for Python (PySpark)\r\n",
							"* **`%%sql`** for SparkSQL\r\n",
							"\r\n",
							"Adding the language magic for the currently set notebook type is not necessary.\r\n",
							"\r\n",
							"**NOTE**: Rather than changing the default language of a notebook constantly, you should stick with a primary language as the default and only use language magics as necessary to execute code in another language.\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"Hello Python!\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"select \"Hello SQL!\""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Markdown\r\n",
							"\r\n",
							"Synapse doesn't have a \"magic command\" for Markdown, but has a split into two types of cells instead:\r\n",
							"\r\n",
							"* All \"Code\" cells always go to the Spark cluster\r\n",
							"* All \"Markdown\" cells are interpreted as Markdown without the need of a cluster.\r\n",
							"\r\n",
							"* Double click this cell to begin editing it\r\n",
							"* Then hit **`Esc`** to stop editing\r\n",
							"\r\n",
							"# Title One\r\n",
							"## Title Two\r\n",
							"### Title Three\r\n",
							"\r\n",
							"This is a test of the emergency broadcast system. This is only a test.\r\n",
							"\r\n",
							"This is text with a **bold** word in it.\r\n",
							"\r\n",
							"This is text with an *italicized* word in it.\r\n",
							"\r\n",
							"This is an ordered list\r\n",
							"1. once\r\n",
							"1. two\r\n",
							"1. three\r\n",
							"\r\n",
							"This is an unordered list\r\n",
							"* apples\r\n",
							"* peaches\r\n",
							"* bananas\r\n",
							"\r\n",
							"Links/Embedded HTML: <a href=\"https://en.wikipedia.org/wiki/Markdown\" target=\"_blank\">Markdown - Wikipedia</a>\r\n",
							"\r\n",
							"Images:\r\n",
							"![Spark Engines](https://files.training.databricks.com/images/Apache-Spark-Logo_TM_200px.png)\r\n",
							"\r\n",
							"And of course, tables:\r\n",
							"\r\n",
							"| name   | value |\r\n",
							"|--------|-------|\r\n",
							"| Yi     | 1     |\r\n",
							"| Ali    | 2     |\r\n",
							"| Selina | 3     |"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### %run\r\n",
							"* You can run a notebook from another notebook by using the magic command **%run**\r\n",
							"* Notebooks to be run are specified with relative paths\r\n",
							"* The referenced notebook executes as if it were part of the current notebook, so temporary views and other local declarations will be available from the calling notebook\r\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Uncommenting and executing the following cell will generate the following error:<br/>\r\n",
							"**`Error: Table or view not found: demo_tmp_vw (...)`**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\r\n",
							"SELECT * FROM demo_tmp_vw"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"But we can declare it and a handful of other variables and functions buy running this cell:"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run /Includes/Classroom-Setup-01-2"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"The **`../Includes/Classroom-Setup-01.2`** notebook we referenced includes logic to create and **`USE`** a database, as well as creating the temp view **`demo_temp_vw`**.\r\n",
							"\r\n",
							"We can see this temp view is now available in our current notebook session with the following query."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%sql \r\n",
							"SELECT * FROM demo_tmp_vw"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"We'll use this pattern of \"setup\" notebooks throughout the course to help configure the environment for lessons and labs.\r\n",
							"\r\n",
							"These \"provided\" variables, functions and other objects should be easily identifiable in that they are part of the **`DA`** object which is an instance of **`DBAcademyHelper`**.\r\n",
							"\r\n",
							"With that in mind, most lessons will use variables derived from your username to organize files and databases. \r\n",
							"\r\n",
							"This pattern allows us to avoid collision with other users in shared a workspace.\r\n",
							"\r\n",
							"The cell below uses Python to print some of those variables previously defined in this notebook's setup script"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(f\"DA:                   {DA}\")\r\n",
							"print(f\"DA.username:          {DA.username}\")\r\n",
							"print(f\"DA.paths.working_dir: {DA.paths.working_dir}\")\r\n",
							"print(f\"DA.db_name:           {DA.db_name}\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"In addition to this, these same variables are \"injected\" into the SQL context so that we can use them in SQL statements.\r\n",
							"\r\n",
							"We will talk more about this later, but you can see a quick example in the following cell.\r\n",
							"\r\n",
							"<img src=\"https://files.training.databricks.com/images/icon_note_32.png\"> Note the subtle but important difference in the casing of the word **`da`** and **`DA`** in these two examples.\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%sql\r\n",
							"SELECT '${da.username}' AS current_username,\r\n",
							"       '${da.paths.working_dir}' AS working_directory,\r\n",
							"       '${da.db_name}' as database_name\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Databricks Utilities\r\n",
							"Databricks notebooks provide a number of utility commands for configuring and interacting with the environment: <a href=\"https://docs.databricks.com/user-guide/dev-tools/dbutils.html\" target=\"_blank\">dbutils docs</a>\r\n",
							"\r\n",
							"Throughout this course, we'll occasionally use **`dbutils.fs.ls()`** to list out directories of files from Python cells."
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"path = f\"{DA.paths.datasets}\"\r\n",
							"dbutils.fs.ls(path)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## display()\r\n",
							"\r\n",
							"When running SQL queries from cells, results will always be displayed in a rendered tabular format.\r\n",
							"\r\n",
							"When we have tabular data returned by a Python cell, we can call **`display`** to get the same type of preview.\r\n",
							"\r\n",
							"Here, we'll wrap the previous list command on our file system with **`display`**."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"path = f\"{DA.paths.datasets}\"\r\n",
							"files = dbutils.fs.ls(path)\r\n",
							"display(files)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"The **`display()`** command has the following capabilities and limitations:\r\n",
							"* Preview of results limited to 1000 records\r\n",
							"* Provides button to download results data as CSV\r\n",
							"* Allows rendering plots"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Downloading Notebooks\r\n",
							"\r\n",
							"There are a number of options for downloading either individual notebooks or collections of notebooks.\r\n",
							"\r\n",
							"Here, you'll go through the process to download this notebook as well as a collection of all the notebooks in this course.\r\n",
							"\r\n",
							"### Download a Notebook\r\n",
							"\r\n",
							"Steps:\r\n",
							"* Click the **File** option to the right of the cluster selection at the top of the notebook\r\n",
							"* From the menu that appears, hover over **Export** and then select **Source File**\r\n",
							"\r\n",
							"The notebook will download to your personal laptop. It will be named with the current notebook name and have the file extension for the default language. You can open this notebook with any file editor and see the raw contents of Databricks notebooks.\r\n",
							"\r\n",
							"These source files can be uploaded into any Databricks workspace.\r\n",
							"\r\n",
							"### Download a Collection of Notebooks\r\n",
							"\r\n",
							"**NOTE**: The following instructions assume you have imported these materials using **Repos**.\r\n",
							"\r\n",
							"Steps:\r\n",
							"* Click the  ![](https://files.training.databricks.com/images/repos-icon.png) **Repos** on the left sidebar\r\n",
							"  * This should give you a preview of the parent directories for this notebook\r\n",
							"* On the left side of the directory preview around the middle of the screen, there should be a left arrow. Click this to move up in your file hierarchy.\r\n",
							"* You should see a directory called **Data Engineering with Databricks**. Click the the down arrow/chevron to bring up a menu\r\n",
							"* From the menu, hover over **Export** and select **DBC Archive**\r\n",
							"\r\n",
							"The DBC(Databricks Cloud) file that is downloaded contains a zipped collection of the directories and notebooks in this course. Users should not attempt to edit these DBC files locally, but they can be safely uploaded into any Databricks workspace to move or share notebook contents.\r\n",
							"\r\n",
							"**NOTE**: When downloading a collection of DBCs, result previews and plots will also be exported. When downloading source notebooks, only code will be saved.\r\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Learning More\r\n",
							"\r\n",
							"We like to encourage you to explore the documentation to learn more about the various features of the Databricks platform and notebooks.\r\n",
							"* <a href=\"https://docs.databricks.com/user-guide/index.html#user-guide\" target=\"_blank\">User Guide</a>\r\n",
							"* <a href=\"https://docs.databricks.com/user-guide/getting-started.html\" target=\"_blank\">Getting Started with Databricks</a>\r\n",
							"* <a href=\"https://docs.databricks.com/user-guide/notebooks/index.html\" target=\"_blank\">User Guide / Notebooks</a>\r\n",
							"* <a href=\"https://docs.databricks.com/notebooks/notebooks-manage.html#notebook-external-formats\" target=\"_blank\">Importing notebooks - Supported Formats</a>\r\n",
							"* <a href=\"https://docs.databricks.com/repos/index.html\" target=\"_blank\">Repos</a>\r\n",
							"* <a href=\"https://docs.databricks.com/administration-guide/index.html#administration-guide\" target=\"_blank\">Administration Guide</a>\r\n",
							"* <a href=\"https://docs.databricks.com/user-guide/clusters/index.html\" target=\"_blank\">Cluster Configuration</a>\r\n",
							"* <a href=\"https://docs.databricks.com/api/latest/index.html#rest-api-2-0\" target=\"_blank\">REST API</a>\r\n",
							"* <a href=\"https://docs.databricks.com/release-notes/index.html#release-notes\" target=\"_blank\">Release Notes</a>"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## One more note! \r\n",
							"\r\n",
							"At the end of each lesson you will see the following command, **`DA.cleanup()`**.\r\n",
							"\r\n",
							"This method drops lesson-specific databases and working directories in an attempt to keep your workspace clean and maintain the immutability of each lesson.\r\n",
							"\r\n",
							"Run the following cell to delete the tables and files associated with this lesson."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"DA.cleanup()"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 1-3L - Getting Started with the Databricks Platform Lab')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "01 - Synapse Workspace and Services"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "LakehouseTest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "76564c94-bb5b-48ba-9688-ae763d9e83b4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/8aa90780-bc26-461d-9db6-6df705873871/resourceGroups/domo-demo/providers/Microsoft.Synapse/workspaces/domodemo/bigDataPools/LakehouseTest",
						"name": "LakehouseTest",
						"type": "Spark",
						"endpoint": "https://domodemo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/LakehouseTest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"\n",
							"# Getting Started with the Databricks Platform\n",
							"\n",
							"This notebook provides a hands-on review of some of the basic functionality of the Databricks Data Science and Engineering Workspace.\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lab, you should be able to:\n",
							"- Rename a notebook and change the default language\n",
							"- Attach a cluster\n",
							"- Use the **`%run`** magic command\n",
							"- Run Python and SQL cells\n",
							"- Create a Markdown cell"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"\n",
							"# Renaming a Notebook\n",
							"\n",
							"Changing the name of a notebook is easy. Click on the name at the top of this page, then make changes to the name. To make it easier to navigate back to this notebook later in case you need to, append a short test string to the end of the existing name."
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"\n",
							"# Attaching a cluster\n",
							"\n",
							"Executing cells in a notebook requires computing resources, which is provided by clusters. The first time you execute a cell in a notebook, you will be prompted to attach to a cluster if one is not already attached.\n",
							"\n",
							"Attach a cluster to this notebook now by clicking the dropdown near the top-left corner of this page. Select the cluster you created previously. This will clear the execution state of the notebook and connect the notebook to the selected cluster.\n",
							"\n",
							"Note that the dropdown menu provides the option of starting or restarting the cluster as needed. You can also detach and re-attach to a cluster in a single movement. This is useful for clearing the execution state when needed."
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"\n",
							"# Using %run\n",
							"\n",
							"Complex projects of any type can benefit from the ability to break them down into simpler, reusable components.\n",
							"\n",
							"In the context of Databricks notebooks, this facility is provided through the **`%run`** magic command.\n",
							"\n",
							"When used this way, variables, functions and code blocks become part of the current programming context.\n",
							"\n",
							"Consider this example:\n",
							"\n",
							"**`Notebook_A`** has four commands:\n",
							"  1. **`name = \"John\"`**\n",
							"  2. **`print(f\"Hello {name}\")`**\n",
							"  3. **`%run ./Notebook_B`**\n",
							"  4. **`print(f\"Welcome back {full_name}`**\n",
							"\n",
							"**`Notebook_B`** has only one commands:\n",
							"  1. **`full_name = f\"{name} Doe\"`**\n",
							"\n",
							"If we run **`Notebook_B`** it will fail to execute because the variable **`name`** is not defined in **`Notebook_B`**\n",
							"\n",
							"Likewise, one might think that **`Notebook_A`** would fail becase it uses the variable **`full_name`** which is likewise not defined in **`Notebook_A`**, but it doesn't!\n",
							"\n",
							"What actually happens is that the two notebooks are merged together as we see below and **then** executed:\n",
							"1. **`name = \"John\"`**\n",
							"2. **`print(f\"Hello {name}\")`**\n",
							"3. **`full_name = f\"{name} Doe\"`**\n",
							"4. **`print(f\"Welcome back {full_name}\")`**\n",
							"\n",
							"And thus providing the expected behavior:\n",
							"* **`Hello John`**\n",
							"* **`Welcome back John Doe`**"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"\n",
							"The folder that contains this notebook contains a subfolder named **`ExampleSetupFolder`**, which in turn contains a notebook called **`example-setup`**. \n",
							"\n",
							"This simple notebook declares the variable **`my_name`**, sets it to **`None`** and then creates a DataFrame called **`example_df`**. \n",
							"\n",
							"Open the example-setup notebook and modify it so that name is not **`None`** but rather your name (or anyone's name) enclosed in quotes, and so that the following two cells execute without throwing an **`AssertionError`**.\n",
							"\n",
							"<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"> You will see additional references **`_utility-methods`** and **`DBAcademyHelper`** which are used to this configure  courseware and should be ignored for this exercise."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ./ExampleSetupFolder/example-setup"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"assert my_name is not None, \"Name is still None\"\n",
							"print(my_name)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"\n",
							"## Run a Python cell\n",
							"\n",
							"Run the following cell to verify that the **`example-setup`** notebook was executed by displaying the **`example_df`** Dataframe. This table consists of 16 rows of increasing values."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"display(example_df)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"\n",
							"# Change Language\n",
							"\n",
							"Notice that the default language for this notebook is set to Python. Change this by clicking the **Python** button to the right of the notebook name. Change the default language to SQL.\n",
							"\n",
							"Notice that the Python cells are automatically prepended with a <strong><code>&#37;python</code></strong> magic command to maintain validity of those cells. Notice that this operation also clears the execution state."
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"\n",
							"# Create a Markdown Cell\n",
							"\n",
							"Add a new cell below this one. Populate with some Markdown that includes at least the following elements:\n",
							"* A header\n",
							"* Bullet points\n",
							"* A link (using your choice of HTML or Markdown conventions)"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"\n",
							"## Run a SQL cell\n",
							"\n",
							"Run the following cell to query a Delta table using SQL. This executes a simple query against a table is backed by a Databricks-provided example dataset included in all DBFS installations."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"SELECT * FROM delta.`${DA.paths.datasets}/nyctaxi-with-zipcodes/data`"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"\n",
							"Execute the following cell to view the underlying files backing this table."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"files = dbutils.fs.ls(f\"{DA.paths.datasets}/nyctaxi-with-zipcodes/data\")\n",
							"display(files)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"# Clearing notebook state\n",
							"\n",
							"Sometimes it is useful to clear all variables defined in the notebook and start from the begining.  This can be useful when you want to test cells in isolation, or you simply want to reset the execution state.\n",
							"\n",
							"Visit the **Clear** menu and select the **Clear State & Cell Outputs**.\n",
							"\n",
							"Now try running the cell below and notice the variables defined earlier are no longer defined, until you rerun the earlier cells above."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"print(my_name)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"\n",
							"# Review Changes\n",
							"\n",
							"Assuming you have imported this material into your workspace using a Databricks Repo, open the Repo dialog by clicking the **`published`** branch button at the top-left corner of this page. You should see three changes:\n",
							"1. **Removed** with the old notebook name\n",
							"1. **Added** with the new notebook name\n",
							"1. **Modified** for creating a markdown cell above\n",
							"\n",
							"Use the dialog to revert the changes and restore this notebook to its original state."
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"\n",
							"## Wrapping Up\n",
							"\n",
							"By completing this lab, you should now feel comfortable manipulating notebooks, creating new cells, and running notebooks within notebooks."
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 10-1 - Navigating Databricks SQL and Attaching to Endpoints')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "10 - Running a DBSQL Query"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ceb2c2a8-3461-4e01-a488-419e9a78aaaf"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Navigating Databricks SQL and Attaching to SQL Warehouses\n",
							"\n",
							"* Navigate to Databricks SQL  \n",
							"  * Make sure that SQL is selected from the workspace option in the sidebar (directly below the Databricks logo)\n",
							"* Make sure a SQL warehouse is on and accessible\n",
							"  * Navigate to SQL Warehouses in the sidebar\n",
							"  * If a SQL warehouse exists and has the State **`Running`**, you'll use this SQL warehouse\n",
							"  * If a SQL warehouse exists but is **`Stopped`**, click the **`Start`** button if you have this option (**NOTE**: Start the smallest SQL warehouse you have available to you) \n",
							"  * If no SQL warehouses exist and you have the option, click **`Create SQL Warehouse`**; name the SQL warehouse something you'll recognize and set the cluster size to 2X-Small. Leave all other options as default.\n",
							"  * If you have no way to create or attach to a SQL warehouse, you'll need to contact a workspace administrator and request access to compute resources in Databricks SQL to continue.\n",
							"* Navigate to home page in Databricks SQL\n",
							"  * Click the Databricks logo at the top of the side nav bar\n",
							"* Locate the **Sample dashboards** and click **`Visit gallery`**\n",
							"* Click **`Import`** next to the **Retail Revenue & Supply Chain** option\n",
							"  * Assuming you have a SQL warehouse available, this should load a dashboard and immediately display results\n",
							"  * Click **Refresh** in the top right (the underlying data has not changed, but this is the button that would be used to pick up changes)\n",
							"\n",
							"# Updating a DBSQL Dashboard\n",
							"\n",
							"* Use the sidebar navigator to find the **Dashboards**\n",
							"  * Locate the sample dashboard you just loaded; it should be called **Retail Revenue & Supply Chain** and have your username under the **`Created By`** field. **NOTE**: the **My Dashboards** option on the right hand side can serve as a shortcut to filter out other dashboards in the workspace\n",
							"  * Click on the dashboard name to view it\n",
							"* View the query behind the **Shifts in Pricing Priorities** plot\n",
							"  * Hover over the plot; three vertical dots should appear. Click on these\n",
							"  * Select **View Query** from the menu that appears\n",
							"* Review the SQL code used to populate this plot\n",
							"  * Note that 3 tier namespacing is used to identify the source table; this is a preview of new functionality to be supported by Unity Catalog\n",
							"  * Click **`Run`** in the top right of the screen to preview the results of the query\n",
							"* Review the visualization\n",
							"  * Under the query, a tab named **Table** should be selected; click **Price by Priority over Time** to switch to a preview of your plot\n",
							"  * Click **Edit Visualization** at the bottom of the screen to review settings\n",
							"  * Explore how changing settings impacts your visualization\n",
							"  * If you wish to apply your changes, click **Save**; otherwise, click **Cancel**\n",
							"* Back in the query editor, click the **Add Visualization** button to the right of the visualization name\n",
							"  * Create a bar graph\n",
							"  * Set the **X Column** as **`Date`**\n",
							"  * Set the **Y Column** as **`Total Price`**\n",
							"  * **Group by** **`Priority`**\n",
							"  * Set **Stacking** to **`Stack`**\n",
							"  * Leave all other settings as defaults\n",
							"  * Click **Save**\n",
							"* Back in the query editor, click the default name for this visualization to edit it; change the visualization name to **`Stacked Price`**\n",
							"* Click the three vertical dots to the right of the name of the visualization you just changed\n",
							"  * Select **Add to Dashboard** from the menu\n",
							"  * Select your **`Retail Revenue & Supply Chain`** dashboard\n",
							"* Navigate back to your dashboard to view this change\n",
							"\n",
							"# Create a New Query\n",
							"\n",
							"* Use the sidebar to navigate to **Queries**\n",
							"* Click the **`Create Query`** button\n",
							"* Make sure you are connected to a SQL warehouse. In the **Schema Browser**, click on the current metastore and select **`samples`**. \n",
							"  * Select the **`tpch`** database\n",
							"  * Click on the **`partsupp`** table to get a preview of the schema\n",
							"  * While hovering over the **`partsupp`** table name, click the **>>** button to insert the table name into your query text\n",
							"* Write your first query:\n",
							"  * **`SELECT * FROM`** the **`partsupp`** table using the full name imported in the last step; click **Run** to preview results\n",
							"  * Modify this query to **`GROUP BY ps_partkey`** and return the **`ps_partkey`** and **`sum(ps_availqty)`**; click **Run** to preview results\n",
							"  * Update your query to alias the 2nd column to be named **`total_availqty`** and re-execute the query\n",
							"* Save your query\n",
							"  * Click the **Save** button next to **Run** near the top right of the screen\n",
							"  * Give the query a name you'll remember\n",
							"* Add the query to your dashboard\n",
							"  * Click the three vertical buttons at the bottom of the screen\n",
							"  * Click **Add to Dashboard**\n",
							"  * Select your **`Retail Revenue & Supply Chain`** dashboard\n",
							"* Navigate back to your dashboard to view this change\n",
							"  * If you wish to change the organization of visualizations, click the three vertical buttons in the top right of the screen; click **Edit** in the menu that appears and you'll be able to drag and resize visualizations"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 11-1 - Managing Permissions for Databases Tables and Views')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "11 - Managing Permissions"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c6cdb12d-decf-482e-8973-2c768f917518"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Managing Permissions for Databases, Tables, and Views\n",
							"\n",
							"The instructions as detailed below are provided for groups of users to explore how Table ACLs on Databricks work. It leverages Databricks SQL and the Data Explorer to accomplish these tasks, and assumes that at least one user in the group has administrator status (or that an admin has previously configured permissions to allow proper permissions for users to create databases, tables, and views). \n",
							"\n",
							"As written, these instructions are for the admin user to complete. The following notebook will have a similar exercise for users to complete in pairs.\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lesson, you should be able to:\n",
							"* Describe the default permissions for users and admins in DBSQL\n",
							"* Identify the default owner for databases, tables, and views created in DBSQL and change ownership\n",
							"* Use Data Explorer to navigate relational entities\n",
							"* Configure permissions for tables and views with Data Explorer\n",
							"* Configure minimal permissions to allow for table discovery and querying"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../Includes/Classroom-Setup-11.1"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Generate Setup Statements\n",
							"\n",
							"The following cell uses Python to extract username of the current user and format this into several statements used to create databases, tables, and views.\n",
							"\n",
							"Only the admin needs to execute the following cell. Successful execution will print out a series of formatted SQL queries, which can be copied into the DBSQL query editor and executed."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.generate_users_table()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Steps:\n",
							"1. Run the cell above\n",
							"1. Copy the entire output to your clipboard\n",
							"1. Navigate to the Databricks SQL workspace\n",
							"1. Make sure that a SQL Warehouse is running\n",
							"1. Use the left sidebar to select the **SQL Editor**\n",
							"1. Paste the query above and click the blue **Run** in the top right\n",
							"\n",
							"**NOTE**: You will need to be connected to a SQL warehouse to execute these queries successfully. If you cannot connect to a SQL warehouse, you will need to contact your administrator to give you access."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Using Data Explorer\n",
							"\n",
							"* Use the left sidebar navigator to select the **Data** tab; this places you in the **Data Explorer**\n",
							"\n",
							"## What is the Data Explorer?\n",
							"\n",
							"The data explorer allows users and admins to:\n",
							"* Navigate databases, tables, and views\n",
							"* Explore data schema, metadata, and history\n",
							"* Set and modify permissions of relational entities\n",
							"\n",
							"Note that at the moment these instructions are being written, Unity Catalog is not yet generally available. The 3 tier namespacing functionality it adds can be previewed to an extent by switching between the default **`hive_metastore`** and the **`sample`** catalog used for example dashboards and queries. Expect the Data Explorer UI and functionality to evolve as Unity Catalog is added to workspaces.\n",
							"\n",
							"## Configuring Permissions\n",
							"\n",
							"By default, admins will have the ability to view all objects registered to the metastore and will be able to control permissions for other users in the workspace. Users will default to having **no** permissions on anything registered to the metastore, other than objects that they create in DBSQL; note that before users can create any databases, tables, or views, they must have create and usage privileges specifically granted to them.\n",
							"\n",
							"Generally, permissions will be set using Groups that have been configured by an administrator, often by importing organizational structures from SCIM integration with a different identity provider. This lesson will explore Access Control Lists (ACLs) used to control permissions, but will use individuals rather than groups.\n",
							"\n",
							"## Table ACLs\n",
							"\n",
							"Databricks allows you to configure permissions for the following objects:\n",
							"\n",
							"| Object | Scope |\n",
							"| --- | --- |\n",
							"| CATALOG | controls access to the entire data catalog. |\n",
							"| DATABASE | controls access to a database. |\n",
							"| TABLE | controls access to a managed or external table. |\n",
							"| VIEW | controls access to SQL views. |\n",
							"| FUNCTION | controls access to a named function. |\n",
							"| ANY FILE | controls access to the underlying filesystem. Users granted access to ANY FILE can bypass the restrictions put on the catalog, databases, tables, and views by reading from the file system directly. |\n",
							"\n",
							"**NOTE**: At present, the **`ANY FILE`** object cannot be set from Data Explorer.\n",
							"\n",
							"## Granting Privileges\n",
							"\n",
							"Databricks admins and object owners can grant privileges according to the following rules:\n",
							"\n",
							"| Role | Can grant access privileges for |\n",
							"| --- | --- |\n",
							"| Databricks administrator | All objects in the catalog and the underlying filesystem. |\n",
							"| Catalog owner | All objects in the catalog. |\n",
							"| Database owner | All objects in the database. |\n",
							"| Table owner | Only the table (similar options for views and functions). |\n",
							"\n",
							"**NOTE**: At present, Data Explorer can only be used to modify ownership of databases, tables, and views. Catalog permissions can be set interactively with the SQL Query Editor.\n",
							"\n",
							"## Privileges\n",
							"\n",
							"The following privileges can be configured in Data Explorer:\n",
							"\n",
							"| Privilege | Ability |\n",
							"| --- | --- |\n",
							"| ALL PRIVILEGES | gives all privileges (is translated into all the below privileges). |\n",
							"| SELECT | gives read access to an object. |\n",
							"| MODIFY | gives ability to add, delete, and modify data to or from an object. |\n",
							"| READ_METADATA | gives ability to view an object and its metadata. |\n",
							"| USAGE | does not give any abilities, but is an additional requirement to perform any action on a database object. |\n",
							"| CREATE | gives ability to create an object (for example, a table in a database). |\n",
							"| CREATE_NAMED_FUNCTION | gives ability to create a named UDF in an existing catalog or schema. |"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Review the Default Permissions\n",
							"In the Data Explorer, find the database you created earlier (this should follow the pattern **`dbacademy_<username>_acls_demo`**).\n",
							"\n",
							"Clicking on the database name should display a list of the contained tables and views on the left hand side. On the right, you'll see some details about the database, including the **Owner** and **Location**. \n",
							"\n",
							"Click the **Permissions** tab to review who presently has permissions (depending on your workspace configuration, some permissions may have been inherited from settings on the catalog)."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Assigning Ownership\n",
							"\n",
							"Click the blue pencil next to the **Owner** field. Note that an owner can be set as an individual OR a group. For most implementations, having one or several small groups of trusted power users as owners will limit admin access to important datasets while ensuring that a single user does not create a choke point in productivity.\n",
							"\n",
							"Here, we'll set the owner to **Admins**, which is a default group containing all workspace administrators."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Change Database Permissions\n",
							"\n",
							"Begin by allowing all users to review metadata about the database.\n",
							"\n",
							"Steps:\n",
							"1. Make sure you have the **Permissions** tab selected for the database\n",
							"1. Click the blue **Grant** button\n",
							"1. Select the **USAGE** and **READ_METADATA** options\n",
							"1. Select the **All Users** group from the drop down menu at the top\n",
							"1. Click **OK**\n",
							"\n",
							"Note that users may need to refresh their view to see these permissions updated. Updates should be reflected for users in near real time for both the Data Explorer and the SQL Editor."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Change View Permissions\n",
							"\n",
							"While users can now see information about this database, they won't be able to interact with the table of view declared above.\n",
							"\n",
							"Let's start by giving users the ability to query our view.\n",
							"\n",
							"Steps:\n",
							"1. Select the **`ny_users_vw`**\n",
							"1. Select the **Permissions** tab\n",
							"   * Users should have inherited the permissions granted at the database level; you'll be able to see which permissions users currently have on an asset, as well as where that permission is inherited from\n",
							"1. Click the blue **Grant** button\n",
							"1. Select the **SELECT** and **READ_METADATA** options\n",
							"   * **READ_METADATA** is technically redundant, as users have already inherited this from the database. However, granting it at the view level allows us to ensure users still have this permission even if the database permissions are revoked\n",
							"1. Select the **All Users** group from the drop down menu at the top\n",
							"1. Click **OK**"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Run a Query to Confirm\n",
							"\n",
							"In the **SQL Editor**, all users should use the **Schema Browser** on the lefthand side to navigate to the database being controlled by the admin.\n",
							"\n",
							"Users should start a query by typing **`SELECT * FROM`** and then click the **>>** that appears while hovering over the view name to insert it into their query.\n",
							"\n",
							"This query should return 2 results.\n",
							"\n",
							"**NOTE**: This view is defined against the **`users`** table, which has not had any permissions set yet. Note that users have access only to that portion of the data that passes through the filters defined on the view; this pattern demonstrates how a single underlying table can be used to drive controlled access to data for relevant stakeholders."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Change Table Permissions\n",
							"\n",
							"Perform the same steps as above, but now for the **`users`** table.\n",
							"\n",
							"Steps:\n",
							"1. Select the **`users`** table\n",
							"1. Select the **Permissions** tab\n",
							"1. Click the blue **Grant** button\n",
							"1. Select the **SELECT** and **READ_METADATA** options\n",
							"1. Select the **All Users** group from the drop down menu at the top\n",
							"1. Click **OK**"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Have Users Attempt to **`DROP TABLE`**\n",
							"\n",
							"In the **SQL Editor**, encourage users to explore the data in this table.\n",
							"\n",
							"Encourage users to try to modify the data here; assuming permissions were set correctly, these commands should error out."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Create a Database for Derivative Datasets\n",
							"\n",
							"In most cases users will need a location to save out derivative datasets. At present, users may not have the ability to create new tables in any location (depending on existing ACLs in the workspace and databases created during previous lessons students have completed).\n",
							"\n",
							"The cell below prints out the code to generate a new database and grant permissions to all users.\n",
							"\n",
							"**NOTE**: Here we set permissions using the SQL Editor rather than the Data Explorer. You can review the Query History to note that all of our previous permission changes from Data Explorer were executed as SQL queries and logged here (additionally, most actions in the Data Explorer are logged with the corresponding SQL query used to populate the UI fields)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.generate_create_database_with_grants()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Have Users Create New Tables or Views\n",
							"\n",
							"Give users a moment to test that they can create tables and views in this new database.\n",
							"\n",
							"**NOTE**: because users were also granted **MODIFY** and **SELECT** permissions, all users will immediately be able to query and modify entities created by their peers."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Admin Configuration\n",
							"\n",
							"At present, users do not have any Table ACL permissions granted on the default catalog **`hive_metastore`** by default. The next lab assumes that users will be able to create databases.\n",
							"\n",
							"To enable the ability to create databases and tables in the default catalog using Databricks SQL, have a workspace admin run the following command in the DBSQL query editor:\n",
							"\n",
							"<strong><code>GRANT usage, create ON CATALOG &#x60;hive_metastore&#x60; TO &#x60;users&#x60;</code></strong>\n",
							"\n",
							"To confirm this has run successfully, execute the following query:\n",
							"\n",
							"<strong><code>SHOW GRANT ON CATALOG &#x60;hive_metastore&#x60;</code></strong>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 11-2L - Configuring Privileges for Production Data and Derived Tables Lab')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "11 - Managing Permissions"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c6ff6d13-077c-43d6-9783-8f36afd597fa"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Configuring Privileges for Production Data and Derived Tables\n",
							"\n",
							"The instructions as detailed below are provided for pairs of users to explore how Table ACLs on Databricks work. It leverages Databricks SQL and the Data Explorer to accomplish these tasks, and assumes that neither user has admin privileges for the workspace. An admin will need to have previously granted **`CREATE`** and **`USAGE`** privileges on a catalog for users to be able to create databases in Databricks SQL.\n",
							"\n",
							"##Learning Objectives\n",
							"\n",
							"By the end of this lab, you should be able to:\n",
							"* Use Data Explorer to navigate relational entities\n",
							"* Configure permissions for tables and views with Data Explorer\n",
							"* Configure minimal permissions to allow for table discovery and querying\n",
							"* Change ownership for databases, tables, and views created in DBSQL"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../Includes/Classroom-Setup-11.2L"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Exchange User Names with your Partner\n",
							"If you are not in a workspace where your usernames correspond with your email address, make sure your partner has your username.\n",
							"\n",
							"They will need this when assigning privileges and searching for your database at later steps.\n",
							"\n",
							"The following cell will print your username."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"print(f\"Your username: {DA.username}\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Generate Setup Statements\n",
							"\n",
							"The following cell uses Python to extract the username of the current user and format this into several statements used to create databases, tables, and views.\n",
							"\n",
							"Both students should execute the following cell. \n",
							"\n",
							"Successful execution will print out a series of formatted SQL queries, which can be copied into the DBSQL query editor and executed."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.generate_query()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Steps:\n",
							"1. Run the cell above\n",
							"1. Copy the entire output to your clipboard\n",
							"1. Navigate to the Databricks SQL workspace\n",
							"1. Make sure that a SQL warehouse is running\n",
							"1. Use the left sidebar to select the **SQL Editor**\n",
							"1. Paste the query above and click the blue **Run** in the top right\n",
							"\n",
							"**NOTE**: You will need to be connected to a SQL warehouse to execute these queries successfully. If you cannot connect to a SQL warehouse, you will need to contact your administrator to give you access."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Find Your Database\n",
							"In the Data Explorer, find the database you created earlier (this should follow the pattern **`dbacademy_<username>_dewd_acls_lab`**).\n",
							"\n",
							"Clicking on the database name should display a list of the contained tables and views on the left hand side.\n",
							"\n",
							"On the right, you'll see some details about the database, including the **Owner** and **Location**.\n",
							"\n",
							"Click the **Permissions** tab to review who presently has permissions (depending on your workspace configuration, some permissions may have been inherited from settings on the catalog)."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Change Database Permissions\n",
							"\n",
							"Steps:\n",
							"1. Make sure you have the **Permissions** tab selected for the database\n",
							"1. Click the blue **Grant** button\n",
							"1. Select the **USAGE**, **SELECT**, and **READ_METADATA** options\n",
							"1. Enter the username of your partner in the field at the top.\n",
							"1. Click **OK**\n",
							"\n",
							"Confirm with your partner that you can each see each others' databases and tables."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Run a Query to Confirm\n",
							"\n",
							"By granting **`USAGE`**, **`SELECT`**, and **`READ_METADATA`** on your database, your partner should now be able to freely query the tables and views in this database, but will not be able to create new tables OR modify your data.\n",
							"\n",
							"In the SQL Editor, each user should run a series of queries to confirm this behavior in the database they were just added to.\n",
							"\n",
							"**Make sure you specify your partner's database while running the queries below.**\n",
							"\n",
							"**NOTE**: These first 3 queries should succeed, but the last should fail."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Replace FILL_IN with your partner's username\n",
							"DA.generate_confirmation_query(\"FILL_IN\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Execute a Query to Generate the Union of Your Beans\n",
							"\n",
							"Execute the query below against your own databases.\n",
							"\n",
							"**NOTE**: Because random values were inserted for the **`grams`** and **`delicious`** columns, you should see 2 distinct rows for each **`name`**, **`color`** pair."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.generate_union_query()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Register a Derivative View to Your Database\n",
							"\n",
							"Execute the query below to register the results of the previous query to your database."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.generate_derivative_view()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Query Your Partner's View\n",
							"\n",
							"Once your partner has successfully completed the previous step, run the following query against each of your tables; you should get the same results:"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Replace FILL_IN with your partner's username\n",
							"DA.generate_partner_view(\"FILL_IN\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Add Modify Permissions\n",
							"\n",
							"Now try to drop each other's **`beans`** tables. \n",
							"\n",
							"At the moment, this shouldn't work.\n",
							"\n",
							"Using the Data Explorer, add the **`MODIFY`** permission for your **`beans`** table for your partner.\n",
							"\n",
							"Again, attempt to drop your partner's **`beans`** table. \n",
							"\n",
							"It should again fail. \n",
							"\n",
							"**Only the owner of a table should be able to issue this statement**.<br/>\n",
							"(Note that ownership can be transferred from an individual to a group, if desired).\n",
							"\n",
							"Instead, execute a query to delete records from your partner's table:"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Replace FILL_IN with your partner's username\n",
							"DA.generate_delete_query(\"FILL_IN\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"This query should successfully drop all records from the target table.\n",
							"\n",
							"Try to re-execute queries against any of the views of tables you'd previously queried in this lab.\n",
							"\n",
							"**NOTE**: If steps were completed successfully, none of your previous queries should return results, as the data referenced by your views has been deleted. This demonstrates the risks associated with providing **`MODIFY`** privileges to users on data that will be used in production applications and dashboards.\n",
							"\n",
							"If you have additional time, see if you can use the Delta methods **`DESCRIBE HISTORY`** and **`RESTORE`** to revert the records in your table."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 12-1 - Last Mile ETL with DBSQL')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "12 - Productionalizing Dashboards and Queries in DBSQL"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "43022879-580e-48f1-b829-ea9e33e0e670"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Last Mile ETL with Databricks SQL\n",
							"\n",
							"Before we continue, let's do a recap of some of the things we've learned so far:\n",
							"1. The Databricks workspace contains a suite of tools to simplify the data engineering development lifecycle\n",
							"1. Databricks notebooks allow users to mix SQL with other programming languages to define ETL workloads\n",
							"1. Delta Lake provides ACID compliant transactions and makes incremental data processing easy in the Lakehouse\n",
							"1. Delta Live Tables extends the SQL syntax to support many design patterns in the Lakehouse, and simplifies infrastructure deployment\n",
							"1. Multi-task jobs allows for full task orchestration, adding dependencies while scheduling a mix of notebooks and DLT pipelines\n",
							"1. Databricks SQL allows users to edit and execute SQL queries, build visualizations, and define dashboards\n",
							"1. Data Explorer simplifies managing Table ACLs, making Lakehouse data available to SQL analysts (soon to be expanded greatly by Unity Catalog)\n",
							"\n",
							"In this section, we'll focus on exploring more DBSQL functionality to support production workloads. \n",
							"\n",
							"We'll start by focusing on leveraging Databricks SQL to configure queries that support last mile ETL for analytics. Note that while we'll be using the Databricks SQL UI for this demo, SQL Warehouses <a href=\"https://docs.databricks.com/integrations/partners.html\" target=\"_blank\">integrate with a number of other tools to allow external query execution</a>, as well as having <a href=\"https://docs.databricks.com/sql/api/index.html\" target=\"_blank\">full API support for executing arbitrary queries programmatically</a>.\n",
							"\n",
							"From these query results, we'll generate a series of visualizations, which we'll combine into a dashboard.\n",
							"\n",
							"Finally, we'll walk through scheduling updates for queries and dashboards, and demonstrate setting alerts to help monitor the state of production datasets over time.\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lesson, you should be able to:\n",
							"* Use Databricks SQL as a tool to support production ETL tasks backing analytic workloads\n",
							"* Configure SQL queries and visualizations with the Databricks SQL Editor\n",
							"* Create dashboards in Databricks SQL\n",
							"* Schedule updates for queries and dashboards\n",
							"* Set alerts for SQL queries"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Run Setup Script\n",
							"The following cells runs a notebook that defines a class we'll use to generate SQL queries."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../Includes/Classroom-Setup-12.1"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Create a Demo Database\n",
							"Execute the following cell and copy the results into the Databricks SQL Editor.\n",
							"\n",
							"These queries:\n",
							"* Create a new database\n",
							"* Declare two tables (we'll use these for loading data)\n",
							"* Declare two functions (we'll use these for generating data)\n",
							"\n",
							"Once copied, execute the query using the **Run** button."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.generate_config()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"**NOTE**: The queries above are only designed to be run once after resetting the demo completely to reconfigure the environment. \n",
							"\n",
							"Users will need to have **`CREATE`** and **`USAGE`** permissions on the catalog to execute them."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"<img src=\"https://files.training.databricks.com/images/icon_warn_32.png\"> \n",
							"**WARNING:** Make sure to select your database before proceeding as the **`USE`** statement<br/>doesn't yet change the database against which your queries will execute"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Create a Query to Load Data\n",
							"Steps:\n",
							"1. Execute the cell below to print out a formatted SQL query for loading data in the **`user_ping`** table created in the previous step.\n",
							"1. Save this query with the name **Load Ping Data**.\n",
							"1. Run this query to load a batch of data."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.generate_load()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Executing the query should load some data and return a preview of the data in the table.\n",
							"\n",
							"**NOTE**: Random numbers are being used to define and load data, so each user will have slightly different values present."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Set a Query Refresh Schedule\n",
							"\n",
							"Steps:\n",
							"1. Click the **Schedule** button on the upper right corner of the SQL query editor box\n",
							"1. Use the drop down to change to Refresh every **1 week** at **12:00**\n",
							"1. For **Ends**, click the **On** radio button\n",
							"1. Select tomorrow's date\n",
							"1. Click **OK**\n",
							"\n",
							"**NOTE:** Although we are using a refresh schedule of 1 week for classroom purposes, you'll likely see shorter trigger intervals in production, such as schedules to refresh every 1 minute."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Create a Query to Track Total Records\n",
							"Steps:\n",
							"1. Execute the cell below.\n",
							"1. Save this query with the name **User Counts**.\n",
							"1. Run the query to calculate the current results."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.generate_user_counts()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Create a Bar Graph Visualization\n",
							"\n",
							"Steps:\n",
							"1. Click the **Add Visualization** button, located beneath the Refresh Schedule button in the bottom right-hand corner of the query window\n",
							"1. Click on the name (should default to something like **`Visualization 1`**) and change the name to **Total User Records**\n",
							"1. Set **`user_id`** for the **X Column**\n",
							"1. Set **`total_records`** for the **Y Columns**\n",
							"1. Click **Save**"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Create a New Dashboard\n",
							"\n",
							"Steps:\n",
							"1. Click the button with three vertical dots at the bottom of the screen and select **Add to Dashboard**.\n",
							"1. Click the **Create new dashboard** option\n",
							"1. Name your dashboard <strong>User Ping Summary **`<your_initials_here>`**</strong>\n",
							"1. Click **Save** to create the new dashboard\n",
							"1. Your newly created dashboard should now be selected as the target; click **OK** to add your visualization"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Create a Query to Calculate the Recent Average Ping\n",
							"Steps:\n",
							"1. Execute the cell below to print out the formatted SQL query.\n",
							"1. Save this query with the name **Avg Ping**.\n",
							"1. Run the query to calculate the current results."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.generate_avg_ping()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Add a Line Plot Visualization to your Dashboard\n",
							"\n",
							"Steps:\n",
							"1. Click the **Add Visualization** button\n",
							"1. Click on the name (should default to something like **`Visualization 1`**) and change the name to **Avg User Ping**\n",
							"1. Select **`Line`** for the **Visualization Type**\n",
							"1. Set **`end_time`** for the **X Column**\n",
							"1. Set **`avg_ping`** for the **Y Columns**\n",
							"1. Set **`user_id`** for the **Group by**\n",
							"1. Click **Save**\n",
							"1. Click the button with three vertical dots at the bottom of the screen and select **Add to Dashboard**.\n",
							"1. Select the dashboard you created earlier\n",
							"1. Click **OK** to add your visualization"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Create a Query to Report Summary Statistics\n",
							"Steps:\n",
							"1. Execute the cell below.\n",
							"1. Save this query with the name **Ping Summary**.\n",
							"1. Run the query to calculate the current results."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.generate_summary()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Add the Summary Table to your Dashboard\n",
							"\n",
							"Steps:\n",
							"1. Click the button with three vertical dots at the bottom of the screen and select **Add to Dashboard**.\n",
							"1. Select the dashboard you created earlier\n",
							"1. Click **OK** to add your visualization"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Review and Refresh your Dashboard\n",
							"\n",
							"Steps:\n",
							"1. Use the left side bar to navigate to **Dashboards**\n",
							"1. Find the dashboard you've added your queries to\n",
							"1. Click the blue **Refresh** button to update your dashboard\n",
							"1. Click the **Schedule** button to review dashboard scheduling options\n",
							"  * Note that scheduling a dashboard to update will execute all queries associated with that dashboard\n",
							"  * Do not schedule the dashboard at this time"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Share your Dashboard\n",
							"\n",
							"Steps:\n",
							"1. Click the blue **Share** button\n",
							"1. Select **All Users** from the top field\n",
							"1. Choose **Can Run** from the right field\n",
							"1. Click **Add**\n",
							"1. Change the **Credentials** to **Run as viewer**\n",
							"\n",
							"**NOTE**: At present, no other users should have any permissions to run your dashboard, as they have not been granted permissions to the underlying databases and tables using Table ACLs. If you wish other users to be able to trigger updates to your dashboard, you will either need to grant them permissions to **Run as owner** or add permissions for the tables referenced in your queries."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Set Up an Alert\n",
							"\n",
							"Steps:\n",
							"1. Use the left side bar to navigate to **Alerts**\n",
							"1. Click **Create Alert** in the top right\n",
							"1. Select your **User Counts** query\n",
							"1. Click the field at the top left of the screen to give the alert a name **`<your_initials> Count Check`**\n",
							"1. For the **Trigger when** options, configure:\n",
							"  * **Value column**: **`total_records`**\n",
							"  * **Condition**: **`>`**\n",
							"  * **Threshold**: **`15`**\n",
							"1. For **Refresh**, select **Never**\n",
							"1. Click **Create Alert**\n",
							"1. On the next screen, click the blue **Refresh** in the top right to evaluate the alert"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Review Alert Destination Options\n",
							"\n",
							"\n",
							"\n",
							"Steps:\n",
							"1. From the preview of your alert, click the blue **Add** button to the right of **Destinations** on the right side of the screen\n",
							"1. At the bottom of the window that pops up, locate the and click the blue text in the message **Create new destinations in Alert Destinations**\n",
							"1. Review the available alerting options"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.cleanup()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 12-2-1L - Instructions and Configuration')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "12 - Productionalizing Dashboards and Queries in DBSQL/DE 12.2L - OPTIONAL Capstone"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "bb591a1e-de2b-4a41-af4f-1f37a0402bd0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## End-to-End ETL in the Lakehouse\n",
							"\n",
							"In this notebook, you will pull together concepts learned throughout the course to complete an example data pipeline.\n",
							"\n",
							"The following is a non-exhaustive list of skills and tasks necessary to successfully complete this exercise:\n",
							"* Using Databricks notebooks to write queries in SQL and Python\n",
							"* Creating and modifying databases, tables, and views\n",
							"* Using Auto Loader and Spark Structured Streaming for incremental data processing in a multi-hop architecture\n",
							"* Using Delta Live Table SQL syntax\n",
							"* Configuring a Delta Live Table pipeline for continuous processing\n",
							"* Using Databricks Jobs to orchestrate tasks from notebooks stored in Repos\n",
							"* Setting chronological scheduling for Databricks Jobs\n",
							"* Defining queries in Databricks SQL\n",
							"* Creating visualizations in Databricks SQL\n",
							"* Defining Databricks SQL dashboards to review metrics and results"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Run Setup\n",
							"Run the following cell to reset all the databases and directories associated with this lab."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../../Includes/Classroom-Setup-12.2.1L"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Land Initial Data\n",
							"Seed the landing zone with some data before proceeding."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.data_factory.load()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Create and Configure a DLT Pipeline\n",
							"**NOTE**: The main difference between the instructions here and in previous labs with DLT is that in this instance, we will be setting up our pipeline for **Continuous** execution in **Production** mode."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.print_pipeline_config()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Steps:\n",
							"1. Click the **Workflows** button on the sidebar.\n",
							"1. Select the **Delta Live Tables** tab.\n",
							"1. Click **Create Pipeline**.\n",
							"1. Fill in a **Pipeline Name** - because these names must be unique, we suggest using the **Pipeline Name** provided in the cell above.\n",
							"1. For **Notebook Libraries**, use the navigator to locate and select the companion notebook provided in the cell above.\n",
							"1. Under **Configuration**, add the three configuration parameters:\n",
							"   * Click **Add configuration**, set the \"key\" to **spark.master** and the \"value\" to **local[\\*]**.\n",
							"   * Click **Add configuration**, set the \"key\" to **datasets_path** and the \"value\" to the value provided in the cell above.\n",
							"   * Click **Add configuration**, set the \"key\" to **source** and the \"value\" to the value provided in the cell above.\n",
							"1. In the **Target** field, enter the database name provided in the cell above.<br/>\n",
							"This should follow the pattern **`da_<name>_<hash>_dewd_cap_12`**\n",
							"1. In the **Storage location** field, copy the directory as printed above.\n",
							"1. For **Pipeline Mode**, select **Continuous**\n",
							"1. Uncheck the **Enable autoscaling** box\n",
							"1. Set the number of **`workers`** to **`0`** (zero).\n",
							"1. Enable **Photon Acceleration**.\n",
							"1. Click **Create**.\n",
							"1. After the UI updates, change from **Development** to **Production** mode\n",
							"\n",
							"This should begin the deployment of infrastructure."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.validate_pipeline_config()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Schedule a Notebook Job\n",
							"\n",
							"Our DLT pipeline is setup to process data as soon as it arrives. \n",
							"\n",
							"We'll schedule a notebook to land a new batch of data each minute so we can see this functionality in action.\n",
							"\n",
							"Before we start run the following cell to get the values used in this step."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.print_job_config()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Steps:\n",
							"1. Click the **Workflows** button on the sidebar\n",
							"1. Select the **Jobs** tab.\n",
							"1. Click the blue **Create Job** button\n",
							"1. Configure the task:\n",
							"    1. Enter **Land-Data** for the task name\n",
							"    1. For **Type**, select **Notebook**\n",
							"    1. For **Path**, select the **Notebook Path** value provided in the cell above\n",
							"    1. From the **Cluster** dropdown, under **Existing All Purpose Clusters**, select your cluster\n",
							"    1. Click **Create**\n",
							"1. In the top-left of the screen rename the job (not the task) from **`Land-Data`** (the defaulted value) to the **Job Name** provided for you in the previous cell.    \n",
							"\n",
							"<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"> **Note**: When selecting your all purpose cluster, you will get a warning about how this will be billed as all purpose compute. Production jobs should always be scheduled against new job clusters appropriately sized for the workload, as this is billed at a much lower rate."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Set a Chronological Schedule for your Job\n",
							"Steps:\n",
							"1. Locate the **Schedule** section in the side panel on the right.\n",
							"1. Click on the **Edit schedule** button to explore scheduling options.\n",
							"1. Change the **Schedule type** field from **Manual (Paused)** to **Scheduled**, which will bring up a chron scheduling UI.\n",
							"1. Set the schedule to update **Every 2**, **Minutes** from **0 minutes past the hour** \n",
							"1. Click **Save**\n",
							"\n",
							"**NOTE**: If you wish, you can click **Run now** to trigger the first run, or wait until the top of the next minute to make sure your scheduling has worked successfully."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.validate_job_config()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Register DLT Event Metrics for Querying with DBSQL\n",
							"\n",
							"The following cell prints out SQL statements to register the DLT event logs to your target database for querying in DBSQL.\n",
							"\n",
							"Execute the output code with the DBSQL Query Editor to register these tables and views. \n",
							"\n",
							"Explore each and make note of the logged event metrics."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.generate_register_dlt_event_metrics_sql()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Define a Query on the Gold Table\n",
							"\n",
							"The **daily_patient_avg** table is automatically updated each time a new batch of data is processed through the DLT pipeline. Each time a query is executed against this table, DBSQL will confirm if there is a newer version and then materialize results from the newest available version.\n",
							"\n",
							"Run the following cell to print out a query with your database name. Save this as a DBSQL query."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.generate_daily_patient_avg()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Add a Line Plot Visualization\n",
							"\n",
							"To track trends in patient averages over time, create a line plot and add it to a new dashboard.\n",
							"\n",
							"Create a line plot with the following settings:\n",
							"* **X Column**: **`date`**\n",
							"* **Y Column**: **`avg_heartrate`**\n",
							"* **Group By**: **`name`**\n",
							"\n",
							"Add this visualization to a dashboard."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Track Data Processing Progress\n",
							"\n",
							"The code below extracts the **`flow_name`**, **`timestamp`**, and **`num_output_rows`** from the DLT event logs.\n",
							"\n",
							"Save this query in DBSQL, then define a bar plot visualization that shows:\n",
							"* **X Column**: **`timestamp`**\n",
							"* **Y Column**: **`num_output_rows`**\n",
							"* **Group By**: **`flow_name`**\n",
							"\n",
							"Add your visualization to your dashboard."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.generate_visualization_query()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Refresh your Dashboard and Track Results\n",
							"\n",
							"The **Land-Data** notebook scheduled with Jobs above has 12 batches of data, each representing a month of recordings for our small sampling of patients. As configured per our instructions, it should take just over 20 minutes for all of these batches of data to be triggered and processed (we scheduled the Databricks Job to run every 2 minutes, and batches of data will process through our pipeline very quickly after initial ingestion).\n",
							"\n",
							"Refresh your dashboard and review your visualizations to see how many batches of data have been processed. (If you followed the instructions as outlined here, there should be 12 distinct flow updates tracked by your DLT metrics.) If all source data has not yet been processed, you can go back to the Databricks Jobs UI and manually trigger additional batches."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"With everything configured, you can now continue to the final part of your lab in the notebook [DE 12.2.4L - Final Steps]($./DE 12.2.4L - Final Steps)"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 12-2-2L - DLT Task')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "12 - Productionalizing Dashboards and Queries in DBSQL/DE 12.2L - OPTIONAL Capstone"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "2dfec068-5b7c-48c8-b0f3-dfd88c807820"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REFRESH STREAMING LIVE TABLE recordings_bronze\n",
							"AS SELECT current_timestamp() AS receipt_time, input_file_name() AS source_file, *\n",
							"  FROM cloud_files(\"${source}\", \"json\", map(\"cloudFiles.schemaHints\", \"time DOUBLE\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REFRESH STREAMING LIVE TABLE pii\n",
							"AS SELECT *\n",
							"  FROM cloud_files(\"${datasets_path}/healthcare/patient\", \"csv\", map(\"header\", \"true\", \"cloudFiles.inferColumnTypes\", \"true\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REFRESH STREAMING LIVE TABLE recordings_enriched\n",
							"  (CONSTRAINT positive_heartrate EXPECT (heartrate > 0) ON VIOLATION DROP ROW)\n",
							"AS SELECT \n",
							"  CAST(a.device_id AS INTEGER) AS device_id, \n",
							"  CAST(a.mrn AS LONG) AS mrn, \n",
							"  CAST(a.heartrate AS DOUBLE) AS heartrate, \n",
							"  CAST(from_unixtime(a.time, 'yyyy-MM-dd HH:mm:ss') AS TIMESTAMP) AS time,\n",
							"  b.name\n",
							"  FROM STREAM(live.recordings_bronze) a\n",
							"  INNER JOIN STREAM(live.pii) b\n",
							"  ON a.mrn = b.mrn"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REFRESH STREAMING LIVE TABLE daily_patient_avg\n",
							"  COMMENT \"Daily mean heartrates by patient\"\n",
							"AS SELECT mrn, name, MEAN(heartrate) avg_heartrate, DATE(time) `date`\n",
							"  FROM STREAM(live.recordings_enriched)\n",
							"  GROUP BY mrn, name, DATE(time)\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 12-2-3L - Land New Data')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "12 - Productionalizing Dashboards and Queries in DBSQL/DE 12.2L - OPTIONAL Capstone"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b558f3e3-0638-4f9e-b62c-ef976dba19e1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ../../Includes/Classroom-Setup-12.2.3L"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA.data_factory.load()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 12-2-4L - Final Steps')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "12 - Productionalizing Dashboards and Queries in DBSQL/DE 12.2L - OPTIONAL Capstone"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "2afbe21e-978a-43a5-b439-65dfba6b43c3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"# End-to-End ETL in the Lakehouse\n",
							"## Final Steps\n",
							"\n",
							"We are picking up from the first notebook in this lab, [DE 12.2.1L - Instructions and Configuration]($./DE 12.2.1L - Instructions and Configuration)\n",
							"\n",
							"If everything is setup correctly, you should have:\n",
							"* A DLT Pipeline running in **Continuous** mode\n",
							"* A job that is feeding that pipeline new data every 2 minutes\n",
							"* A series of Databricks SQL Queries analysing the outputs of that pipeline"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../../Includes/Classroom-Setup-12.2.4L"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Execute a Query to Repair Broken Data\n",
							"\n",
							"Review the code that defined the **`recordings_enriched`** table to identify the filter applied for the quality check.\n",
							"\n",
							"In the cell below, write a query that returns all the records from the **`recordings_bronze`** table that were refused by this quality check."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"-- TODO\n",
							"<FILL-IN>"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"For the purposes of our demo, let's assume that thorough manual review of our data and systems has demonstrated that occasionally otherwise valid heartrate recordings are returned as negative values.\n",
							"\n",
							"Run the following query to examine these same rows with the negative sign removed."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"SELECT abs(heartrate), * FROM ${da.db_name}.recordings_bronze WHERE heartrate <= 0"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"To complete our dataset, we wish to insert these fixed records into the silver **`recordings_enriched`** table.\n",
							"\n",
							"Use the cell below to update the query used in the DLT pipeline to execute this repair.\n",
							"\n",
							"**NOTE**: Make sure you update the code to only process those records that were previously rejected due to the quality check."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# TODO\n",
							"# CREATE OR REFRESH STREAMING LIVE TABLE recordings_enriched\n",
							"#   (CONSTRAINT positive_heartrate EXPECT (heartrate > 0) ON VIOLATION DROP ROW)\n",
							"# AS SELECT \n",
							"#   CAST(a.device_id AS INTEGER) device_id, \n",
							"#   CAST(a.mrn AS LONG) mrn, \n",
							"#   CAST(a.heartrate AS DOUBLE) heartrate, \n",
							"#   CAST(from_unixtime(a.time, 'yyyy-MM-dd HH:mm:ss') AS TIMESTAMP) time,\n",
							"#   b.name\n",
							"#   FROM STREAM(live.recordings_bronze) a\n",
							"#   INNER JOIN STREAM(live.pii) b\n",
							"#   ON a.mrn = b.mrn"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Use the cell below to manually or programmatically confirm that this update has been successful.\n",
							"\n",
							"(The total number of records in the **`recordings_bronze`** should now be equal to the total records in **`recordings_enriched`**)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# TODO\n",
							"<FILL-IN>"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Consider Production Data Permissions\n",
							"\n",
							"Note that while our manual repair of the data was successful, as the owner of these datasets, by default we have permissions to modify or delete these data from any location we're executing code.\n",
							"\n",
							"To put this another way: our current permissions would allow us to change or drop our production tables permanently if an errant SQL query is accidentally executed with the current user's permissions (or if other users are granted similar permissions).\n",
							"\n",
							"While for the purposes of this lab, we desired to have full permissions on our data, as we move code from development to production, it is safer to leverage <a href=\"https://docs.databricks.com/administration-guide/users-groups/service-principals.html\" target=\"_blank\">service principals</a> when scheduling Jobs and DLT Pipelines to avoid accidental data modifications."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Shut Down Production Infrastructure\n",
							"\n",
							"Note that Databricks Jobs, DLT Pipelines, and scheduled DBSQL queries and dashboards are all designed to provide sustained execution of production code. In this end-to-end demo, you were instructed to configure a Job and Pipeline for continuous data processing. To prevent these workloads from continuing to execute, you should **Pause** your Databricks Job and **Stop** your DLT pipeline. Deleting these assets will also ensure that production infrastructure is terminated.\n",
							"\n",
							"**NOTE**: All instructions for DBSQL asset scheduling in previous lessons instructed users to set the update schedule to end tomorrow. You may choose to go back and also cancel these updates to prevent Databricks SQL warehouses from staying on until that time."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.cleanup()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 2-1 - Managing Delta Tables')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "02 - Delta Lake"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "LakehouseTest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "161f127d-8f8a-4bc2-b85a-fa8e405febd9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/8aa90780-bc26-461d-9db6-6df705873871/resourceGroups/domo-demo/providers/Microsoft.Synapse/workspaces/domodemo/bigDataPools/LakehouseTest",
						"name": "LakehouseTest",
						"type": "Spark",
						"endpoint": "https://domodemo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/LakehouseTest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"# Managing Delta Tables\n",
							"\n",
							"If you know any flavor of SQL, you already have much of the knowledge you'll need to work effectively in the data lakehouse.\n",
							"\n",
							"In this notebook, we'll explore basic manipulation of data and tables with SQL on Databricks.\n",
							"\n",
							"Note that Delta Lake is the default format for all tables created with Databricks; if you've been running SQL statements on Databricks, you're likely already working with Delta Lake.\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lesson, you should be able to:\n",
							"* Create Delta Lake tables\n",
							"* Query data from Delta Lake tables\n",
							"* Insert, update, and delete records in Delta Lake tables\n",
							"* Write upsert statements with Delta Lake\n",
							"* Drop Delta Lake tables"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Run Setup\n",
							"The first thing we're going to do is run a setup script. It will define a username, userhome, and database that is scoped to each user."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../Includes/Classroom-Setup-02.1"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Creating a Delta Table\n",
							"\n",
							"There's not much code you need to write to create a table with Delta Lake. There are a number of ways to create Delta Lake tables that we'll see throughout the course. We'll begin with one of the easiest methods: registering an empty Delta Lake table.\n",
							"\n",
							"We need: \n",
							"- A **`CREATE TABLE`** statement\n",
							"- A table name (below we use **`students`**)\n",
							"- A schema\n",
							"\n",
							"**NOTE:** In Databricks Runtime 8.0 and above, Delta Lake is the default format and you dont need **`USING DELTA`**."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE TABLE students\n",
							"  (id INT, name STRING, value DOUBLE);"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"If we try to go back and run that cell again...it will error out! This is expected - because the table exists already, we receive an error.\n",
							"\n",
							"We can add in an additional argument, **`IF NOT EXISTS`** which checks if the table exists. This will overcome our error."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE TABLE IF NOT EXISTS students \n",
							"  (id INT, name STRING, value DOUBLE)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Inserting Data\n",
							"Most often, data will be inserted to tables as the result of a query from another source.\n",
							"\n",
							"However, just as in standard SQL, you can also insert values directly, as shown here."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"INSERT INTO students VALUES (1, \"Yve\", 1.0);\n",
							"INSERT INTO students VALUES (2, \"Omar\", 2.5);\n",
							"INSERT INTO students VALUES (3, \"Elia\", 3.3);"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"In the cell above, we completed three separate **`INSERT`** statements. Each of these is processed as a separate transaction with its own ACID guarantees. Most frequently, we'll insert many records in a single transaction."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"INSERT INTO students\n",
							"VALUES \n",
							"  (4, \"Ted\", 4.7),\n",
							"  (5, \"Tiffany\", 5.5),\n",
							"  (6, \"Vini\", 6.3)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Note that Databricks doesn't have a **`COMMIT`** keyword; transactions run as soon as they're executed, and commit as they succeed."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Querying a Delta Table\n",
							"\n",
							"You probably won't be surprised that querying a Delta Lake table is as easy as using a standard **`SELECT`** statement."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT * FROM students"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"What may surprise you is that Delta Lake guarantees that any read against a table will **always** return the most recent version of the table, and that you'll never encounter a state of deadlock due to ongoing operations.\n",
							"\n",
							"To repeat: table reads can never conflict with other operations, and the newest version of your data is immediately available to all clients that can query your lakehouse. Because all transaction information is stored in cloud object storage alongside your data files, concurrent reads on Delta Lake tables is limited only by the hard limits of object storage on cloud vendors. (**NOTE**: It's not infinite, but it's at least thousands of reads per second.)"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Updating Records\n",
							"\n",
							"Updating records provides atomic guarantees as well: we perform a snapshot read of the current version of our table, find all fields that match our **`WHERE`** clause, and then apply the changes as described.\n",
							"\n",
							"Below, we find all students that have a name starting with the letter **T** and add 1 to the number in their **`value`** column."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"UPDATE students \n",
							"SET value = value + 1\n",
							"WHERE name LIKE \"T%\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Query the table again to see these changes applied."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT * FROM students"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Deleting Records\n",
							"\n",
							"Deletes are also atomic, so there's no risk of only partially succeeding when removing data from your data lakehouse.\n",
							"\n",
							"A **`DELETE`** statement can remove one or many records, but will always result in a single transaction."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DELETE FROM students \n",
							"WHERE value > 6"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Using Merge\n",
							"\n",
							"Some SQL systems have the concept of an upsert, which allows updates, inserts, and other data manipulations to be run as a single command.\n",
							"\n",
							"Databricks uses the **`MERGE`** keyword to perform this operation.\n",
							"\n",
							"Consider the following temporary view, which contains 4 records that might be output by a Change Data Capture (CDC) feed."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REPLACE TEMP VIEW updates(id, name, value, type) AS VALUES\n",
							"  (2, \"Omar\", 15.2, \"update\"),\n",
							"  (3, \"\", null, \"delete\"),\n",
							"  (7, \"Blue\", 7.7, \"insert\"),\n",
							"  (11, \"Diya\", 8.8, \"update\");\n",
							"  \n",
							"SELECT * FROM updates;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Using the syntax we've seen so far, we could filter from this view by type to write 3 statements, one each to insert, update, and delete records. But this would result in 3 separate transactions; if any of these transactions were to fail, it might leave our data in an invalid state.\n",
							"\n",
							"Instead, we combine these actions into a single atomic transaction, applying all 3 types of changes together.\n",
							"\n",
							"**`MERGE`** statements must have at least one field to match on, and each **`WHEN MATCHED`** or **`WHEN NOT MATCHED`** clause can have any number of additional conditional statements.\n",
							"\n",
							"Here, we match on our **`id`** field and then filter on the **`type`** field to appropriately update, delete, or insert our records."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"MERGE INTO students b\n",
							"USING updates u\n",
							"ON b.id=u.id\n",
							"WHEN MATCHED AND u.type = \"update\"\n",
							"  THEN UPDATE SET *\n",
							"WHEN MATCHED AND u.type = \"delete\"\n",
							"  THEN DELETE\n",
							"WHEN NOT MATCHED AND u.type = \"insert\"\n",
							"  THEN INSERT *"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Note that only 3 records were impacted by our **`MERGE`** statement; one of the records in our updates table did not have a matching **`id`** in the students table but was marked as an **`update`**. Based on our custom logic, we ignored this record rather than inserting it. \n",
							"\n",
							"How would you modify the above statement to include unmatched records marked **`update`** in the final **`INSERT`** clause?"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Dropping a Table\n",
							"\n",
							"Assuming that you have proper permissions on the target table, you can permanently delete data in the lakehouse using a **`DROP TABLE`** command.\n",
							"\n",
							"**NOTE**: Later in the course, we'll discuss Table Access Control Lists (ACLs) and default permissions. In a properly configured lakehouse, users should **not** be able to delete production tables."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DROP TABLE students"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Run the following cell to delete the tables and files associated with this lesson."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"DA.cleanup()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 2-2L - Manipulating Tables with Delta Lake Lab')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "02 - Delta Lake"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "LakehouseTest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "460be32e-e01c-49d1-8603-3303d1374525"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/8aa90780-bc26-461d-9db6-6df705873871/resourceGroups/domo-demo/providers/Microsoft.Synapse/workspaces/domodemo/bigDataPools/LakehouseTest",
						"name": "LakehouseTest",
						"type": "Spark",
						"endpoint": "https://domodemo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/LakehouseTest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"# Manipulating Tables with Delta Lake\n",
							"\n",
							"This notebook provides a hands-on review of some of the basic functionality of Delta Lake.\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lab, you should be able to:\n",
							"- Execute standard operations to create and manipulate Delta Lake tables, including:\n",
							"  - **`CREATE TABLE`**\n",
							"  - **`INSERT INTO`**\n",
							"  - **`SELECT FROM`**\n",
							"  - **`UPDATE`**\n",
							"  - **`DELETE`**\n",
							"  - **`MERGE`**\n",
							"  - **`DROP TABLE`**"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Setup\n",
							"Run the following script to setup necessary variables and clear out past runs of this notebook. Note that re-executing this cell will allow you to start the lab over."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../Includes/Classroom-Setup-02.2L"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Create a Table\n",
							"\n",
							"In this notebook, we'll be creating a table to track our bean collection.\n",
							"\n",
							"Use the cell below to create a managed Delta Lake table named **`beans`**.\n",
							"\n",
							"Provide the following schema:\n",
							"\n",
							"| Field Name | Field type |\n",
							"| --- | --- |\n",
							"| name | STRING |\n",
							"| color | STRING |\n",
							"| grams | FLOAT |\n",
							"| delicious | BOOLEAN |"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"<FILL-IN>"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"**NOTE**: We'll use Python to run checks occasionally throughout the lab. The following cell will return as error with a message on what needs to change if you have not followed instructions. No output from cell execution means that you have completed this step."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"assert spark.table(\"beans\"), \"Table named `beans` does not exist\"\n",
							"assert spark.table(\"beans\").columns == [\"name\", \"color\", \"grams\", \"delicious\"], \"Please name the columns in the order provided above\"\n",
							"assert spark.table(\"beans\").dtypes == [(\"name\", \"string\"), (\"color\", \"string\"), (\"grams\", \"float\"), (\"delicious\", \"boolean\")], \"Please make sure the column types are identical to those provided above\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Insert Data\n",
							"\n",
							"Run the following cell to insert three rows into the table."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"INSERT INTO beans VALUES\n",
							"(\"black\", \"black\", 500, true),\n",
							"(\"lentils\", \"brown\", 1000, true),\n",
							"(\"jelly\", \"rainbow\", 42.5, false)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Manually review the table contents to ensure data was written as expected."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"<FILL-IN>"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Insert the additional records provided below. Make sure you execute this as a single transaction."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"<FILL-IN>\n",
							"('pinto', 'brown', 1.5, true),\n",
							"('green', 'green', 178.3, true),\n",
							"('beanbag chair', 'white', 40000, false)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Run the cell below to confirm the data is in the proper state."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"assert spark.table(\"beans\").count() == 6, \"The table should have 6 records\"\n",
							"assert spark.conf.get(\"spark.databricks.delta.lastCommitVersionInSession\") == \"2\", \"Only 3 commits should have been made to the table\"\n",
							"assert set(row[\"name\"] for row in spark.table(\"beans\").select(\"name\").collect()) == {'beanbag chair', 'black', 'green', 'jelly', 'lentils', 'pinto'}, \"Make sure you have not modified the data provided\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Update Records\n",
							"\n",
							"A friend is reviewing your inventory of beans. After much debate, you agree that jelly beans are delicious.\n",
							"\n",
							"Run the following cell to update this record."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"UPDATE beans\n",
							"SET delicious = true\n",
							"WHERE name = \"jelly\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"You realize that you've accidentally entered the weight of your pinto beans incorrectly.\n",
							"\n",
							"Update the **`grams`** column for this record to the correct weight of 1500."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"<FILL-IN>"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Run the cell below to confirm this has completed properly."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"assert spark.table(\"beans\").filter(\"name='pinto'\").count() == 1, \"There should only be 1 entry for pinto beans\"\n",
							"row = spark.table(\"beans\").filter(\"name='pinto'\").first()\n",
							"assert row[\"color\"] == \"brown\", \"The pinto bean should be labeled as the color brown\"\n",
							"assert row[\"grams\"] == 1500, \"Make sure you correctly specified the `grams` as 1500\"\n",
							"assert row[\"delicious\"] == True, \"The pinto bean is a delicious bean\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Delete Records\n",
							"\n",
							"You've decided that you only want to keep track of delicious beans.\n",
							"\n",
							"Execute a query to drop all beans that are not delicious."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"<FILL-IN>"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Run the following cell to confirm this operation was successful."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"assert spark.table(\"beans\").filter(\"delicious=true\").count() == 5, \"There should be 5 delicious beans in your table\"\n",
							"assert spark.table(\"beans\").filter(\"name='beanbag chair'\").count() == 0, \"Make sure your logic deletes non-delicious beans\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Using Merge to Upsert Records\n",
							"\n",
							"Your friend gives you some new beans. The cell below registers these as a temporary view."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REPLACE TEMP VIEW new_beans(name, color, grams, delicious) AS VALUES\n",
							"('black', 'black', 60.5, true),\n",
							"('lentils', 'green', 500, true),\n",
							"('kidney', 'red', 387.2, true),\n",
							"('castor', 'brown', 25, false);\n",
							"\n",
							"SELECT * FROM new_beans"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"In the cell below, use the above view to write a merge statement to update and insert new records to your **`beans`** table as one transaction.\n",
							"\n",
							"Make sure your logic:\n",
							"- Matches beans by name **and** color\n",
							"- Updates existing beans by adding the new weight to the existing weight\n",
							"- Inserts new beans only if they are delicious"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"<FILL-IN>"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Run the cell below to check your work."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"version = spark.sql(\"DESCRIBE HISTORY beans\").selectExpr(\"max(version)\").first()[0]\n",
							"last_tx = spark.sql(\"DESCRIBE HISTORY beans\").filter(f\"version={version}\")\n",
							"assert last_tx.select(\"operation\").first()[0] == \"MERGE\", \"Transaction should be completed as a merge\"\n",
							"metrics = last_tx.select(\"operationMetrics\").first()[0]\n",
							"assert metrics[\"numOutputRows\"] == \"3\", \"Make sure you only insert delicious beans\"\n",
							"assert metrics[\"numTargetRowsUpdated\"] == \"1\", \"Make sure you match on name and color\"\n",
							"assert metrics[\"numTargetRowsInserted\"] == \"2\", \"Make sure you insert newly collected beans\"\n",
							"assert metrics[\"numTargetRowsDeleted\"] == \"0\", \"No rows should be deleted by this operation\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Dropping Tables\n",
							"\n",
							"When working with managed Delta Lake tables, dropping a table results in permanently deleting access to the table and all underlying data files.\n",
							"\n",
							"**NOTE**: Later in the course, we'll learn about external tables, which approach Delta Lake tables as a collection of files and have different persistence guarantees.\n",
							"\n",
							"In the cell below, write a query to drop the **`beans`** table."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"<FILL-IN>"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Run the cell below to assert that your table no longer exists."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"assert spark.sql(\"SHOW TABLES LIKE 'beans'\").collect() == [], \"Confirm that you have dropped the `beans` table from your current database\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Wrapping Up\n",
							"\n",
							"By completing this lab, you should now feel comfortable:\n",
							"* Completing standard Delta Lake table creation and data manipulation commands"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Run the following cell to delete the tables and files associated with this lesson."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"DA.cleanup()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 2-3 - Advanced Delta Lake Features')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "02 - Delta Lake"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "LakehouseTest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "e92a0b60-598c-43ab-bb59-017e09688c98"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/8aa90780-bc26-461d-9db6-6df705873871/resourceGroups/domo-demo/providers/Microsoft.Synapse/workspaces/domodemo/bigDataPools/LakehouseTest",
						"name": "LakehouseTest",
						"type": "Spark",
						"endpoint": "https://domodemo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/LakehouseTest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Advanced Delta Lake Features\n",
							"\n",
							"Now that you feel comfortable performing basic data tasks with Delta Lake, we can discuss a few features unique to Delta Lake.\n",
							"\n",
							"Note that while some of the keywords used here aren't part of standard ANSI SQL, all Delta Lake operations can be run on Databricks using SQL\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lesson, you should be able to:\n",
							"* Use **`OPTIMIZE`** to compact small files\n",
							"* Use **`ZORDER`** to index tables\n",
							"* Describe the directory structure of Delta Lake files\n",
							"* Review a history of table transactions\n",
							"* Query and roll back to previous table version\n",
							"* Clean up stale data files with **`VACUUM`**\n",
							"\n",
							"**Resources**\n",
							"* <a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/delta-optimize.html\" target=\"_blank\">Delta Optimize - Databricks Docs</a>\n",
							"* <a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/delta-vacuum.html\" target=\"_blank\">Delta Vacuum - Databricks Docs</a>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Run Setup\n",
							"The first thing we're going to do is run a setup script. It will define a username, userhome, and database that is scoped to each user."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../Includes/Classroom-Setup-02.3"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Creating a Delta Table with History\n",
							"\n",
							"The cell below condenses all the transactions from the previous lesson into a single cell. (Except for the **`DROP TABLE`**!)\n",
							"\n",
							"As you're waiting for this query to run, see if you can identify the total number of transactions being executed."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE TABLE students\n",
							"  (id INT, name STRING, value DOUBLE);\n",
							"  \n",
							"INSERT INTO students VALUES (1, \"Yve\", 1.0);\n",
							"INSERT INTO students VALUES (2, \"Omar\", 2.5);\n",
							"INSERT INTO students VALUES (3, \"Elia\", 3.3);\n",
							"\n",
							"INSERT INTO students\n",
							"VALUES \n",
							"  (4, \"Ted\", 4.7),\n",
							"  (5, \"Tiffany\", 5.5),\n",
							"  (6, \"Vini\", 6.3);\n",
							"  \n",
							"UPDATE students \n",
							"SET value = value + 1\n",
							"WHERE name LIKE \"T%\";\n",
							"\n",
							"DELETE FROM students \n",
							"WHERE value > 6;\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW updates(id, name, value, type) AS VALUES\n",
							"  (2, \"Omar\", 15.2, \"update\"),\n",
							"  (3, \"\", null, \"delete\"),\n",
							"  (7, \"Blue\", 7.7, \"insert\"),\n",
							"  (11, \"Diya\", 8.8, \"update\");\n",
							"  \n",
							"MERGE INTO students b\n",
							"USING updates u\n",
							"ON b.id=u.id\n",
							"WHEN MATCHED AND u.type = \"update\"\n",
							"  THEN UPDATE SET *\n",
							"WHEN MATCHED AND u.type = \"delete\"\n",
							"  THEN DELETE\n",
							"WHEN NOT MATCHED AND u.type = \"insert\"\n",
							"  THEN INSERT *;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Examine Table Details\n",
							"\n",
							"Databricks uses a Hive metastore by default to register databases, tables, and views.\n",
							"\n",
							"Using **`DESCRIBE EXTENDED`** allows us to see important metadata about our table."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DESCRIBE EXTENDED students"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"**`DESCRIBE DETAIL`** is another command that allows us to explore table metadata."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DESCRIBE DETAIL students"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Note the **`Location`** field.\n",
							"\n",
							"While we've so far been thinking about our table as just a relational entity within a database, a Delta Lake table is actually backed by a collection of files stored in cloud object storage."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Explore Delta Lake Files\n",
							"\n",
							"We can see the files backing our Delta Lake table by using a Databricks Utilities function.\n",
							"\n",
							"**NOTE**: It's not important right now to know everything about these files to work with Delta Lake, but it will help you gain a greater appreciation for how the technology is implemented."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"display(dbutils.fs.ls(f\"{DA.paths.user_db}/students\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Note that our directory contains a number of Parquet data files and a directory named **`_delta_log`**.\n",
							"\n",
							"Records in Delta Lake tables are stored as data in Parquet files.\n",
							"\n",
							"Transactions to Delta Lake tables are recorded in the **`_delta_log`**.\n",
							"\n",
							"We can peek inside the **`_delta_log`** to see more."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"display(dbutils.fs.ls(f\"{DA.paths.user_db}/students/_delta_log\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Each transaction results in a new JSON file being written to the Delta Lake transaction log. Here, we can see that there are 8 total transactions against this table (Delta Lake is 0 indexed)."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Reasoning about Data Files\n",
							"\n",
							"We just saw a lot of data files for what is obviously a very small table.\n",
							"\n",
							"**`DESCRIBE DETAIL`** allows us to see some other details about our Delta table, including the number of files."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DESCRIBE DETAIL students"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Here we see that our table currently contains 4 data files in its present version. So what are all those other Parquet files doing in our table directory? \n",
							"\n",
							"Rather than overwriting or immediately deleting files containing changed data, Delta Lake uses the transaction log to indicate whether or not files are valid in a current version of the table.\n",
							"\n",
							"Here, we'll look at the transaction log corresponding the **`MERGE`** statement above, where records were inserted, updated, and deleted."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"display(spark.sql(f\"SELECT * FROM json.`{DA.paths.user_db}/students/_delta_log/00000000000000000007.json`\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"The **`add`** column contains a list of all the new files written to our table; the **`remove`** column indicates those files that no longer should be included in our table.\n",
							"\n",
							"When we query a Delta Lake table, the query engine uses the transaction logs to resolve all the files that are valid in the current version, and ignores all other data files."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Compacting Small Files and Indexing\n",
							"\n",
							"Small files can occur for a variety of reasons; in our case, we performed a number of operations where only one or several records were inserted.\n",
							"\n",
							"Files will be combined toward an optimal size (scaled based on the size of the table) by using the **`OPTIMIZE`** command.\n",
							"\n",
							"**`OPTIMIZE`** will replace existing data files by combining records and rewriting the results.\n",
							"\n",
							"When executing **`OPTIMIZE`**, users can optionally specify one or several fields for **`ZORDER`** indexing. While the specific math of Z-order is unimportant, it speeds up data retrieval when filtering on provided fields by colocating data with similar values within data files."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"OPTIMIZE students\n",
							"ZORDER BY id"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Given how small our data is, **`ZORDER`** does not provide any benefit, but we can see all of the metrics that result from this operation."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Reviewing Delta Lake Transactions\n",
							"\n",
							"Because all changes to the Delta Lake table are stored in the transaction log, we can easily review the <a href=\"https://docs.databricks.com/spark/2.x/spark-sql/language-manual/describe-history.html\" target=\"_blank\">table history</a>."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DESCRIBE HISTORY students"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"As expected, **`OPTIMIZE`** created another version of our table, meaning that version 8 is our most current version.\n",
							"\n",
							"Remember all of those extra data files that had been marked as removed in our transaction log? These provide us with the ability to query previous versions of our table.\n",
							"\n",
							"These time travel queries can be performed by specifying either the integer version or a timestamp.\n",
							"\n",
							"**NOTE**: In most cases, you'll use a timestamp to recreate data at a time of interest. For our demo we'll use version, as this is deterministic (whereas you may be running this demo at any time in the future)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT * \n",
							"FROM students VERSION AS OF 3"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"What's important to note about time travel is that we're not recreating a previous state of the table by undoing transactions against our current version; rather, we're just querying all those data files that were indicated as valid as of the specified version."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Rollback Versions\n",
							"\n",
							"Suppose you're typing up query to manually delete some records from a table and you accidentally execute this query in the following state."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DELETE FROM students"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Note that when we see a **`-1`** for number of rows affected by a delete, this means an entire directory of data has been removed.\n",
							"\n",
							"Let's confirm this below."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT * FROM students"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Deleting all the records in your table is probably not a desired outcome. Luckily, we can simply rollback this commit."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"RESTORE TABLE students TO VERSION AS OF 8 "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Note that a **`RESTORE`** <a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/delta-restore.html\" target=\"_blank\">command</a> is recorded as a transaction; you won't be able to completely hide the fact that you accidentally deleted all the records in the table, but you will be able to undo the operation and bring your table back to a desired state."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Cleaning Up Stale Files\n",
							"\n",
							"Databricks will automatically clean up stale files in Delta Lake tables.\n",
							"\n",
							"While Delta Lake versioning and time travel are great for querying recent versions and rolling back queries, keeping the data files for all versions of large production tables around indefinitely is very expensive (and can lead to compliance issues if PII is present).\n",
							"\n",
							"If you wish to manually purge old data files, this can be performed with the **`VACUUM`** operation.\n",
							"\n",
							"Uncomment the following cell and execute it with a retention of **`0 HOURS`** to keep only the current version:"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- VACUUM students RETAIN 0 HOURS"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"By default, **`VACUUM`** will prevent you from deleting files less than 7 days old, just to ensure that no long-running operations are still referencing any of the files to be deleted. If you run **`VACUUM`** on a Delta table, you lose the ability time travel back to a version older than the specified data retention period.  In our demos, you may see Databricks executing code that specifies a retention of **`0 HOURS`**. This is simply to demonstrate the feature and is not typically done in production.  \n",
							"\n",
							"In the following cell, we:\n",
							"1. Turn off a check to prevent premature deletion of data files\n",
							"1. Make sure that logging of **`VACUUM`** commands is enabled\n",
							"1. Use the **`DRY RUN`** version of vacuum to print out all records to be deleted"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SET spark.databricks.delta.retentionDurationCheck.enabled = false;\n",
							"SET spark.databricks.delta.vacuum.logging.enabled = true;\n",
							"\n",
							"VACUUM students RETAIN 0 HOURS DRY RUN"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"By running **`VACUUM`** and deleting the 10 files above, we will permanently remove access to versions of the table that require these files to materialize."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"VACUUM students RETAIN 0 HOURS"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Check the table directory to show that files have been successfully deleted."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"display(dbutils.fs.ls(f\"{DA.paths.user_db}/students\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"Run the following cell to delete the tables and files associated with this lesson."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"DA.cleanup()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 2-4L - Delta Lake Versioning  Optimization  and Vacuuming Lab')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "02 - Delta Lake"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "LakehouseTest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "654489ef-5bab-4dc0-92c2-a20460ea5fe3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/8aa90780-bc26-461d-9db6-6df705873871/resourceGroups/domo-demo/providers/Microsoft.Synapse/workspaces/domodemo/bigDataPools/LakehouseTest",
						"name": "LakehouseTest",
						"type": "Spark",
						"endpoint": "https://domodemo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/LakehouseTest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Delta Lake Versioning, Optimization, and Vacuuming\n",
							"\n",
							"This notebook provides a hands-on review of some of the more esoteric features Delta Lake brings to the data lakehouse.\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lab, you should be able to:\n",
							"- Review table history\n",
							"- Query previous table versions and rollback a table to a specific version\n",
							"- Perform file compaction and Z-order indexing\n",
							"- Preview files marked for permanent deletion and commit these deletes"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Setup\n",
							"Run the following script to setup necessary variables and clear out past runs of this notebook. Note that re-executing this cell will allow you to start the lab over."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../Includes/Classroom-Setup-02.4L"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Recreate the History of your Bean Collection\n",
							"\n",
							"This lab picks up where the last lab left off. The cell below condenses all the operations from the last lab into a single cell (other than the final **`DROP TABLE`** statement).\n",
							"\n",
							"For quick reference, the schema of the **`beans`** table created is:\n",
							"\n",
							"| Field Name | Field type |\n",
							"| --- | --- |\n",
							"| name | STRING |\n",
							"| color | STRING |\n",
							"| grams | FLOAT |\n",
							"| delicious | BOOLEAN |"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE TABLE beans \n",
							"(name STRING, color STRING, grams FLOAT, delicious BOOLEAN);\n",
							"\n",
							"INSERT INTO beans VALUES\n",
							"(\"black\", \"black\", 500, true),\n",
							"(\"lentils\", \"brown\", 1000, true),\n",
							"(\"jelly\", \"rainbow\", 42.5, false);\n",
							"\n",
							"INSERT INTO beans VALUES\n",
							"('pinto', 'brown', 1.5, true),\n",
							"('green', 'green', 178.3, true),\n",
							"('beanbag chair', 'white', 40000, false);\n",
							"\n",
							"UPDATE beans\n",
							"SET delicious = true\n",
							"WHERE name = \"jelly\";\n",
							"\n",
							"UPDATE beans\n",
							"SET grams = 1500\n",
							"WHERE name = 'pinto';\n",
							"\n",
							"DELETE FROM beans\n",
							"WHERE delicious = false;\n",
							"\n",
							"CREATE OR REPLACE TEMP VIEW new_beans(name, color, grams, delicious) AS VALUES\n",
							"('black', 'black', 60.5, true),\n",
							"('lentils', 'green', 500, true),\n",
							"('kidney', 'red', 387.2, true),\n",
							"('castor', 'brown', 25, false);\n",
							"\n",
							"MERGE INTO beans a\n",
							"USING new_beans b\n",
							"ON a.name=b.name AND a.color = b.color\n",
							"WHEN MATCHED THEN\n",
							"  UPDATE SET grams = a.grams + b.grams\n",
							"WHEN NOT MATCHED AND b.delicious = true THEN\n",
							"  INSERT *;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Review the Table History\n",
							"\n",
							"Delta Lake's transaction log stores information about each transaction that modifies a table's contents or settings.\n",
							"\n",
							"Review the history of the **`beans`** table below."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"<FILL-IN>"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"If all the previous operations were completed as described you should see 7 versions of the table (**NOTE**: Delta Lake versioning starts with 0, so the max version number will be 6).\n",
							"\n",
							"The operations should be as follows:\n",
							"\n",
							"| version | operation |\n",
							"| --- | --- |\n",
							"| 0 | CREATE TABLE |\n",
							"| 1 | WRITE |\n",
							"| 2 | WRITE |\n",
							"| 3 | UPDATE |\n",
							"| 4 | UPDATE |\n",
							"| 5 | DELETE |\n",
							"| 6 | MERGE |\n",
							"\n",
							"The **`operationsParameters`** column will let you review predicates used for updates, deletes, and merges. The **`operationMetrics`** column indicates how many rows and files are added in each operation.\n",
							"\n",
							"Spend some time reviewing the Delta Lake history to understand which table version matches with a given transaction.\n",
							"\n",
							"**NOTE**: The **`version`** column designates the state of a table once a given transaction completes. The **`readVersion`** column indicates the version of the table an operation executed against. In this simple demo (with no concurrent transactions), this relationship should always increment by 1."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Query a Specific Version\n",
							"\n",
							"After reviewing the table history, you decide you want to view the state of your table after your very first data was inserted.\n",
							"\n",
							"Run the query below to see this."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT * FROM beans VERSION AS OF 1"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"And now review the current state of your data."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT * FROM beans"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"You want to review the weights of your beans before you deleted any records.\n",
							"\n",
							"Fill in the statement below to register a temporary view of the version just before data was deleted, then run the following cell to query the view."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"CREATE OR REPLACE TEMP VIEW pre_delete_vw AS\n",
							"<FILL-IN>"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT * FROM pre_delete_vw"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Run the cell below to check that you have captured the correct version."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"assert spark.table(\"pre_delete_vw\"), \"Make sure you have registered the temporary view with the provided name `pre_delete_vw`\"\n",
							"assert spark.table(\"pre_delete_vw\").count() == 6, \"Make sure you're querying a version of the table with 6 records\"\n",
							"assert spark.table(\"pre_delete_vw\").selectExpr(\"int(sum(grams))\").first()[0] == 43220, \"Make sure you query the version of the table after updates were applied\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Restore a Previous Version\n",
							"\n",
							"Apparently there was a misunderstanding; the beans your friend gave you that you merged into your collection were not intended for you to keep.\n",
							"\n",
							"Revert your table to the version before this **`MERGE`** statement completed."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"<FILL-IN>"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Review the history of your table. Make note of the fact that restoring to a previous version adds another table version."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DESCRIBE HISTORY beans"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"last_tx = spark.conf.get(\"spark.databricks.delta.lastCommitVersionInSession\")\n",
							"assert spark.sql(f\"DESCRIBE HISTORY beans\").select(\"operation\").first()[0] == \"RESTORE\", \"Make sure you reverted your table with the `RESTORE` keyword\"\n",
							"assert spark.table(\"beans\").count() == 5, \"Make sure you reverted to the version after deleting records but before merging\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## File Compaction\n",
							"Looking at the transaction metrics during your reversion, you are surprised you have some many files for such a small collection of data.\n",
							"\n",
							"While indexing on a table of this size is unlikely to improve performance, you decide to add a Z-order index on the **`name`** field in anticipation of your bean collection growing exponentially over time.\n",
							"\n",
							"Use the cell below to perform file compaction and Z-order indexing."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"<FILL-IN>"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Your data should have been compacted to a single file; confirm this manually by running the following cell."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DESCRIBE DETAIL beans"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Run the cell below to check that you've successfully optimized and indexed your table."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"last_tx = spark.sql(\"DESCRIBE HISTORY beans\").first()\n",
							"assert last_tx[\"operation\"] == \"OPTIMIZE\", \"Make sure you used the `OPTIMIZE` command to perform file compaction\"\n",
							"assert last_tx[\"operationParameters\"][\"zOrderBy\"] == '[\"name\"]', \"Use `ZORDER BY name` with your optimize command to index your table\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Cleaning Up Stale Data Files\n",
							"\n",
							"You know that while all your data now resides in 1 data file, the data files from previous versions of your table are still being stored alongside this. You wish to remove these files and remove access to previous versions of the table by running **`VACUUM`** on the table.\n",
							"\n",
							"Executing **`VACUUM`** performs garbage cleanup on the table directory. By default, a retention threshold of 7 days will be enforced.\n",
							"\n",
							"The cell below modifies some Spark configurations. The first command overrides the retention threshold check to allow us to demonstrate permanent removal of data. \n",
							"\n",
							"**NOTE**: Vacuuming a production table with a short retention can lead to data corruption and/or failure of long-running queries. This is for demonstration purposes only and extreme caution should be used when disabling this setting.\n",
							"\n",
							"The second command sets **`spark.databricks.delta.vacuum.logging.enabled`** to **`true`** to ensure that the **`VACUUM`** operation is recorded in the transaction log.\n",
							"\n",
							"**NOTE**: Because of slight differences in storage protocols on various clouds, logging **`VACUUM`** commands is not on by default for some clouds as of DBR 9.1."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SET spark.databricks.delta.retentionDurationCheck.enabled = false;\n",
							"SET spark.databricks.delta.vacuum.logging.enabled = true;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Before permanently deleting data files, review them manually using the **`DRY RUN`** option."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"VACUUM beans RETAIN 0 HOURS DRY RUN"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"All data files not in the current version of the table will be shown in the preview above.\n",
							"\n",
							"Run the command again without **`DRY RUN`** to permanently delete these files.\n",
							"\n",
							"**NOTE**: All previous versions of the table will no longer be accessible."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"VACUUM beans RETAIN 0 HOURS"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Because **`VACUUM`** can be such a destructive act for important datasets, it's always a good idea to turn the retention duration check back on. Run the cell below to reactive this setting."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SET spark.databricks.delta.retentionDurationCheck.enabled = true"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Note that the table history will indicate the user that completed the **`VACUUM`** operation, the number of files deleted, and log that the retention check was disabled during this operation."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DESCRIBE HISTORY beans"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Query your table again to confirm you still have access to the current version."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT * FROM beans"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"<img src=\"https://files.training.databricks.com/images/icon_warn_32.png\"> Because Delta Cache stores copies of files queried in the current session on storage volumes deployed to your currently active cluster, you may still be able to temporarily access previous table versions (though systems should **not** be designed to expect this behavior). \n",
							"\n",
							"Restarting the cluster will ensure that these cached data files are permanently purged.\n",
							"\n",
							"You can see an example of this by uncommenting and running the following cell that may, or may not, fail\n",
							"(depending on the state of the cache)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- SELECT * FROM beans@v1"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"By completing this lab, you should now feel comfortable:\n",
							"* Completing standard Delta Lake table creation and data manipulation commands\n",
							"* Reviewing table metadata including table history\n",
							"* Leverage Delta Lake versioning for snapshot queries and rollbacks\n",
							"* Compacting small files and indexing tables\n",
							"* Using **`VACUUM`** to review files marked for deletion and committing these deletes"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"Run the following cell to delete the tables and files associated with this lesson."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"DA.cleanup()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 3-1 - Databases and Tables on Databricks')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "03 - Relational Entities on Databricks"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "LakehouseTest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9eb5bd13-6d5a-454b-bf84-50484cdb463c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/8aa90780-bc26-461d-9db6-6df705873871/resourceGroups/domo-demo/providers/Microsoft.Synapse/workspaces/domodemo/bigDataPools/LakehouseTest",
						"name": "LakehouseTest",
						"type": "Spark",
						"endpoint": "https://domodemo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/LakehouseTest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Schemas and Tables on Databricks\n",
							"In this demonstration, you will create and explore schemas and tables.\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lesson, you should be able to:\n",
							"* Use Spark SQL DDL to define schemas and tables\n",
							"* Describe how the **`LOCATION`** keyword impacts the default storage directory\n",
							"\n",
							"\n",
							"\n",
							"**Resources**\n",
							"* <a href=\"https://docs.databricks.com/user-guide/tables.html\" target=\"_blank\">Schemas and Tables - Databricks Docs</a>\n",
							"* <a href=\"https://docs.databricks.com/user-guide/tables.html#managed-and-unmanaged-tables\" target=\"_blank\">Managed and Unmanaged Tables</a>\n",
							"* <a href=\"https://docs.databricks.com/user-guide/tables.html#create-a-table-using-the-ui\" target=\"_blank\">Creating a Table with the UI</a>\n",
							"* <a href=\"https://docs.databricks.com/user-guide/tables.html#create-a-local-table\" target=\"_blank\">Create a Local Table</a>\n",
							"* <a href=\"https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#saving-to-persistent-tables\" target=\"_blank\">Saving to Persistent Tables</a>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Lesson Setup\n",
							"The following script clears out previous runs of this demo and configures some Hive variables that will be used in our SQL queries."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../Includes/Classroom-Setup-03.1"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Using Hive Variables\n",
							"\n",
							"While not a pattern that is generally recommended in Spark SQL, this notebook will use some Hive variables to substitute in string values derived from the account email of the current user.\n",
							"\n",
							"The following cell demonstrates this pattern."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT \"${da.db_name}\" AS db_name,\n",
							"       \"${da.paths.working_dir}\" AS working_dir"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Because you may be working in a shared workspace, this course uses variables derived from your username so the schemas don't conflict with other users. Again, consider this use of Hive variables a hack for our lesson environment rather than a good practice for development."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"## Schemas\n",
							"Let's start by creating two schemas:\n",
							"- One with no **`LOCATION`** specified\n",
							"- One with **`LOCATION`** specified"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE SCHEMA IF NOT EXISTS ${da.db_name}_default_location;\n",
							"CREATE SCHEMA IF NOT EXISTS ${da.db_name}_custom_location LOCATION '${da.paths.working_dir}/_custom_location.db';"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"Note that the location of the first schema is in the default location under **`dbfs:/user/hive/warehouse/`** and that the schema directory is the name of the schema with the **`.db`** extension"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DESCRIBE SCHEMA EXTENDED ${da.db_name}_default_location;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Note that the location of the second schema is in the directory specified after the **`LOCATION`** keyword."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DESCRIBE SCHEMA EXTENDED ${da.db_name}_custom_location;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"We will create a table in the schema with default location and insert data. \n",
							"\n",
							"Note that the schema must be provided because there is no data from which to infer the schema."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"USE ${da.db_name}_default_location;\n",
							"\n",
							"CREATE OR REPLACE TABLE managed_table_in_db_with_default_location (width INT, length INT, height INT);\n",
							"INSERT INTO managed_table_in_db_with_default_location \n",
							"VALUES (3, 2, 1);\n",
							"SELECT * FROM managed_table_in_db_with_default_location;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"We can look at the extended table description to find the location (you'll need to scroll down in the results)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DESCRIBE DETAIL managed_table_in_db_with_default_location;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"By default, managed tables in a schema without the location specified will be created in the **`dbfs:/user/hive/warehouse/<schema_name>.db/`** directory.\n",
							"\n",
							"We can see that, as expected, the data and metadata for our Delta Table are stored in that location."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"hive_root =  f\"dbfs:/user/hive/warehouse\"\n",
							"db_name =    f\"{DA.db_name}_default_location.db\"\n",
							"table_name = f\"managed_table_in_db_with_default_location\"\n",
							"\n",
							"tbl_location = f\"{hive_root}/{db_name}/{table_name}\"\n",
							"print(tbl_location)\n",
							"\n",
							"files = dbutils.fs.ls(tbl_location)\n",
							"display(files)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"Drop the table."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DROP TABLE managed_table_in_db_with_default_location;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"Note the table's directory and its log and data files are deleted. Only the schema directory remains."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"\n",
							"db_location = f\"{hive_root}/{db_name}\"\n",
							"print(db_location)\n",
							"dbutils.fs.ls(db_location)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"We now create a table in the schema with custom location and insert data. \n",
							"\n",
							"Note that the schema must be provided because there is no data from which to infer the schema."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"USE ${da.db_name}_custom_location;\n",
							"\n",
							"CREATE OR REPLACE TABLE managed_table_in_db_with_custom_location (width INT, length INT, height INT);\n",
							"INSERT INTO managed_table_in_db_with_custom_location VALUES (3, 2, 1);\n",
							"SELECT * FROM managed_table_in_db_with_custom_location;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"Again, we'll look at the description to find the table location."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DESCRIBE DETAIL managed_table_in_db_with_custom_location;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"As expected, this managed table is created in the path specified with the **`LOCATION`** keyword during schema creation. As such, the data and metadata for the table are persisted in a directory here."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"\n",
							"table_name = f\"managed_table_in_db_with_custom_location\"\n",
							"tbl_location =   f\"{DA.paths.working_dir}/_custom_location.db/{table_name}\"\n",
							"print(tbl_location)\n",
							"\n",
							"files = dbutils.fs.ls(tbl_location)\n",
							"display(files)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"Let's drop the table."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DROP TABLE managed_table_in_db_with_custom_location;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"Note the table's folder and the log file and data file are deleted.  \n",
							"  \n",
							"Only the schema location remains"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"\n",
							"db_location =   f\"{DA.paths.working_dir}/_custom_location.db\"\n",
							"print(db_location)\n",
							"\n",
							"dbutils.fs.ls(db_location)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"## Tables\n",
							"We will create an external (unmanaged) table from sample data. \n",
							"\n",
							"The data we are going to use are in CSV format. We want to create a Delta table with a **`LOCATION`** provided in the directory of our choice."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"USE ${da.db_name}_default_location;\n",
							"\n",
							"CREATE OR REPLACE TEMPORARY VIEW temp_delays USING CSV OPTIONS (\n",
							"  path = '${DA.paths.datasets}/flights/departuredelays.csv',\n",
							"  header = \"true\",\n",
							"  mode = \"FAILFAST\" -- abort file parsing with a RuntimeException if any malformed lines are encountered\n",
							");\n",
							"CREATE OR REPLACE TABLE external_table LOCATION '${da.paths.working_dir}/external_table' AS\n",
							"  SELECT * FROM temp_delays;\n",
							"\n",
							"SELECT * FROM external_table;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"Let's note the location of the table's data in this lesson's working directory."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DESCRIBE TABLE EXTENDED external_table;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"Now, we drop the table."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DROP TABLE external_table;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"The table definition no longer exists in the metastore, but the underlying data remain intact."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"tbl_path = f\"{DA.paths.working_dir}/external_table\"\n",
							"files = dbutils.fs.ls(tbl_path)\n",
							"display(files)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"## Clean up\n",
							"Drop both schemas."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DROP SCHEMA ${da.db_name}_default_location CASCADE;\n",
							"DROP SCHEMA ${da.db_name}_custom_location CASCADE;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"Run the following cell to delete the tables and files associated with this lesson."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"DA.cleanup()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 3-2A - Views and CTEs on Databricks')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "03 - Relational Entities on Databricks"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "LakehouseTest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a28c5ca4-621c-45d8-98e4-f7ebecb9c729"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/8aa90780-bc26-461d-9db6-6df705873871/resourceGroups/domo-demo/providers/Microsoft.Synapse/workspaces/domodemo/bigDataPools/LakehouseTest",
						"name": "LakehouseTest",
						"type": "Spark",
						"endpoint": "https://domodemo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/LakehouseTest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Views and CTEs on Databricks\n",
							"In this demonstration, you will create and explore views and common table expressions (CTEs).\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lesson, you should be able to:\n",
							"* Use Spark SQL DDL to define views\n",
							"* Run queries that use common table expressions\n",
							"\n",
							"\n",
							"\n",
							"**Resources**\n",
							"* <a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/sql-ref-syntax-ddl-create-view.html\" target=\"_blank\">Create View - Databricks Docs</a>\n",
							"* <a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/sql-ref-syntax-qry-select-cte.html\" target=\"_blank\">Common Table Expressions - Databricks Docs</a>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Classroom Setup\n",
							"The following script clears out previous runs of this demo and configures some Hive variables that will be used in our SQL queries."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../Includes/Classroom-Setup-03.2A"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"We start by creating a table of data we can use for the demonstration."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- mode \"FAILFAST\" will abort file parsing with a RuntimeException if any malformed lines are encountered\n",
							"CREATE TABLE external_table\n",
							"USING CSV OPTIONS (\n",
							"  path = '${da.paths.datasets}/flights/departuredelays.csv',\n",
							"  header = \"true\",\n",
							"  mode = \"FAILFAST\"\n",
							");\n",
							"\n",
							"SELECT * FROM external_table;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"To show a list of tables (and views), we use the **`SHOW TABLES`** command also demonstrated below."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SHOW TABLES;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Views, Temp Views & Global Temp Views\n",
							"\n",
							"To set this demonstration up, we are going to first create one of each type of view.\n",
							"\n",
							"Then in the next notebook, we will explore the differences between how each one behaves."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"### Views\n",
							"Let's create a view that contains only the data where the origin is \"ABQ\" and the destination is \"LAX\"."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE VIEW view_delays_abq_lax AS\n",
							"  SELECT * \n",
							"  FROM external_table \n",
							"  WHERE origin = 'ABQ' AND destination = 'LAX';\n",
							"\n",
							"SELECT * FROM view_delays_abq_lax;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							" \n",
							"Note that the **`view_delays_abq_lax`** view has been added to the list below:"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SHOW TABLES;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"### Temporary Views\n",
							"\n",
							"Next we'll create a temporary view. \n",
							"\n",
							"The syntax is very similar but adds **`TEMPORARY`** to the command."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE TEMPORARY VIEW temp_view_delays_gt_120\n",
							"AS SELECT * FROM external_table WHERE delay > 120 ORDER BY delay ASC;\n",
							"\n",
							"SELECT * FROM temp_view_delays_gt_120;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Now if we show our tables again, we will see the one table and both views.\n",
							"\n",
							"Make note of the values in the **`isTemporary`** column."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SHOW TABLES;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"### Global Temp Views\n",
							"\n",
							"Lastly, we'll create a global temp view. \n",
							"\n",
							"Here we simply add **`GLOBAL`** to the command. \n",
							"\n",
							"Also note the **`global_temp`** database qualifer in the subsequent **`SELECT`** statement."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE GLOBAL TEMPORARY VIEW global_temp_view_dist_gt_1000 \n",
							"AS SELECT * FROM external_table WHERE distance > 1000;\n",
							"\n",
							"SELECT * FROM global_temp.global_temp_view_dist_gt_1000;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Before we move on, review one last time the database's tables and views..."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SHOW TABLES;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"...and the tables and views in the **`global_temp`** database:"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SHOW TABLES IN global_temp;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Next we are going to demonstrate how tables and views are persisted across multiple sessions and how temp views are not.\n",
							"\n",
							"To do this simply open the next notebook, [DE 3.2B - Views and CTEs on Databricks, Cont]($./DE 3.2B - Views and CTEs on Databricks, Cont), and continue with the lesson.\n",
							"\n",
							"<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"> Note: There are several scenarios in which a new session may be created:\n",
							"* Restarting a cluster\n",
							"* Detaching and reataching to a cluster\n",
							"* Installing a python package which in turn restarts the Python interpreter\n",
							"* Or simply opening a new notebook"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 3-2B - Views and CTEs on Databricks  Cont')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "03 - Relational Entities on Databricks"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "LakehouseTest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7b81a44a-aeb5-4466-9e47-22fcc675debe"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/8aa90780-bc26-461d-9db6-6df705873871/resourceGroups/domo-demo/providers/Microsoft.Synapse/workspaces/domodemo/bigDataPools/LakehouseTest",
						"name": "LakehouseTest",
						"type": "Spark",
						"endpoint": "https://domodemo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/LakehouseTest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Views and CTEs on Databricks, Cont'\n",
							"\n",
							"We are picking up from notebook [DE 3.2A - Views and CTEs on Databricks]($./DE 3.2A - Views and CTEs on Databricks) where we just reviewed the following two lists of tables<br/>\n",
							"and views with the special note that our global temp view **`global_temp_view_dist_gt_1000`** was not included in the first list."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAsMAAAC9CAIAAADZWeeNAAAALHRFWHRDcmVhdGlvbiBUaW1lAFN1biAxMyBNYXIgMjAyMiAxNzoxMToxMCAtMDYwMClifCwAAAAHdElNRQfmAw0XChauURgcAAAACXBIWXMAAAsSAAALEgHS3X78AAAABGdBTUEAALGPC/xhBQAAJyxJREFUeNrtnU9rJdfRh/ORHPkb+ENk8D6QjMbLQJhdEHhlMGhhGPDCIFAWDkNAi2EWJogwNmHQwlmYxMO8oFmFsUzwWIuY2fRbvmUVpfOv+/btc+85V88DI+70Pd19urq66nfqnJZ+9X8AAAAAc/mV/HvbHm32CgB2DsEBZoDb1AMlAQCdQXCAGeA29UBJAEBnEBxgBrhNPVASANAZBAeYAW5TD5QEAHQGwQFmgNvUAyUBAJ1BcIAZ4Db16FVJXF9fHx0dHR4eXl1dzT7Fy5cv79279+jRo11fKwCswbyQdXZ2dnBwcHFxMXG7ISFCGkgz/S9xo1Nit5H0IUlEUokklLi9ZpmDCPMEMPZcSZycnMhjn/uWiADQI8ngIKFAnuVkSlA2VBIWbYgbnbKukjC446Pss5KQGy+3HyUB0AU//fTT6xXyodxyXkrYUEkIGiuIG02xidtM5M7e8bVs25OSkKddH+mPP/7YKwnbbk+7RgcrRvk6lckL9Q89lN8e717Y6I98B10NYBH+97//XV5e/m2FfJD/FhrHwSF4ulVY6BaLEvr8fv7559JGPpjs8EpCY4LuaNrClIQePMgrcVjQcHRycqJ9kMZffPGFHtYPfuywo2NiyLGh23gBmvQZJVYSlnEsa0y56eoqgTBVvBeZ4/n2Wm/LZTHNhr9dYe6kXZo9F7OubbtREv6uq430Dqkp1V4+KPiahHzWxv4gumO8XQ8eHMSfxTb60ohvAADT8TFrSuQqBwd9Kv0z7kcX+tkHWQsaQWMfPeSzSBDd3TdLxgrdGEcq/aw7WmPtOYOQGSzrNpYjYt0QbPHOY3tNuemjHpjcbucdzWJvU35bqMova9tulIQ3cTy74csSsZJ4e7t4EEeZuL2JRK8kggGEP4KPX7s2HkA3xDFrNHJNmd3wQ0wfxzU4+Pa2PS5OBCMHHR1qVSM5oPRKIp4Ksc9Bby1D7PpW9MTibmOlgnieq5ApTEqO3vS3GQ/8/vvvfeLweSSedxvNYua3E5eALGjbbpSEN2tcDNBbW6hJ6G2O1VxQpTSf0IPHB/HlL18LTZbFAKBALmaVI9eUmoSOOnJx3AeQQEl4gvqBf979yDKIFX7MU1AS/kSzx453kxpuEwRzX12OlUTgJ9J49Ka/zXig9Mqn/KRn+vOWs5gJkWfPns2b2pht226URK4m4c2dVBLxZFi5JhGoh+AhtygQ1DkBYC3KMasQuaakhIkjwlxNwpMcTgQi4200uzGqJKhfzqOS29hGdQMf9svVa2VdJbFuTWJiFnt7M8Fx//79GfJ0E9t2oyRy6yTU3FbP8arCKwnf2N+DWG1osLB501x0sIWcudk1ACjw3XffffXVV+WwJQ2kWbBjHBzitRH+WfZx3AeQ8jqJZHXTDyTe3giL0VgRJ5XkOGfXN6QPFnSbIJ1bRTmYbyqvk5h+00c98G1+/UQ5i8WrOuat5N3Ett0oCW8j/+6GnzqS7Ta7aY3FyraKQv3G5379r5+YCGZYg3vsRyRvUxNXu7YcQB/8+OOP33zzTTlsSQNpFuyYDA42N+HnKf64wo/w4pXzuXc3rDKcXEGluydjxZSk8taVN5gSXYsF3SY5KRZPNhXe3UgqgLKSsLeHJr67Yf8tZDF/KD+7t03b9qQkAAAIDjCDnbvN6G8uWQRVElteeYOSAIDOIDjADHbuNttREn52b2uXhpIAgM4gOMAMdu42W1AS8XLR7YCSAIDOIDjADHCbeqAkAKAzCA4wA9ymHigJAOgMggPMALepB0oCADqD4AAzwG3qgZIAgM4gOMAMcJt6/KIkAAAAAObRbk1iAACIIDjADHCbeqAkAKAzCA4wA9ymHigJAOgMggPMALepB0oCADqD4AAzwG3qgZIAgM4gOMAMcJt6oCQAoDMIDjAD3KYevSqJn3766cMPP3zy5Il9qGejr7/++oMPPvjhhx9q34xPV2x4kC0YZPtdGrXM5eXlgwcP5GcXBskhnZSuSodrH0cavLti1LHFpO+//7423tw5l6LBlLChm+V2XyQmgJJ0Gwnv4tvi5MnosbXg3zsoiXFQErW7NJr5UBILHsf7s1i10FLaSEtpb58bMeMmSqLgKpuwoZOjJLZA7Dajdw0lMZEllcT19fXR0dHZ2RlKYoc0mDi3oCT6MshsOyx+nLV8u52shpKAGeSUhGrlJH0F/x2ymJJQGXFwcFBVSVit9eEKUxKnp6dyv2W7f1blCdSqrK9c2RFsox4hLvba7vLBtse7a2B6/PixNn6ywj4HnipbCnHBokauS3Zku0wdKepGPUvBIDH6nEhjf4QhVdPWw6pNtJ/Hx8fSpnD8+GYlL02ri9bV5LXr6XS7bZQd9chyli+//NLSQ3CPfIzWc8lPf5bRSD3P7N9++612o3wLrCfSWK9RGwcnlTPKlXqXs8+yXb7NHafMWomqnayWDA7xTbHbbd/+5S9/0Tb2/PrH3C7TfPvFixdiavlv0Cb20rKSCJw893zFj623eeyHsBax2wRBIE4ZpiRyESP2n7vJYkri0aNHJycnh4eH9ZREnBIsVfucoXdUGsRPo6/WmotYdg+O7x1IPyd316BgJ9VeDavHPnn8gv61fpa7ZC2Db70RYoMk0Q5b5/1e2k9LWrrRm7Qs1ZM3K3dpfriWbKCPq3bJCvI+T1g/43v0+vVrPY434PSbspbZvZ/4nFE4vl273ys+qX4wc5kyswtJHqfw5Jenh2NGL2SblIODV1pmSTOj/9aMFjhbMHKwHc0CsZdOr0kUnq/4sbVuJ10C1mI0p8QpI5cmzA1i/7mbLDm7cXV1VVVJ+BAQPMD2XMXFqEBeFPKfnwn2z6rt5V3N/CkYIMYhzEaN9iF3M+IAkevSJgbxWCIcMvXVIFPat2stXEge2V9asvAbXLs1sCP7S0tu9A4glg+qOxOT6Fpm93UX/eyVTUxgGbODiYbBVR0slsnI9fz8XH6aE+aOM3p13gEK6FW0k72SwcE/XN6AKkNjVwlEpD3d/o77e528leal82Y3gucrfmxjHRlfLExk4jqJZMqII0bOf3Z9lbuhJyURJAl9tAJX8GnbStwSC4JxW3BYq2hJy8IxrbpoyJZRJWEOJ5RjsRfC5S7FBsmNjcpBJ2nSwZXsHqyYoSRyR44vLbgvyQb+dFbnTyqJ+BbbaM+OptjdLKTSeWafriSCbzUYvXnzxuqovod6K1+9eiXf+p9BtWxYJ6hNGUu1JiOGVEowtzG8GvDVlKB85XcJkveQVxITn1CP98zR58seW18Gi11i1/ehM0ZLWXHKCAqZ3vg5/9n1Ve6GnpTE6FhwSGn5IVMhVHIJYLQmkexVUkkMN7VQoVwcjovnuS7NMEjyjMmaRPKKFqlJ5C4tWSTMXfu8mkRwXYHZC3l35zUJ3/j4+FirEcOqMvH48eN4JmhYpyZR7t7Q2CsbxmhNIrCwr0jlahLGqJJI3uvpSmLK81WuScA8ykpiYsoYbiKGKv5G5vt2Tk9KwhedVD/aA+xH//bg+YnqeJ2EPsMvXrzwhSybzvcp1uZNg911oDNFSWjj0WGEVxLJLnlpYq5cmLCfuE4iOIIfw8m+82Y3CjcruabEK4m4gXVjuL1OIk4PuVscnChYyFY20XSzr6skBreexq9ECU7qJ8j9Cgn57BfzxseZckUFoTnqP7uinBKSE2fldRKD86uJSiLw0rWURO75Ssax5OD4Lg9/ZzOqJOKUkRyaxouThrEByd7T2e+TsIKSrikrvLthpSrZIkO3eHuwwNteSYhXSstA0IY7fvd4eVdOSYwGdyWe3ch1yY68yLsbVji1tGTVVxkBx+OwYdqar/hm5S5NrRq8yuEbfJp5dyNWEvEtTuaY3NsxSaabfYaSGG4v/47f3fAC1C8yjUdLyeOUr6isbr0lm3prYPTdDQv0doHxrbdcHtzctWY3zEtHlYR38tzzNfHdDaY25jFxdsOnjOS7G8lX/O64tutMSfTLlOLk9pdk87Y09MieBQfYDrhNPVAS28D/MoAC258KRUlAj+xTcICtgdvUAyVRHT/fP9z+nTbGO++8895779UrWloJLqhUP3/+fEMlEa+Z77H6mrwp7y7322ZqHz9J7qbH0xPxLEYw29UazQaHndxomEizbrMHoCQAoDMIDjAD3KYeKAkA6AyCA8wAt6kHSgIAOoPgADPAbeqBkgCAziA4wAxwm3qgJACgMwgOMAPcph6/KAkAAACAeVCTAICeIDjADHCbeqAkAKAzCA4wA9ymHigJAOgMggPMALepB0oCADqD4AAzwG3qgZIAgM4gOMAMcJt6oCQAoDMIDjAD3KYevSoJ+7vy/g/MV2JrfzBzkb8qvgWDbL9Lo5a5vLx88OBB8s+GNWiQHNLJ5F/YWvw40kD/stSoY/s/7tXOn6GamBIWeaBylI285b+yu7mTF56gvSHpNvo3CHN/dJC/ljwRlMQ4KInaXRrNfCiJBY/j/VmsWmgpbaSl/kVQ/dyIGVESASiJKcRuM2o3lMREFlMSZ2dnByuOjo6ur69REruiwcS5BSXRl0Fm22Hx46zl21UT81q0UKZGSXRHTkmoVk7SV/DfIcsoiaurq8PDw4uLC9EQoiREVVRSElZrfbjClMTp6ancb9nun22JelqV9ZUrO4Jt1CPExV7bXT7Y9nh3fQIfP36sjZ+ssM+Bp8qWQiy2SJ3rkh3ZLlNHirpRz1IwSIw+J9LYH2FI1bT1sGoT7efx8bG0KRw/vlnJS9PqonU1ee16Ot1uG2VHPbKc5csvv7Q4GNwjH2T1XPLTn2U0O84z+7fffqvdKN8C64k01mvUxsFJ5Yxypd7l7LNsl29zxymzljhoWUn4vpl9/MbAnnLXvKn958I1TrxZQ1T48YEoFxPsQchV2o3c41kIg9JeYlQhKXqnsp5oENOD+OdutIdtErtNEATilGH3MRcxfJrY9fXtkoVnN1RSVFIScUqwVO0fUXss7XGygOKrteYi9iQHx/cOpJ+Tu+tT7WOBHkF+Jo9f0L/Wz3KXrGXwrTdCbJAk2mHrfBzpLL7oRm/SslRP3qzcpSVDuW+gj6t2yQryJgt8P+N79Pr1az2ON+D0m7KW2b2faCfVPQrHt2v3e8Un1Q9mLlNmdiHJ4xSe/PL0cMzohWyTODgEN1cv30wX29OUmTQTTWCfy84w/WbZxmQgit3PK0W/S0zh8YyfeotC3j9z99eeIN+T2G72fHXHaE6J71QuTagFzB86KnxWYkkloQWJe/fuvXz5soaS8Ko5mN2wZBkXowJ5UXiW/EywH80kg4L5UzBA9LrepIaOGu1D7mbEY75clzYxiMcPL5IPQ5Ap7du1Fi4kj+wvLVkoDq7dGtiR/aUlN3oHEMsH1Z2JSXQts/u6i34uR97AMj4F2kZzG4tlMvQ8Pz+Xn+aEueOMXt3E8aVeRTujrjg4eDtLP+1DIMLMnqovNYXLtzJe12e58ISudbPKgSiOCfMsHDyewVNvGtpvnFKTMEyIDK4O144brMvEdRLJlBFHjEB3lvXf3rOYklAZcXBwcHFxsYguKQ87hpvoELiCf0StxC1PSDBuCw5rFS1pWTim1TB96W9USZjDCeWH0Avhcpdig+QWjpSDY9KkgyvZPVgxQ0nkjhxfWnBfkg386WzMlFQS8S224ZodTbG7WUil88w+XUkE32owevPmjdVRfQ/1Vr569Uq+9T+DatmwTlCbMpZqTUYMmXUSqrSkt2qT4XYNKbantten0v8s22HizQpqlkEgSsYEP2U2GijKj6dXS+sqCW+uWH93vWigrCSSdyqodflb7O/XxDeh9pjFlMSjR4+WqkbklMToWHBwN97nnmSRU8klgNGaRLJXSSUx3MQ4GyrliIvnuS7NMEjyjMmaRPKKFqlJ5C4tWSTMXfu8mkRwXYHZC3l35zUJ3/j4+FirEcOqMiEj6XgmaFinJjFarG7qlQ0jt4hKMp/YJ37wk/aU9mJPrUZoxJD/Fkyx1s0qB6KhGBPKE0lTHs9NahK+WfyIWYrtkbKSmJgyhpuIoSKy04mexVlGSYiAEBlxcEPVdRJWHpSHzSK4H/1b7PAT1fE6CX0wXrx44QtZNp3vU6ytCQh216d9ipLQxqNlZK8kkl0KZoLVlQsT9hPXSQRH8IFM9p03u1G4Wck1JV5JxA2sG8PtdRKxksjd4uBEcUwvmGi62ddVEkM0kx1fmh8ISj/9Cgn57BfuxceZckWF7DLqP7siqST8WgHdkswHdsla3PKrJayYsfnN8koiDkRDFBN86PArFWIKj2f81CfXe+UOa0oid0y5uj1eJ5G8U0lFGC9OGsYGJHtPZ79PwgpKutyp8O6Glapkiww74u22WM/K6Z/eoOeyWpYuyAre3bAkN0VJjAZ3JZ7dyHUpnrzc5N0NK2ZagLDyqYzwNHCsqySSNyt3aWrV4FUO3+DTzLsbsZKIb3Gc181tJtYkp5t9hpIYbi//jl8H8KnRLzKNR0vJ45SvqKxuvSWDdxN2S+4t0KAYk3x2AqkRL5YsM/FmxbMbQSCKY0L8xsRoH4LHM/nUW98+La6Sjmc3pNtyfNPifv5xb97dSM5u+DuVfHcj+YrfXZ7aGLpTEv2SLIHGbbY8/uNtaeiR/QgOU2LCgvCw74fbtAlKYhuUy5XGliPLQHCBPtmD4DAxJiwID/seuE2zoCSq4+f7h9u/msl455133nvvvXo1QyvBBZXq58+fbxhcfEk2WL6+E2vPI3lTRpfQt3P8JLmbHk9PxLMYwWxXa1QNDluwRhATCm0WfLJUSTx79mwPHth57E1OaRCUBAB0BsEBZoDb1AMlAQCdQXCAGeA29UBJAEBnEBxgBrhNPVASANAZBAeYAW5TD5QEAHQGwQFmgNvU4xclAQAAADAPahIA0BMEB5gBblMPlAQAdAbBAWaA29QDJQEAnUFwgBngNvVASQBAZxAcYAa4TT1QEgDQGQQHmAFuUw+UBAB0BsEBZoDb1AMlAQCd0WBwqPqXNp88eZL802tbOPUm6B+us798Jj2UfsZ/Sc7+ZFrhGhehQbfZG1ASANAZcXDQLLXDP156p5REuT+K/f1bvSn6X/3zp/pZ/xCubfcNKtGg2+wNiymJi4uLgxVnZ2coCQCox11LCd0pCS0zSJvcTbEjSMsHDx7o3zSXllXLEnfNbbbJMkri6urq8PBQNMTLly/v378vP1ESAFCJODjo8NdGwJKogiq6ptvT01NrJmNiv0vcIHd2n7ltJO03BmeXjQ8fPtRk6ROn/JTtOQVgY3pJycfHx3YhuUvT48jnd2/Q8b1cpg30LVXbhEL5SpPdeP78eTxDEfB6RSFPm5IIOl9VEpXdRgwlF6gC6MWLF3abgqswt9GaCigLz26gJACgNuXBpU9In64YblKs5lRNxtpYvrWUZulBPktG0UQS489lysBOmjy7pXP5KerBPheykeVaPV2cd/2l6UYvWXx/LOVrN/zyhbKayXVjyuzGkB/x63Y1gu/eaGequo0Yx2zrBZ9vYxe+hbmYvlhSSYiAuHfv3sXFxSJHQ0kAQJLRlGAh3pJTbuzrB8emHkbzhM/iQTpPnt1Ew+np6fn5ufwMViMGBB2wTo5emiHNAnlhH/TgU6RArhubKIng7E0pCRN2SSUR3LLaczF9sXBN4vr6+ujoaJGlEigJAEhSSAlWjTdUH0xREj4f+5wdYzlPmvlCyOvXr5Nn1/avXr2Sb/3PXNYMcrAmrTdv3pQvzV+7nM5XL+RapOXx8bFmPt+ykA6DbmyuJGIRs9vZjbWUhH/3RGnzlZmdsOQ6iYuLC1swgZIAgEpMr0kYy9YktMH5+bmpgWRNwreXLK7ViGFVmXj8+HFhamNKTSK+tNw1qhCRM8b7+omG6d2YpyT8KxuGr0PsdsXlujUJ8Cz/7sbR0dH19TVKAgAqMX2dhBX5pygJW0hRXieh6GILv5gxPpGdfVglKr9CQj6Xc5Lt68fx5Uuzbusu1tIG03pGb6vR7LjgOglbkuLbtPMWqFcSwUoOv5LXLzphdsPg90kAQGckg4MuqvdrKq3+P0yrSWimnPJGw3B72BofMzj7cDshTSzj+9cE4nc34kvz71mcn5/77vnlhIP7ZVCj7yDYMfXFFv/qx+gl+DxdmBrY7W+mMrfxSsLbWd9Yid/dYGrDg5IAgM6oERya/U2Ri1B+T2SbB9kh5JR6oCQAoDNQEmuxyQS/rcwor6joAnJKPVASANAZ21ESfgrAs+Cau/hNk8V/65FeRfmAhSstzINswT7LQk6pB0oCADqD4AAzwG3qgZIAgM4gOMAMcJt6oCQAoDMIDjAD3KYeKAkA6AyCA8wAt6kHSgIAOoPgADPAberxi5IAAAAAmMevdq1mAAAAoGNQEgAAADAflAQAAADMByUBAAAA80FJAAAAwHxQEgAAADAflAQAAADMByUBAAAA80FJAAAAwHxQEgAAADAflAQAAADMByUBAAAA8+EveAEA9MquMwjAz/yqTV9ss1dtgq2gZfDPemBbaASURPdgK2gZ/LMe2BYaASXRPdgKWgb/rAe2hUZASXQPtoKWwT/rgW2hEVAS3YOtoGXwz3pgW2gElET3YCtoGfyzHtgWGqFXJXF0dHR4eHh1dTX7FC9fvrx3796jR492fa2bMu8Onp2dHRwcXFxcTNxuiMWkgTTT/+6NGaESsX/KYysPrzzCuV3kq4MIczkw2ozecAfZZyVxcnIieS737d6kwKStxDLlS9tQSZjx98aMUIkZSkLBtUZpM3rDHWRvlYQEIAlDXSuJ1ytm2GpKpN5QSQhquvbNCJWY7Z8TucuuVdu2AMvSk5KQ9KY57OOPP/ZKwrZbetN06IuiVi81eaFxSg/ltyd3z230R1425F1eXv5thXxY11bBxaqw0C1mNL2czz//XNrIB5MdXkmoiXRH0xamJPTgQbiPraR35+TkRPsgjb/44gs9rNeCdtjRoSq0wCb+6ZVu0jmVWEnYk25P6xTvUp8MFLDi3dU83LfXxrnooVHotyvMb7VLm8zFbGJbgJ3QjZLw0UefVY0U+kjrc+uzoK9JyGdt7A+iO8bb9eDBQfxZ/JFN0PgGm2OhZEpAGa1JHK0YbodmC5fD7dhnNgwae2PKZ5EgurtvljSdboxvnH7WHa2x9vxuDkM7YkH/tGcz1g3BFu+lttcU7xp19eR2O+9o9BhSD0ihGlrVtgA7oRslESj9YHbDlyViJWG7+FFvEKeC9jZY8UoiGDEHR7CEvSFBKBkNKFNmN/zIz4dXtZVvb9vj4kQgpHTQplWN5DjPK4l4KsQ+B721wF3FsWBjlvVPKxXEE2qFJ9Q066h3DRlX//7774MH1v6bnOArRw97QCYuAalnW4Cd0I2SCB7voBigIaZQk9BwE48qgrK8xSY9eHwQX4b1xf9keXYGyVBSDihTahIqwnLh1dszUBKeoH7gL98P+ALTeQlYUBL+RJsM6aAqi/tn8BD5ql6sJAKHlMaj3jVkXF16FaT8+BHw5y1Hj+FGiDx79mz21MYitgXYCd0oiVxNwj/2SSURT8qWaxKBegiymqW9oLC/CIVQUggoUyL1xIHakKlJeJLqKhAZQzS7MaokWB7RPjX801B/849buWqorKsk1q1JTIwew80Ex/379+fp4KVsC7ATulESuXUS+thbXdGrCq8kfGMfC2K1odnRFgrk0qEt5MzN8q7Ld99999VXX5WjiTSQZqO2GqK1Ef7SfHj19iyvk0gWe7yumm66ONYnZR80xYL+GaRzq+QFE1vldRLTvWvU1Qvby9EjXtUxb8nwss8+wPbpRkkM7lkN3t2wKUzZbjneGsvTbqsoNH753K//9RMTwZKCINb4IbgSTKDO5scff/zmm2/K0UQaSLMptrK5CT9P8ccVfuAVL2jPvbth1aDkghLdPWm6KbF+cOUNFkm0yYL+mZx9i2e1Cu9uJBVAWUnYa0oT392w/xaiRzBysGnEHdoWYCf0pCQgCbaCltm5f47+ipSlECWx5SU+O7ctgIKS6B5sBS2zc//cjpLw04hbY+e2BVBQEt2DraBldu6fW1AS8XLR7bBz2wIoKInuwVbQMvhnPbAtNAJKonuwFbQM/lkPbAuNgJLoHmwFLYN/1gPbQiOgJLoHW0HL4J/1wLbQCCiJ7sFW0DL4Zz2wLTTCz0oCAAB6ZNcZBOBnqEl0D7aClsE/64FtoRFQEt2DraBl8M96YFtoBJRE92AraBn8sx7YFhoBJdE92ApaBv+sB7aFRkBJdA+2gpbBP+uBbaERUBLdg62gZfDPemBbaITulcSHH3745MmTed96vv766w8++OCHH37Y9aWvDbYK+HRFpYOLicRQtc9SRuyvfVDkRsjtePfdd4ObIt17d4W09/3XjRPv9ea0GWH2A2wLjYCS+IVydtRg7SNyO2CrgJ0riXkWmLiXNBAd4JWE9cR3ye6Rv1mXl5cPHjy4XKEfKlnJE/vnzj1kb2gzesMdZEklcXR0dHZ2tsihyI7TwVbbxJREgXpKQoTC+++/f3x8bH3we4kyePjwod4XryqksTbwnZdvt1OWQEnUAyUBjbCYkhAZcXBwUFVJSKCUMCoDsocrNA5a/rMary/nyrenp6e6PRjGaUs5oI7MytlR9vVHthKxHVN3l3NZs6C2LP/VBBBXobHVbFv5fGnjbL8xOLscSqyhV+HH5T4HFy7KZ3E7i5na+h9YIImWFrRjco3Pnz+fuNdwWxAEV2GfvS60z94yW5udif3TX6neazGgbHzx4oX1P1Ab5iG7mlFqE5QENMIySuLRo0cnJyeHh4dVlYQFRI3CQXb031rOk43+s4YhP81s8XT6ONu39LtblzR7WVK0xBNvxFYb2sofP553SJ7dxuLy0zSW/CykKDuyXlpwFhvxmxwZHXP7BmaB6SP1QEmYBvI6yXrlL9nXIcqXvCDlmoT0IZ58iU1k1zu9cnYXQElAIyxWk7i6uqqqJIKZ3SApFr6NZ479YX3KnJgdfTi2OO53z6W35DgSW21iq+Bc9iEQDf7slkFPT0/Pz8/l53A778b47BXrlQ9X5HqVxF+1Nb6zSiKuKgVt/LUEq03vOCgJaIRulESQvYLsWP5WN1rMtXK0/JTItW52tNqsr/lPyY4WMX3Ex1Yb2kozojSTfYPyQ/LsemmvXr2Sb/3Pics+LJMFCxGCOZSyJvBSYHMl0d3sxlpKws/EKS2/N7RlUBLQCN0oidnjbIunlrSS8XTeONvYj5pEj7ZSZXB+fh73M7eo8Pj42KoR8vPx48flnFquSQQXLg22WZMIVn74FZdeSdg0gbdSCysu161JgAclAY3QjZIY5s79W8i24rwlKh1wbzL3L2eM37XLZUfrVbPrJDq1lV/hMWQUj51dG/gVEvK5nKj8vjaXoWcJ0r/mvG2ukxhuK6fu3gIN6k8qxYYbv02+csLshoGSgEbo6fdJWKVd18/Pfh/BjiNbZDyq20d/25IuA/TrBHOvMxTG2cnfIIStNrRVLrMmzz7czlITf8uWf3eg8O6GH/GXf/uTvbuhNre0OuV3RgXX2+NvprIrDUo71j312/jdDaY2PG1Gb7iD9KQkPN0VPOtNS2Orrll20UyDtBlh9gNsC43QjZLwAdfX5Hthm0oCWzWOX9Dgp072kjYjzH6AbaERulESg6t8ln97zyZYzTle87/hkbdck9gzW9U7l+EnKTybWM9q8h7/OzOSV1HYq0fajDD7AbaFRuhJSUASbAUtg3/WA9tCI6AkugdbQcvgn/XAttAIKInuwVbQMvhnPbAtNAJKonuwFbQM/lkPbAuNgJLoHmwFLYN/1gPbQiP8rCQAAKBHdp1BAH6GmkT3YCtoGfyzHtgWGgEl0T3YCloG/6wHtoVGQEl0D7aClsE/64FtoRFQEt2DraBl8M96YFtoBJRE92AraBn8sx7YFhoBJdE92ApaBv+sB7aFRkBJdE+DtvrHP/7x+9///j//+U+Ng//1r3/905/+9N///nf7p94E6bB0W7qn/5UeSj9/vcJfzr/+9a/f/OY3wcauadA/9wZsC42Akuie2FaapSxpbZ87pSTK/VFURog+0Jui/5Ud7fMnn3zit/sGvdOgf+4NxElohGWUhIS8eHS1CTwh07lrkbo7JaFlBmmTuyl2BGn5u9/9Tn7qhexHWeKu+ec2IU5CIyygJCwuLDiQ4gmZTmwrHf7aCDjWeZpuP/vsM2smY2K/S9wgd3afuc0B/Mbg7LLxD3/4gyZLnzjlp2zPKQAb00tK/uijj+xCcpemx5HPv75B3VIu0/zTUrVNKJSvNNmNv//976Ma+nJFIX2akgg63+Y0zbqU/VPuiFhSldY///lP84fAXOafWrwBhTgJjbDk7IY+/CiJLVMe8/mE9MmK4SbF6p3SZKyN5VtLaRa15bMEeo3vMf5cpgzspMmzWzqXn6Ie7HMhSViu1dPFeddfmm70ksX3x1K+dsMvXyirmVw3psxuDPmBuH9qfPdGO9MLZf+Uu2A30StL38YsvE+TPotAnIRGWExJ6ENeSDlrwRMyndFIbZHXklNu7OsHx3YrR8O3z+JBOk+e3UTDZ5999vTpU/kZrEYMCDpgnRy9NEOaBfLCPujBp0iBXDc2URLB2e+mkjAFmVQSgW/szaTPIhAnoRGWURLBgrLN4QmZTiFS230xVB9MURI+H/ucHWM5T5r5QsirV6+SZ9f2//73v+Vb/zOXNYMcrLlENpYvzV+7nM5XL+RapOVHH32kCcm3LGSpoBubK4lYxNyR2Y21lIR/yUXZD7MsAnESGmEZJSGxYKlqhMITMp3pNQlj2ZqENnj69KmpgWRNwreXLK7ViGFVmfjzn/9cmNqYUpOILy13jSpE5IzxvuXpuWVrEv6VDcPXIfZm8L1sTQI8xElohAWUhF+wZrPvG8ITMp3p6ySsyD9FSditLK+TUHSxhV/MGJ/Izj6s8odfISGfy6nC9vXj+PKlWbd1F2tpY1w9o7fVaNJacJ2ELUnxbe7gW6BeSQRLRvySYb+6ZT8E1iIQJ6ER+H0S3ZO0la5192sqrf4/TKtJaKac8kbDcHs0GR8zOPtwO09MLOP71fvxuxvxpfn3LJ4+feq751f5DbelcPnVADumvtjiX/0YvQSfPgsV+zvym6nMP72SGNwN1Vdj4nc3mNrwECehEVAS3VPDVnszSZ+k/J7INg9yF+BZrge2hUZASXQPSmItNpl3t5UZC77wvPfwLNcD20IjoCS6ZztKIlgNYyy4FC5+02TxX0akV1E+YOFKC/MgW7BPp/As1wPbQiOgJLoHW0HL4J/1wLbQCCiJ7sFW0DL4Zz2wLTQCSqJ7sBW0DP5ZD2wLjYCS6B5sBS2Df9YD20IjoCS6B1tBy+Cf9cC20Ag/KwkAAOiRXWcQgJ/5f0iFd+UHV8twAAAAAElFTkSuQmCC\" />"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"With the Notebook's state reset, we need to re-initialize some of our lesson-specific configuration.\n",
							"\n",
							"Note: We will **NOT** be recreating the database for the second 1/2 of this lesson."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../Includes/Classroom-Setup-03.2B"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"But, we do need to configure this session to use our database by default."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"USE ${da.db_name};"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Finally, run the following two cells to confirm that:\n",
							"1. The table **`external_table`** still exists.\n",
							"2. The view **`view_delays_abq_lax`** still exists.\n",
							"3. The temp view **`temp_view_delays_gt_120`** does **NOT** exist.\n",
							"3. The global temp view **`global_temp_view_dist_gt_1000`** does exist in the special **`global_temp`** database.\n",
							"\n",
							"<img src=\"https://files.training.databricks.com/images/icon_hint_24.png\"> Hint: If you were to go back to the previous notebook and run **`SHOW TABLES`**<br/>\n",
							"again, all three tables and views from the current database will still be shown."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SHOW TABLES;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"SHOW TABLES IN global_temp;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"As mentioned previously, temp views are tied to a Spark session and as such are not accessible...\n",
							"* After restarting a cluster\n",
							"* After detaching and reataching to a cluster\n",
							"* After installing a python package which in turn restarts the Python interpreter\n",
							"* Or from another notebook\n",
							"\n",
							"...with the special exception of global temporary views."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Global temp views behave much like other temporary views but differ in one important way. \n",
							"\n",
							"They are added to the **`global_temp`** database that exists on the **`cluster`**.\n",
							"\n",
							"As long as the cluster is running, this database persists and any notebooks attached to the cluster can access its global temporary views.  \n",
							"  \n",
							"We can see this in action by running the follow cells:"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT * FROM global_temp.global_temp_view_dist_gt_1000;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Global temp views are \"lost\" when the cluster is restarted.\n",
							"\n",
							"Take our word for it, don't do it now, but if you were to restart the cluster, the above select statement would fail because the table would no longer exist."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"## Common Table Expressions (CTEs)\n",
							"CTEs can be used in a variety of contexts. Below, are a few examples of the different ways a CTE can be used in a query. First, an example of making multiple column aliases using a CTE."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"WITH flight_delays(\n",
							"  total_delay_time,\n",
							"  origin_airport,\n",
							"  destination_airport\n",
							") AS (\n",
							"  SELECT\n",
							"    delay,\n",
							"    origin,\n",
							"    destination\n",
							"  FROM\n",
							"    external_table\n",
							")\n",
							"SELECT\n",
							"  *\n",
							"FROM\n",
							"  flight_delays\n",
							"WHERE\n",
							"  total_delay_time > 120\n",
							"  AND origin_airport = \"ATL\"\n",
							"  AND destination_airport = \"DEN\";"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"Next, is an example of a CTE in a CTE definition."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"WITH lax_bos AS (\n",
							"  WITH origin_destination (origin_airport, destination_airport) AS (\n",
							"    SELECT\n",
							"      origin,\n",
							"      destination\n",
							"    FROM\n",
							"      external_table\n",
							"  )\n",
							"  SELECT\n",
							"    *\n",
							"  FROM\n",
							"    origin_destination\n",
							"  WHERE\n",
							"    origin_airport = 'LAX'\n",
							"    AND destination_airport = 'BOS'\n",
							")\n",
							"SELECT\n",
							"  count(origin_airport) AS `Total Flights from LAX to BOS`\n",
							"FROM\n",
							"  lax_bos;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"Now, here is an example of a CTE in a subquery."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT\n",
							"  max(total_delay) AS `Longest Delay (in minutes)`\n",
							"FROM\n",
							"  (\n",
							"    WITH delayed_flights(total_delay) AS (\n",
							"      SELECT\n",
							"        delay\n",
							"      FROM\n",
							"        external_table\n",
							"    )\n",
							"    SELECT\n",
							"      *\n",
							"    FROM\n",
							"      delayed_flights\n",
							"  );"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"We can also use a CTE in a subquery expression."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT\n",
							"  (\n",
							"    WITH distinct_origins AS (\n",
							"      SELECT DISTINCT origin FROM external_table\n",
							"    )\n",
							"    SELECT\n",
							"      count(origin) AS `Number of Distinct Origins`\n",
							"    FROM\n",
							"      distinct_origins\n",
							"  ) AS `Number of Different Origin Airports`;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"Finally, here is a CTE in a **`CREATE VIEW`** statement."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REPLACE VIEW BOS_LAX \n",
							"AS WITH origin_destination(origin_airport, destination_airport) \n",
							"AS (SELECT origin, destination FROM external_table)\n",
							"SELECT * FROM origin_destination\n",
							"WHERE origin_airport = 'BOS' AND destination_airport = 'LAX';\n",
							"\n",
							"SELECT count(origin_airport) AS `Number of Delayed Flights from BOS to LAX` FROM BOS_LAX;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Clean up \n",
							"We first drop the training database."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DROP DATABASE ${da.db_name} CASCADE;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"Run the following cell to delete the tables and files associated with this lesson."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"DA.cleanup()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 3-3L - Databases  Tables   Views Lab')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "03 - Relational Entities on Databricks"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "LakehouseTest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "f80b53a9-82ef-448e-b796-e0333917489d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/8aa90780-bc26-461d-9db6-6df705873871/resourceGroups/domo-demo/providers/Microsoft.Synapse/workspaces/domodemo/bigDataPools/LakehouseTest",
						"name": "LakehouseTest",
						"type": "Spark",
						"endpoint": "https://domodemo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/LakehouseTest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Databases, Tables, and Views Lab\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lab, you should be able to:\n",
							"- Create and explore interactions between various relational entities, including:\n",
							"  - Databases\n",
							"  - Tables (managed and external)\n",
							"  - Views (views, temp views, and global temp views)\n",
							"\n",
							"**Resources**\n",
							"* <a href=\"https://docs.databricks.com/user-guide/tables.html\" target=\"_blank\">Databases and Tables - Databricks Docs</a>\n",
							"* <a href=\"https://docs.databricks.com/user-guide/tables.html#managed-and-unmanaged-tables\" target=\"_blank\">Managed and Unmanaged Tables</a>\n",
							"* <a href=\"https://docs.databricks.com/user-guide/tables.html#create-a-table-using-the-ui\" target=\"_blank\">Creating a Table with the UI</a>\n",
							"* <a href=\"https://docs.databricks.com/user-guide/tables.html#create-a-local-table\" target=\"_blank\">Create a Local Table</a>\n",
							"* <a href=\"https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#saving-to-persistent-tables\" target=\"_blank\">Saving to Persistent Tables</a>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"### Getting Started\n",
							"\n",
							"Run the following cell to configure variables and datasets for this lesson."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../Includes/Classroom-Setup-03.3L"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Overview of the Data\n",
							"\n",
							"The data include multiple entries from a selection of weather stations, including average temperatures recorded in either Fahrenheit or Celsius. The schema for the table:\n",
							"\n",
							"|ColumnName  | DataType| Description|\n",
							"|------------|---------|------------|\n",
							"|NAME        |string   | Station name |\n",
							"|STATION     |string   | Unique ID |\n",
							"|LATITUDE    |float    | Latitude |\n",
							"|LONGITUDE   |float    | Longitude |\n",
							"|ELEVATION   |float    | Elevation |\n",
							"|DATE        |date     | YYYY-MM-DD |\n",
							"|UNIT        |string   | Temperature units |\n",
							"|TAVG        |float    | Average temperature |\n",
							"\n",
							"This data is stored in the Parquet format; preview the data with the query below."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT * \n",
							"FROM parquet.`${DA.paths.datasets}/weather/StationData-parquet`"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Create a Database\n",
							"\n",
							"Create a database in the default location using the **`da.db_name`** variable defined in setup script."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"\n",
							"<FILL-IN> ${da.db_name}"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"Run the cell below to check your work."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"assert spark.sql(f\"SHOW DATABASES\").filter(f\"databaseName == '{DA.db_name}'\").count() == 1, \"Database not present\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Change to Your New Database\n",
							"\n",
							"**`USE`** your newly created database."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"\n",
							"<FILL-IN> ${da.db_name}"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"Run the cell below to check your work."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"assert spark.sql(f\"SHOW CURRENT DATABASE\").first()[\"namespace\"] == DA.db_name, \"Not using the correct database\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Create a Managed Table\n",
							"Use a CTAS statement to create a managed table named **`weather_managed`**."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"\n",
							"<FILL-IN>\n",
							"SELECT * \n",
							"FROM parquet.`${DA.paths.datasets}/weather/StationData-parquet`"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"Run the cell below to check your work."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"assert spark.table(\"weather_managed\"), \"Table named `weather_managed` does not exist\"\n",
							"assert spark.table(\"weather_managed\").count() == 2559, \"Incorrect row count\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Create an External Table\n",
							"\n",
							"Recall that an external table differs from a managed table through specification of a location. Create an external table called **`weather_external`** below."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"\n",
							"<FILL-IN>\n",
							"LOCATION \"${da.paths.working_dir}/lab/external\"\n",
							"AS SELECT * \n",
							"FROM parquet.`${DA.paths.datasets}/weather/StationData-parquet`"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"Run the cell below to check your work."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"assert spark.table(\"weather_external\"), \"Table named `weather_external` does not exist\"\n",
							"assert spark.table(\"weather_external\").count() == 2559, \"Incorrect row count\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Examine Table Details\n",
							"Use the SQL command **`DESCRIBE EXTENDED table_name`** to examine the two weather tables."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DESCRIBE EXTENDED weather_managed"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DESCRIBE EXTENDED weather_external"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Run the following helper code to extract and compare the table locations."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"def getTableLocation(tableName):\n",
							"    return spark.sql(f\"DESCRIBE DETAIL {tableName}\").select(\"location\").first()[0]"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"managedTablePath = getTableLocation(\"weather_managed\")\n",
							"externalTablePath = getTableLocation(\"weather_external\")\n",
							"\n",
							"print(f\"\"\"The weather_managed table is saved at: \n",
							"\n",
							"    {managedTablePath}\n",
							"\n",
							"The weather_external table is saved at:\n",
							"\n",
							"    {externalTablePath}\"\"\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"List the contents of these directories to confirm that data exists in both locations."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"files = dbutils.fs.ls(managedTablePath)\n",
							"display(files)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"files = dbutils.fs.ls(externalTablePath)\n",
							"display(files)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"### Check Directory Contents after Dropping Database and All Tables\n",
							"The **`CASCADE`** keyword will accomplish this."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"\n",
							"<FILL_IN> ${da.db_name}"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"Run the cell below to check your work."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"assert spark.sql(f\"SHOW DATABASES\").filter(f\"databaseName == '{DA.db_name}'\").count() == 0, \"Database present\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"With the database dropped, the files will have been deleted as well.\n",
							"\n",
							"Uncomment and run the following cell, which will throw a **`FileNotFoundException`** as your confirmation."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"# files = dbutils.fs.ls(managedTablePath)\n",
							"# display(files)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"files = dbutils.fs.ls(externalTablePath)\n",
							"display(files)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"files = dbutils.fs.ls(DA.paths.working_dir)\n",
							"display(files)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"**This highlights the main differences between managed and external tables.** By default, the files associated with managed tables will be stored to this location on the root DBFS storage linked to the workspace, and will be deleted when a table is dropped.\n",
							"\n",
							"Files for external tables will be persisted in the location provided at table creation, preventing users from inadvertently deleting underlying files. **External tables can easily be migrated to other databases or renamed, but these operations with managed tables will require rewriting ALL underlying files.**"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Create a Database with a Specified Path\n",
							"\n",
							"Assuming you dropped your database in the last step, you can use the same **`database`** name."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE DATABASE ${da.db_name} LOCATION '${da.paths.working_dir}/${da.db_name}';\n",
							"USE ${da.db_name};"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Recreate your **`weather_managed`** table in this new database and print out the location of this table."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"\n",
							"<FILL_IN>"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"getTableLocation(\"weather_managed\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"Run the cell below to check your work."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"assert spark.table(\"weather_managed\"), \"Table named `weather_managed` does not exist\"\n",
							"assert spark.table(\"weather_managed\").count() == 2559, \"Incorrect row count\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"While here we're using the **`working_dir`** directory created on the DBFS root, _any_ object store can be used as the database directory. **Defining database directories for groups of users can greatly reduce the chances of accidental data exfiltration**."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Views and their Scoping\n",
							"\n",
							"In this section, use the provided **`AS`** clause to register:\n",
							"- a view named **`celsius`**\n",
							"- a temporary view named **`celsius_temp`**\n",
							"- a global temp view named **`celsius_global`**\n",
							"\n",
							"Start by creating the first view in the code cell below."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"\n",
							"<FILL-IN>\n",
							"AS (SELECT *\n",
							"  FROM weather_managed\n",
							"  WHERE UNIT = \"C\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"Run the cell below to check your work."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"assert spark.table(\"celsius\"), \"Table named `celsius` does not exist\"\n",
							"assert spark.sql(f\"SHOW TABLES\").filter(f\"tableName == 'celsius'\").first()[\"isTemporary\"] == False, \"Table is temporary\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Now create a temporary view."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"\n",
							"<FILL-IN>\n",
							"AS (SELECT *\n",
							"  FROM weather_managed\n",
							"  WHERE UNIT = \"C\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"Run the cell below to check your work."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"assert spark.table(\"celsius_temp\"), \"Table named `celsius_temp` does not exist\"\n",
							"assert spark.sql(f\"SHOW TABLES\").filter(f\"tableName == 'celsius_temp'\").first()[\"isTemporary\"] == True, \"Table is not temporary\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Now register a global temp view."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"\n",
							"<FILL-IN>\n",
							"AS (SELECT *\n",
							"  FROM weather_managed\n",
							"  WHERE UNIT = \"C\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"Run the cell below to check your work."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"assert spark.table(\"global_temp.celsius_global\"), \"Global temporary view named `celsius_global` does not exist\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Views will be displayed alongside tables when listing from the catalog."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SHOW TABLES"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Note the following:\n",
							"- The view is associated with the current database. This view will be available to any user that can access this database and will persist between sessions.\n",
							"- The temp view is not associated with any database. The temp view is ephemeral and is only accessible in the current SparkSession.\n",
							"- The global temp view does not appear in our catalog. **Global temp views will always register to the **`global_temp`** database**. The **`global_temp`** database is ephemeral but tied to the lifetime of the cluster; however, it is only accessible by notebooks attached to the same cluster on which it was created."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT * FROM global_temp.celsius_global"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"While no job was triggered when defining these views, a job is triggered _each time_ a query is executed against the view."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Clean Up\n",
							"Drop the database and all tables to clean up your workspace."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DROP DATABASE ${da.db_name} CASCADE"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Synopsis\n",
							"\n",
							"In this lab we:\n",
							"- Created and deleted databases\n",
							"- Explored behavior of managed and external tables\n",
							"- Learned about the scoping of views"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"Run the following cell to delete the tables and files associated with this lesson."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"DA.cleanup()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 4-1 - Querying Files Directly')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "04 - ETL with Spark SQL"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "LakehouseTest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a6dc63e4-4658-4c94-a60a-8a050917db45"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/8aa90780-bc26-461d-9db6-6df705873871/resourceGroups/domo-demo/providers/Microsoft.Synapse/workspaces/domodemo/bigDataPools/LakehouseTest",
						"name": "LakehouseTest",
						"type": "Spark",
						"endpoint": "https://domodemo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/LakehouseTest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Extracting Data Directly from Files\n",
							"\n",
							"In this notebook, you'll learn to extract data directly from files using Spark SQL on Databricks.\n",
							"\n",
							"A number of file formats support this option, but it is most useful for self-describing data formats (such as parquet and JSON).\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lesson, you should be able to:\n",
							"- Use Spark SQL to directly query data files\n",
							"- Leverage **`text`** and **`binaryFile`** methods to review raw file contents"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Run Setup\n",
							"\n",
							"The setup script will create the data and declare necessary values for the rest of this notebook to execute."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../Includes/Classroom-Setup-04.1"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Data Overview\n",
							"\n",
							"In this example, we'll work with a sample of raw Kafka data written as JSON files. \n",
							"\n",
							"Each file contains all records consumed during a 5-second interval, stored with the full Kafka schema as a multiple-record JSON file.\n",
							"\n",
							"| field | type | description |\n",
							"| --- | --- | --- |\n",
							"| key | BINARY | The **`user_id`** field is used as the key; this is a unique alphanumeric field that corresponds to session/cookie information |\n",
							"| value | BINARY | This is the full data payload (to be discussed later), sent as JSON |\n",
							"| topic | STRING | While the Kafka service hosts multiple topics, only those records from the **`clickstream`** topic are included here |\n",
							"| partition | INTEGER | Our current Kafka implementation uses only 2 partitions (0 and 1) |\n",
							"| offset | LONG | This is a unique value, monotonically increasing for each partition |\n",
							"| timestamp | LONG | This timestamp is recorded as milliseconds since epoch, and represents the time at which the producer appends a record to a partition |"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Note that our source directory contains many JSON files."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"print(DA.paths.kafka_events)\n",
							"\n",
							"files = dbutils.fs.ls(DA.paths.kafka_events)\n",
							"display(files)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Here, we'll be using relative file paths to data that's been written to the DBFS root. \n",
							"\n",
							"Most workflows will require users to access data from external cloud storage locations. \n",
							"\n",
							"In most companies, a workspace administrator will be responsible for configuring access to these storage locations.\n",
							"\n",
							"Instructions for configuring and accessing these locations can be found in the cloud-vendor specific self-paced courses titled \"Cloud Architecture & Systems Integrations\"."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Query a Single File\n",
							"\n",
							"To query the data contained in a single file, execute the query with the following pattern:\n",
							"\n",
							"<strong><code>SELECT * FROM file_format.&#x60;/path/to/file&#x60;</code></strong>\n",
							"\n",
							"Make special note of the use of back-ticks (not single quotes) around the path."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT * FROM json.`${DA.paths.kafka_events}/001.json`"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Note that our preview displays all 321 rows of our source file."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Query a Directory of Files\n",
							"\n",
							"Assuming all of the files in a directory have the same format and schema, all files can be queried simultaneously by specifying the directory path rather than an individual file."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT * FROM json.`${DA.paths.kafka_events}`"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"By default, this query will only show the first 1000 rows."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Create References to Files\n",
							"This ability to directly query files and directories means that additional Spark logic can be chained to queries against files.\n",
							"\n",
							"When we create a view from a query against a path, we can reference this view in later queries. Here, we'll create a temporary view, but you can also create a permanent reference with regular view."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REPLACE TEMP VIEW events_temp_view\n",
							"AS SELECT * FROM json.`${DA.paths.kafka_events}`;\n",
							"\n",
							"SELECT * FROM events_temp_view"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Extract Text Files as Raw Strings\n",
							"\n",
							"When working with text-based files (which include JSON, CSV, TSV, and TXT formats), you can use the **`text`** format to load each line of the file as a row with one string column named **`value`**. This can be useful when data sources are prone to corruption and custom text parsing functions will be used to extract value from text fields."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT * FROM text.`${DA.paths.kafka_events}`"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Extract the Raw Bytes and Metadata of a File\n",
							"\n",
							"Some workflows may require working with entire files, such as when dealing with images or unstructured data. Using **`binaryFile`** to query a directory will provide file metadata alongside the binary representation of the file contents.\n",
							"\n",
							"Specifically, the fields created will indicate the **`path`**, **`modificationTime`**, **`length`**, and **`content`**."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT * FROM binaryFile.`${DA.paths.kafka_events}`"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"Run the following cell to delete the tables and files associated with this lesson."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"DA.cleanup()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 4-2 - Providing Options for External Sources')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "04 - ETL with Spark SQL"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "LakehouseTest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "6ccb97fd-f4a9-40f1-8d0f-69b091a6993b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/8aa90780-bc26-461d-9db6-6df705873871/resourceGroups/domo-demo/providers/Microsoft.Synapse/workspaces/domodemo/bigDataPools/LakehouseTest",
						"name": "LakehouseTest",
						"type": "Spark",
						"endpoint": "https://domodemo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/LakehouseTest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Providing Options for External Sources\n",
							"While directly querying files works well for self-describing formats, many data sources require additional configurations or schema declaration to properly ingest records.\n",
							"\n",
							"In this lesson, we will create tables using external data sources. While these tables will not yet be stored in the Delta Lake format (and therefore not be optimized for the Lakehouse), this technique helps to facilitate extracting data from diverse external systems.\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lesson, you should be able to:\n",
							"- Use Spark SQL to configure options for extracting data from external sources\n",
							"- Create tables against external data sources for various file formats\n",
							"- Describe default behavior when querying tables defined against external sources"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Run Setup\n",
							"\n",
							"The setup script will create the data and declare necessary values for the rest of this notebook to execute."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../Includes/Classroom-Setup-04.2"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## When Direct Queries Don't Work \n",
							"\n",
							"While views can be used to persist direct queries against files between sessions, this approach has limited utility.\n",
							"\n",
							"CSV files are one of the most common file formats, but a direct query against these files rarely returns the desired results."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT * FROM csv.`${DA.paths.sales_csv}`"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"We can see from the above that:\n",
							"1. The header row is being extracted as a table row\n",
							"1. All columns are being loaded as a single column\n",
							"1. The file is pipe-delimited (**`|`**)\n",
							"1. The final column appears to contain nested data that is being truncated"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Registering Tables on External Data with Read Options\n",
							"\n",
							"While Spark will extract some self-describing data sources efficiently using default settings, many formats will require declaration of schema or other options.\n",
							"\n",
							"While there are many <a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/sql-ref-syntax-ddl-create-table-using.html\" target=\"_blank\">additional configurations</a> you can set while creating tables against external sources, the syntax below demonstrates the essentials required to extract data from most formats.\n",
							"\n",
							"<strong><code>\n",
							"CREATE TABLE table_identifier (col_name1 col_type1, ...)<br/>\n",
							"USING data_source<br/>\n",
							"OPTIONS (key1 = val1, key2 = val2, ...)<br/>\n",
							"LOCATION = path<br/>\n",
							"</code></strong>\n",
							"\n",
							"Note that options are passed with keys as unquoted text and values in quotes. Spark supports many <a href=\"https://docs.databricks.com/data/data-sources/index.html\" target=\"_blank\">data sources</a> with custom options, and additional systems may have unofficial support through external <a href=\"https://docs.databricks.com/libraries/index.html\" target=\"_blank\">libraries</a>. \n",
							"\n",
							"**NOTE**: Depending on your workspace settings, you may need administrator assistance to load libraries and configure the requisite security settings for some data sources."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"The cell below demonstrates using Spark SQL DDL to create a table against an external CSV source, specifying:\n",
							"1. The column names and types\n",
							"1. The file format\n",
							"1. The delimiter used to separate fields\n",
							"1. The presence of a header\n",
							"1. The path to where this data is stored"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE TABLE sales_csv\n",
							"  (order_id LONG, email STRING, transactions_timestamp LONG, total_item_quantity INTEGER, purchase_revenue_in_usd DOUBLE, unique_items INTEGER, items STRING)\n",
							"USING CSV\n",
							"OPTIONS (\n",
							"  header = \"true\",\n",
							"  delimiter = \"|\"\n",
							")\n",
							"LOCATION \"${DA.paths.sales_csv}\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Note that no data has moved during table declaration. \n",
							"\n",
							"Similar to when we directly queried our files and created a view, we are still just pointing to files stored in an external location.\n",
							"\n",
							"Run the following cell to confirm that data is now being loaded correctly."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT * FROM sales_csv"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT COUNT(*) FROM sales_csv"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"All the metadata and options passed during table declaration will be persisted to the metastore, ensuring that data in the location will always be read with these options.\n",
							"\n",
							"**NOTE**: When working with CSVs as a data source, it's important to ensure that column order does not change if additional data files will be added to the source directory. Because the data format does not have strong schema enforcement, Spark will load columns and apply column names and data types in the order specified during table declaration.\n",
							"\n",
							"Running **`DESCRIBE EXTENDED`** on a table will show all of the metadata associated with the table definition."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DESCRIBE EXTENDED sales_csv"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Limits of Tables with External Data Sources\n",
							"\n",
							"If you've taken other courses on Databricks or reviewed any of our company literature, you may have heard about Delta Lake and the Lakehouse. Note that whenever we're defining tables or queries against external data sources, we **cannot** expect the performance guarantees associated with Delta Lake and Lakehouse.\n",
							"\n",
							"For example: while Delta Lake tables will guarantee that you always query the most recent version of your source data, tables registered against other data sources may represent older cached versions.\n",
							"\n",
							"The cell below executes some logic that we can think of as just representing an external system directly updating the files underlying our table."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"(spark.read\n",
							"      .option(\"header\", \"true\")\n",
							"      .option(\"delimiter\", \"|\")\n",
							"      .csv(DA.paths.sales_csv)\n",
							"      .write.mode(\"append\")\n",
							"      .format(\"csv\")\n",
							"      .save(DA.paths.sales_csv))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"If we look at the current count of records in our table, the number we see will not reflect these newly inserted rows."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT COUNT(*) FROM sales_csv"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"At the time we previously queried this data source, Spark automatically cached the underlying data in local storage. This ensures that on subsequent queries, Spark will provide the optimal performance by just querying this local cache.\n",
							"\n",
							"Our external data source is not configured to tell Spark that it should refresh this data. \n",
							"\n",
							"We **can** manually refresh the cache of our data by running the **`REFRESH TABLE`** command."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"REFRESH TABLE sales_csv"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Note that refreshing our table will invalidate our cache, meaning that we'll need to rescan our original data source and pull all data back into memory. \n",
							"\n",
							"For very large datasets, this may take a significant amount of time."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT COUNT(*) FROM sales_csv"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Extracting Data from SQL Databases\n",
							"SQL databases are an extremely common data source, and Databricks has a standard JDBC driver for connecting with many flavors of SQL.\n",
							"\n",
							"The general syntax for creating these connections is:\n",
							"\n",
							"<strong><code>\n",
							"CREATE TABLE <jdbcTable><br/>\n",
							"USING JDBC<br/>\n",
							"OPTIONS (<br/>\n",
							"&nbsp; &nbsp; url = \"jdbc:{databaseServerType}://{jdbcHostname}:{jdbcPort}\",<br/>\n",
							"&nbsp; &nbsp; dbtable = \"{jdbcDatabase}.table\",<br/>\n",
							"&nbsp; &nbsp; user = \"{jdbcUsername}\",<br/>\n",
							"&nbsp; &nbsp; password = \"{jdbcPassword}\"<br/>\n",
							")\n",
							"</code></strong>\n",
							"\n",
							"In the code sample below, we'll connect with <a href=\"https://www.sqlite.org/index.html\" target=\"_blank\">SQLite</a>.\n",
							"  \n",
							"**NOTE:** SQLite uses a local file to store a database, and doesn't require a port, username, or password.  \n",
							"  \n",
							"<img src=\"https://files.training.databricks.com/images/icon_warn_24.png\"> **WARNING**: The backend-configuration of the JDBC server assume you are running this notebook on a single-node cluster. If you are running on a cluster with multiple workers, the client running in the executors will not be able to connect to the driver."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DROP TABLE IF EXISTS users_jdbc;\n",
							"\n",
							"CREATE TABLE users_jdbc\n",
							"USING JDBC\n",
							"OPTIONS (\n",
							"  url = \"jdbc:sqlite:${DA.paths.ecommerce_db}\",\n",
							"  dbtable = \"users\"\n",
							")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Now we can query this table as if it were defined locally."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT * FROM users_jdbc"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Looking at the table metadata reveals that we have captured the schema information from the external system.\n",
							"\n",
							"Storage properties (which would include the username and password associated with the connection) are automatically redacted."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DESCRIBE EXTENDED users_jdbc"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"While the table is listed as **`MANAGED`**, listing the contents of the specified location confirms that no data is being persisted locally."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"import pyspark.sql.functions as F\n",
							"\n",
							"location = spark.sql(\"DESCRIBE EXTENDED users_jdbc\").filter(F.col(\"col_name\") == \"Location\").first()[\"data_type\"]\n",
							"print(location)\n",
							"\n",
							"files = dbutils.fs.ls(location)\n",
							"print(f\"Found {len(files)} files\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Note that some SQL systems such as data warehouses will have custom drivers. Spark will interact with various external databases differently, but the two basic approaches can be summarized as either:\n",
							"1. Moving the entire source table(s) to Databricks and then executing logic on the currently active cluster\n",
							"1. Pushing down the query to the external SQL database and only transferring the results back to Databricks\n",
							"\n",
							"In either case, working with very large datasets in external SQL databases can incur significant overhead because of either:\n",
							"1. Network transfer latency associated with moving all data over the public internet\n",
							"1. Execution of query logic in source systems not optimized for big data queries"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"Run the following cell to delete the tables and files associated with this lesson."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"DA.cleanup()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 4-3 - Creating Delta Tables')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "04 - ETL with Spark SQL"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "LakehouseTest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "4c7f82af-f1d6-409d-8965-cc59eb648ed5"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/8aa90780-bc26-461d-9db6-6df705873871/resourceGroups/domo-demo/providers/Microsoft.Synapse/workspaces/domodemo/bigDataPools/LakehouseTest",
						"name": "LakehouseTest",
						"type": "Spark",
						"endpoint": "https://domodemo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/LakehouseTest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Creating Delta Tables\n",
							"\n",
							"After extracting data from external data sources, load data into the Lakehouse to ensure that all of the benefits of the Databricks platform can be fully leveraged.\n",
							"\n",
							"While different organizations may have varying policies for how data is initially loaded into Databricks, we typically recommend that early tables represent a mostly raw version of the data, and that validation and enrichment occur in later stages. This pattern ensures that even if data doesn't match expectations with regards to data types or column names, no data will be dropped, meaning that programmatic or manual intervention can still salvage data in a partially corrupted or invalid state.\n",
							"\n",
							"This lesson will focus primarily on the pattern used to create most tables, **`CREATE TABLE _ AS SELECT`** (CTAS) statements.\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lesson, you should be able to:\n",
							"- Use CTAS statements to create Delta Lake tables\n",
							"- Create new tables from existing views or tables\n",
							"- Enrich loaded data with additional metadata\n",
							"- Declare table schema with generated columns and descriptive comments\n",
							"- Set advanced options to control data location, quality enforcement, and partitioning"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Run Setup\n",
							"\n",
							"The setup script will create the data and declare necessary values for the rest of this notebook to execute."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../Includes/Classroom-Setup-04.3"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Create Table as Select (CTAS)\n",
							"\n",
							"**`CREATE TABLE AS SELECT`** statements create and populate Delta tables using data retrieved from an input query."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REPLACE TABLE sales AS\n",
							"SELECT * FROM parquet.`${da.paths.datasets}/ecommerce/raw/sales-historical`;\n",
							"\n",
							"DESCRIBE EXTENDED sales;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"CTAS statements automatically infer schema information from query results and do **not** support manual schema declaration. \n",
							"\n",
							"This means that CTAS statements are useful for external data ingestion from sources with well-defined schema, such as Parquet files and tables.\n",
							"\n",
							"CTAS statements also do not support specifying additional file options.\n",
							"\n",
							"We can see how this would present significant limitations when trying to ingest data from CSV files."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REPLACE TABLE sales_unparsed AS\n",
							"SELECT * FROM csv.`${da.paths.datasets}/ecommerce/raw/sales-csv`;\n",
							"\n",
							"SELECT * FROM sales_unparsed;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"To correctly ingest this data to a Delta Lake table, we'll need to use a reference to the files that allows us to specify options.\n",
							"\n",
							"In the previous lesson, we showed doing this by registering an external table. Here, we'll slightly evolve this syntax to specify the options to a temporary view, and then use this temp view as the source for a CTAS statement to successfully register the Delta table."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REPLACE TEMP VIEW sales_tmp_vw\n",
							"  (order_id LONG, email STRING, transactions_timestamp LONG, total_item_quantity INTEGER, purchase_revenue_in_usd DOUBLE, unique_items INTEGER, items STRING)\n",
							"USING CSV\n",
							"OPTIONS (\n",
							"  path = \"${da.paths.datasets}/ecommerce/raw/sales-csv\",\n",
							"  header = \"true\",\n",
							"  delimiter = \"|\"\n",
							");\n",
							"\n",
							"CREATE TABLE sales_delta AS\n",
							"  SELECT * FROM sales_tmp_vw;\n",
							"  \n",
							"SELECT * FROM sales_delta"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"## Filtering and Renaming Columns from Existing Tables\n",
							"\n",
							"Simple transformations like changing column names or omitting columns from target tables can be easily accomplished during table creation.\n",
							"\n",
							"The following statement creates a new table containing a subset of columns from the **`sales`** table. \n",
							"\n",
							"Here, we'll presume that we're intentionally leaving out information that potentially identifies the user or that provides itemized purchase details. We'll also rename our fields with the assumption that a downstream system has different naming conventions than our source data."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REPLACE TABLE purchases AS\n",
							"SELECT order_id AS id, transaction_timestamp, purchase_revenue_in_usd AS price\n",
							"FROM sales;\n",
							"\n",
							"SELECT * FROM purchases"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Note that we could have accomplished this same goal with a view, as shown below."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REPLACE VIEW purchases_vw AS\n",
							"SELECT order_id AS id, transaction_timestamp, purchase_revenue_in_usd AS price\n",
							"FROM sales;\n",
							"\n",
							"SELECT * FROM purchases_vw"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"## Declare Schema with Generated Columns\n",
							"\n",
							"As noted previously, CTAS statements do not support schema declaration. We note above that the timestamp column appears to be some variant of a Unix timestamp, which may not be the most useful for our analysts to derive insights. This is a situation where generated columns would be beneficial.\n",
							"\n",
							"Generated columns are a special type of column whose values are automatically generated based on a user-specified function over other columns in the Delta table (introduced in DBR 8.3).\n",
							"\n",
							"The code below demonstrates creating a new table while:\n",
							"1. Specifying column names and types\n",
							"1. Adding a <a href=\"https://docs.databricks.com/delta/delta-batch.html#deltausegeneratedcolumns\" target=\"_blank\">generated column</a> to calculate the date\n",
							"1. Providing a descriptive column comment for the generated column"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REPLACE TABLE purchase_dates (\n",
							"  id STRING, \n",
							"  transaction_timestamp LONG, \n",
							"  price STRING,\n",
							"  date DATE GENERATED ALWAYS AS (\n",
							"    cast(cast(transaction_timestamp/1e6 AS TIMESTAMP) AS DATE))\n",
							"    COMMENT \"generated based on `transactions_timestamp` column\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"\n",
							"Because **`date`** is a generated column, if we write to **`purchase_dates`** without providing values for the **`date`** column, Delta Lake automatically computes them.\n",
							"\n",
							"**NOTE**: The cell below configures a setting to allow for generating columns when using a Delta Lake **`MERGE`** statement. We'll see more on this syntax later in the course."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SET spark.databricks.delta.schema.autoMerge.enabled=true; \n",
							"\n",
							"MERGE INTO purchase_dates a\n",
							"USING purchases b\n",
							"ON a.id = b.id\n",
							"WHEN NOT MATCHED THEN\n",
							"  INSERT *"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"We can see below that all dates were computed correctly as data was inserted, although neither our source data or insert query specified the values in this field.\n",
							"\n",
							"As with any Delta Lake source, the query automatically reads the most recent snapshot of the table for any query; you never need to run **`REFRESH TABLE`**."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT * FROM purchase_dates"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"It's important to note that if a field that would otherwise be generated is included in an insert to a table, this insert will fail if the value provided does not exactly match the value that would be derived by the logic used to define the generated column.\n",
							"\n",
							"We can see this error by uncommenting and running the cell below:"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- INSERT INTO purchase_dates VALUES\n",
							"-- (1, 600000000, 42.0, \"2020-06-18\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Add a Table Constraint\n",
							"\n",
							"The error message above refers to a **`CHECK constraint`**. Generated columns are a special implementation of check constraints.\n",
							"\n",
							"Because Delta Lake enforces schema on write, Databricks can support standard SQL constraint management clauses to ensure the quality and integrity of data added to a table.\n",
							"\n",
							"Databricks currently support two types of constraints:\n",
							"* <a href=\"https://docs.databricks.com/delta/delta-constraints.html#not-null-constraint\" target=\"_blank\">**`NOT NULL`** constraints</a>\n",
							"* <a href=\"https://docs.databricks.com/delta/delta-constraints.html#check-constraint\" target=\"_blank\">**`CHECK`** constraints</a>\n",
							"\n",
							"In both cases, you must ensure that no data violating the constraint is already in the table prior to defining the constraint. Once a constraint has been added to a table, data violating the constraint will result in write failure.\n",
							"\n",
							"Below, we'll add a **`CHECK`** constraint to the **`date`** column of our table. Note that **`CHECK`** constraints look like standard **`WHERE`** clauses you might use to filter a dataset."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"ALTER TABLE purchase_dates ADD CONSTRAINT valid_date CHECK (date > '2020-01-01');"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Table constraints are shown in the **`TBLPROPERTIES`** field."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DESCRIBE EXTENDED purchase_dates"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Enrich Tables with Additional Options and Metadata\n",
							"\n",
							"So far we've only scratched the surface as far as the options for enriching Delta Lake tables.\n",
							"\n",
							"Below, we show evolving a CTAS statement to include a number of additional configurations and metadata.\n",
							"\n",
							"Our **`SELECT`** clause leverages two built-in Spark SQL commands useful for file ingestion:\n",
							"* **`current_timestamp()`** records the timestamp when the logic is executed\n",
							"* **`input_file_name()`** records the source data file for each record in the table\n",
							"\n",
							"We also include logic to create a new date column derived from timestamp data in the source.\n",
							"\n",
							"The **`CREATE TABLE`** clause contains several options:\n",
							"* A **`COMMENT`** is added to allow for easier discovery of table contents\n",
							"* A **`LOCATION`** is specified, which will result in an external (rather than managed) table\n",
							"* The table is **`PARTITIONED BY`** a date column; this means that the data from each date will exist within its own directory in the target storage location\n",
							"\n",
							"**NOTE**: Partitioning is shown here primarily to demonstrate syntax and impact. Most Delta Lake tables (especially small-to-medium sized data) will not benefit from partitioning. Because partitioning physically separates data files, this approach can result in a small files problem and prevent file compaction and efficient data skipping. The benefits observed in Hive or HDFS do not translate to Delta Lake, and you should consult with an experienced Delta Lake architect before partitioning tables.\n",
							"\n",
							"**As a best practice, you should default to non-partitioned tables for most use cases when working with Delta Lake.**"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REPLACE TABLE users_pii\n",
							"COMMENT \"Contains PII\"\n",
							"LOCATION \"${da.paths.working_dir}/tmp/users_pii\"\n",
							"PARTITIONED BY (first_touch_date)\n",
							"AS\n",
							"  SELECT *, \n",
							"    cast(cast(user_first_touch_timestamp/1e6 AS TIMESTAMP) AS DATE) first_touch_date, \n",
							"    current_timestamp() updated,\n",
							"    input_file_name() source_file\n",
							"  FROM parquet.`${da.paths.datasets}/ecommerce/raw/users-historical/`;\n",
							"  \n",
							"SELECT * FROM users_pii;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"The metadata fields added to the table provide useful information to understand when records were inserted and from where. This can be especially helpful if troubleshooting problems in the source data becomes necessary.\n",
							"\n",
							"All of the comments and properties for a given table can be reviewed using **`DESCRIBE TABLE EXTENDED`**.\n",
							"\n",
							"**NOTE**: Delta Lake automatically adds several table properties on table creation."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DESCRIBE EXTENDED users_pii"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"Listing the location used for the table reveals that the unique values in the partition column **`first_touch_date`** are used to create data directories."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"files = dbutils.fs.ls(f\"{DA.paths.working_dir}/tmp/users_pii\")\n",
							"display(files)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Cloning Delta Lake Tables\n",
							"Delta Lake has two options for efficiently copying Delta Lake tables.\n",
							"\n",
							"**`DEEP CLONE`** fully copies data and metadata from a source table to a target. This copy occurs incrementally, so executing this command again can sync changes from the source to the target location."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REPLACE TABLE purchases_clone\n",
							"DEEP CLONE purchases"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Because all the data files must be copied over, this can take quite a while for large datasets.\n",
							"\n",
							"If you wish to create a copy of a table quickly to test out applying changes without the risk of modifying the current table, **`SHALLOW CLONE`** can be a good option. Shallow clones just copy the Delta transaction logs, meaning that the data doesn't move."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REPLACE TABLE purchases_shallow_clone\n",
							"SHALLOW CLONE purchases"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"In either case, data modifications applied to the cloned version of the table will be tracked and stored separately from the source. Cloning is a great way to set up tables for testing SQL code while still in development."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Summary\n",
							"\n",
							"In this notebook, we focused primarily on DDL and syntax for creating Delta Lake tables. In the next notebook, we'll explore options for writing updates to tables."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"Run the following cell to delete the tables and files associated with this lesson."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"DA.cleanup()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 4-4 - Writing to Tables')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "04 - ETL with Spark SQL"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "LakehouseTest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0342991a-9480-47bb-88db-5b1a5157cb0d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/8aa90780-bc26-461d-9db6-6df705873871/resourceGroups/domo-demo/providers/Microsoft.Synapse/workspaces/domodemo/bigDataPools/LakehouseTest",
						"name": "LakehouseTest",
						"type": "Spark",
						"endpoint": "https://domodemo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/LakehouseTest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"# Writing to Delta Tables\n",
							"Delta Lake tables provide ACID compliant updates to tables backed by data files in cloud object storage.\n",
							"\n",
							"In this notebook, we'll explore SQL syntax to process updates with Delta Lake. While many operations are standard SQL, slight variations exist to accommodate Spark and Delta Lake execution.\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lesson, you should be able to:\n",
							"- Overwrite data tables using **`INSERT OVERWRITE`**\n",
							"- Append to a table using **`INSERT INTO`**\n",
							"- Append, update, and delete from a table using **`MERGE INTO`**\n",
							"- Ingest data incrementally into tables using **`COPY INTO`**"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Run Setup\n",
							"\n",
							"The setup script will create the data and declare necessary values for the rest of this notebook to execute."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../Includes/Classroom-Setup-04.4"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"## Complete Overwrites\n",
							"\n",
							"We can use overwrites to atomically replace all of the data in a table. There are multiple benefits to overwriting tables instead of deleting and recreating tables:\n",
							"- Overwriting a table is much faster because it doesnt need to list the directory recursively or delete any files.\n",
							"- The old version of the table still exists; can easily retrieve the old data using Time Travel.\n",
							"- Its an atomic operation. Concurrent queries can still read the table while you are deleting the table.\n",
							"- Due to ACID transaction guarantees, if overwriting the table fails, the table will be in its previous state.\n",
							"\n",
							"Spark SQL provides two easy methods to accomplish complete overwrites.\n",
							"\n",
							"Some students may have noticed previous lesson on CTAS statements actually used CRAS statements (to avoid potential errors if a cell was run multiple times).\n",
							"\n",
							"**`CREATE OR REPLACE TABLE`** (CRAS) statements fully replace the contents of a table each time they execute."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REPLACE TABLE events AS\n",
							"SELECT * FROM parquet.`${da.paths.datasets}/ecommerce/raw/events-historical`"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Reviewing the table history shows a previous version of this table was replaced."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DESCRIBE HISTORY events"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"**`INSERT OVERWRITE`** provides a nearly identical outcome as above: data in the target table will be replaced by data from the query. \n",
							"\n",
							"**`INSERT OVERWRITE`**:\n",
							"\n",
							"- Can only overwrite an existing table, not create a new one like our CRAS statement\n",
							"- Can overwrite only with new records that match the current table schema -- and thus can be a \"safer\" technique for overwriting an existing table without disrupting downstream consumers\n",
							"- Can overwrite individual partitions"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"INSERT OVERWRITE sales\n",
							"SELECT * FROM parquet.`${da.paths.datasets}/ecommerce/raw/sales-historical/`"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Note that different metrics are displayed than a CRAS statement; the table history also records the operation differently."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DESCRIBE HISTORY sales"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"A primary difference here has to do with how Delta Lake enforces schema on write.\n",
							"\n",
							"Whereas a CRAS statement will allow us to completely redefine the contents of our target table, **`INSERT OVERWRITE`** will fail if we try to change our schema (unless we provide optional settings). \n",
							"\n",
							"Uncomment and run the cell below to generate an expected error message."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- INSERT OVERWRITE sales\n",
							"-- SELECT *, current_timestamp() FROM parquet.`${da.paths.datasets}/ecommerce/raw/sales-historical`"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Append Rows\n",
							"\n",
							"We can use **`INSERT INTO`** to atomically append new rows to an existing Delta table. This allows for incremental updates to existing tables, which is much more efficient than overwriting each time.\n",
							"\n",
							"Append new sale records to the **`sales`** table using **`INSERT INTO`**."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"INSERT INTO sales\n",
							"SELECT * FROM parquet.`${da.paths.datasets}/ecommerce/raw/sales-30m`"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Note that **`INSERT INTO`** does not have any built-in guarantees to prevent inserting the same records multiple times. Re-executing the above cell would write the same records to the target table, resulting in duplicate records."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Merge Updates\n",
							"\n",
							"You can upsert data from a source table, view, or DataFrame into a target Delta table using the **`MERGE`** SQL operation. Delta Lake supports inserts, updates and deletes in **`MERGE`**, and supports extended syntax beyond the SQL standards to facilitate advanced use cases.\n",
							"\n",
							"<strong><code>\n",
							"MERGE INTO target a<br/>\n",
							"USING source b<br/>\n",
							"ON {merge_condition}<br/>\n",
							"WHEN MATCHED THEN {matched_action}<br/>\n",
							"WHEN NOT MATCHED THEN {not_matched_action}<br/>\n",
							"</code></strong>\n",
							"\n",
							"We will use the **`MERGE`** operation to update historic users data with updated emails and new users."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REPLACE TEMP VIEW users_update AS \n",
							"SELECT *, current_timestamp() AS updated \n",
							"FROM parquet.`${da.paths.datasets}/ecommerce/raw/users-30m`"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"The main benefits of **`MERGE`**:\n",
							"* updates, inserts, and deletes are completed as a single transaction\n",
							"* multiple conditionals can be added in addition to matching fields\n",
							"* provides extensive options for implementing custom logic\n",
							"\n",
							"Below, we'll only update records if the current row has a **`NULL`** email and the new row does not. \n",
							"\n",
							"All unmatched records from the new batch will be inserted."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"MERGE INTO users a\n",
							"USING users_update b\n",
							"ON a.user_id = b.user_id\n",
							"WHEN MATCHED AND a.email IS NULL AND b.email IS NOT NULL THEN\n",
							"  UPDATE SET email = b.email, updated = b.updated\n",
							"WHEN NOT MATCHED THEN INSERT *"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Note that we explicitly specify the behavior of this function for both the **`MATCHED`** and **`NOT MATCHED`** conditions; the example demonstrated here is just an example of logic that can be applied, rather than indicative of all **`MERGE`** behavior."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Insert-Only Merge for Deduplication\n",
							"\n",
							"A common ETL use case is to collect logs or other every-appending datasets into a Delta table through a series of append operations. \n",
							"\n",
							"Many source systems can generate duplicate records. With merge, you can avoid inserting the duplicate records by performing an insert-only merge.\n",
							"\n",
							"This optimized command uses the same **`MERGE`** syntax but only provided a **`WHEN NOT MATCHED`** clause.\n",
							"\n",
							"Below, we use this to confirm that records with the same **`user_id`** and **`event_timestamp`** aren't already in the **`events`** table."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"MERGE INTO events a\n",
							"USING events_update b\n",
							"ON a.user_id = b.user_id AND a.event_timestamp = b.event_timestamp\n",
							"WHEN NOT MATCHED AND b.traffic_source = 'email' THEN \n",
							"  INSERT *"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"## Load Incrementally\n",
							"\n",
							"**`COPY INTO`** provides SQL engineers an idempotent option to incrementally ingest data from external systems.\n",
							"\n",
							"Note that this operation does have some expectations:\n",
							"- Data schema should be consistent\n",
							"- Duplicate records should try to be excluded or handled downstream\n",
							"\n",
							"This operation is potentially much cheaper than full table scans for data that grows predictably.\n",
							"\n",
							"While here we'll show simple execution on a static directory, the real value is in multiple executions over time picking up new files in the source automatically."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"COPY INTO sales\n",
							"FROM \"${da.paths.datasets}/ecommerce/raw/sales-30m\"\n",
							"FILEFORMAT = PARQUET"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"Run the following cell to delete the tables and files associated with this lesson."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"DA.cleanup()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 4-5L - Extract and Load Data Lab')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "04 - ETL with Spark SQL"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "28cfe6b3-f1f6-482a-9348-8eee98b65fcd"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Extract and Load Data Lab\n",
							"\n",
							"In this lab, you will extract and load raw data from JSON files into a Delta table.\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lab, you should be able to:\n",
							"- Create an external table to extract data from JSON files\n",
							"- Create an empty Delta table with a provided schema\n",
							"- Insert records from an existing table into a Delta table\n",
							"- Use a CTAS statement to create a Delta table from files"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Run Setup\n",
							"\n",
							"Run the following cell to configure variables and datasets for this lesson."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../Includes/Classroom-Setup-04.5L"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Overview of the Data\n",
							"\n",
							"We will work with a sample of raw Kafka data written as JSON files. \n",
							"\n",
							"Each file contains all records consumed during a 5-second interval, stored with the full Kafka schema as a multiple-record JSON file. \n",
							"\n",
							"The schema for the table:\n",
							"\n",
							"| field  | type | description |\n",
							"| ------ | ---- | ----------- |\n",
							"| key    | BINARY | The **`user_id`** field is used as the key; this is a unique alphanumeric field that corresponds to session/cookie information |\n",
							"| offset | LONG | This is a unique value, monotonically increasing for each partition |\n",
							"| partition | INTEGER | Our current Kafka implementation uses only 2 partitions (0 and 1) |\n",
							"| timestamp | LONG    | This timestamp is recorded as milliseconds since epoch, and represents the time at which the producer appends a record to a partition |\n",
							"| topic | STRING | While the Kafka service hosts multiple topics, only those records from the **`clickstream`** topic are included here |\n",
							"| value | BINARY | This is the full data payload (to be discussed later), sent as JSON |"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"## Extract Raw Events From JSON Files\n",
							"To load this data into Delta properly, we first need to extract the JSON data using the correct schema.\n",
							"\n",
							"Create an external table against JSON files located at the filepath provided below. Name this table **`events_json`** and declare the schema above."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"<FILL_IN> ${da.paths.datasets}/ecommerce/raw/events-kafka/"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"**NOTE**: We'll use Python to run checks occasionally throughout the lab. The following cell will return an error with a message on what needs to change if you have not followed instructions. No output from cell execution means that you have completed this step."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"assert spark.table(\"events_json\"), \"Table named `events_json` does not exist\"\n",
							"assert spark.table(\"events_json\").columns == ['key', 'offset', 'partition', 'timestamp', 'topic', 'value'], \"Please name the columns in the order provided above\"\n",
							"assert spark.table(\"events_json\").dtypes == [('key', 'binary'), ('offset', 'bigint'), ('partition', 'int'), ('timestamp', 'bigint'), ('topic', 'string'), ('value', 'binary')], \"Please make sure the column types are identical to those provided above\"\n",
							"\n",
							"total = spark.table(\"events_json\").count()\n",
							"assert total == 2252, f\"Expected 2252 records, found {total}\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Insert Raw Events Into Delta Table\n",
							"Create an empty managed Delta table named **`events_raw`** using the same schema."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"<FILL_IN>"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Run the cell below to confirm the table was created correctly."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"assert spark.table(\"events_raw\"), \"Table named `events_raw` does not exist\"\n",
							"assert spark.table(\"events_raw\").columns == ['key', 'offset', 'partition', 'timestamp', 'topic', 'value'], \"Please name the columns in the order provided above\"\n",
							"assert spark.table(\"events_raw\").dtypes == [('key', 'binary'), ('offset', 'bigint'), ('partition', 'int'), ('timestamp', 'bigint'), ('topic', 'string'), ('value', 'binary')], \"Please make sure the column types are identical to those provided above\"\n",
							"assert spark.table(\"events_raw\").count() == 0, \"The table should have 0 records\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Once the extracted data and Delta table are ready, insert the JSON records from the **`events_json`** table into the new **`events_raw`** Delta table."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"<FILL_IN>"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Manually review the table contents to ensure data was written as expected."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"<FILL_IN>"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Run the cell below to confirm the data has been loaded correctly."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"assert spark.table(\"events_raw\").count() == 2252, \"The table should have 2252 records\"\n",
							"assert set(row['timestamp'] for row in spark.table(\"events_raw\").select(\"timestamp\").limit(5).collect()) == {1593880885085, 1593880892303, 1593880889174, 1593880886106, 1593880889725}, \"Make sure you have not modified the data provided\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Create Delta Table from a Query\n",
							"In addition to new events data, let's also load a small lookup table that provides product details that we'll use later in the course.\n",
							"Use a CTAS statement to create a managed Delta table named **`item_lookup`** that extracts data from the parquet directory provided below."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"<FILL_IN> ${da.paths.datasets}/ecommerce/raw/item-lookup"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Run the cell below to confirm the lookup table has been loaded correctly."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"assert spark.table(\"item_lookup\").count() == 12, \"The table should have 12 records\"\n",
							"assert set(row['item_id'] for row in spark.table(\"item_lookup\").select(\"item_id\").limit(5).collect()) == {'M_PREM_F', 'M_PREM_K', 'M_PREM_Q', 'M_PREM_T', 'M_STAN_F'}, \"Make sure you have not modified the data provided\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"Run the following cell to delete the tables and files associated with this lesson."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"DA.cleanup()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 4-6 - Cleaning Data')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "04 - ETL with Spark SQL"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5839b34b-4bb6-418e-be60-c037d012e9ec"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"# Cleaning Data\n",
							"\n",
							"Most transformations completed with Spark SQL will be familiar to SQL-savvy developers.\n",
							"\n",
							"As we inspect and clean our data, we'll need to construct various column expressions and queries to express transformations to apply on our dataset.  \n",
							"\n",
							"Column expressions are constructed from existing columns, operators, and built-in Spark SQL functions. They can be used in **`SELECT`** statements to express transformations that create new columns from datasets. \n",
							"\n",
							"Along with **`SELECT`**, many additional query commands can be used to express transformations in Spark SQL, including **`WHERE`**, **`DISTINCT`**, **`ORDER BY`**, **`GROUP BY`**, etc.\n",
							"\n",
							"In this notebook, we'll review a few concepts that might differ from other systems you're used to, as well as calling out a few useful functions for common operations.\n",
							"\n",
							"We'll pay special attention to behaviors around **`NULL`** values, as well as formatting strings and datetime fields.\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lesson, you should be able to:\n",
							"- Summarize datasets and describe null behaviors\n",
							"- Retrieve and removing duplicates\n",
							"- Validate datasets for expected counts, missing values, and duplicate records\n",
							"- Apply common transformations to clean and transform data"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Run Setup\n",
							"\n",
							"The setup script will create the data and declare necessary values for the rest of this notebook to execute."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../Includes/Classroom-Setup-04.6"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"We'll work with new users records in **`users_dirty`** table for this lesson."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT * FROM users_dirty"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"## Inspect Data\n",
							"\n",
							"Let's start by counting values in each field of our data."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT count(user_id), count(user_first_touch_timestamp), count(email), count(updated), count(*)\n",
							"FROM users_dirty"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"Note that **`count(col)`** skips **`NULL`** values when counting specific columns or expressions.\n",
							"\n",
							"However, **`count(*)`** is a special case that counts the total number of rows (including rows that are only **`NULL`** values).\n",
							"\n",
							"To count null values, use the **`count_if`** function or **`WHERE`** clause to provide a condition that filters for records where the value **`IS NULL`**."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT\n",
							"  count_if(user_id IS NULL) AS missing_user_ids, \n",
							"  count_if(user_first_touch_timestamp IS NULL) AS missing_timestamps, \n",
							"  count_if(email IS NULL) AS missing_emails,\n",
							"  count_if(updated IS NULL) AS missing_updates\n",
							"FROM users_dirty"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Clearly there are at least a handful of null values in all of our fields. Let's try to discover what is causing this."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Distinct Records\n",
							"\n",
							"Start by looking for distinct rows."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT count(DISTINCT(*))\n",
							"FROM users_dirty"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT count(DISTINCT(user_id))\n",
							"FROM users_dirty"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Because **`user_id`** is generated alongside the **`user_first_touch_timestamp`**, these fields should always be in parity for counts."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT count(DISTINCT(user_first_touch_timestamp))\n",
							"FROM users_dirty"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Here we note that while there are some duplicate records relative to our total row count, we have a much higher number of distinct values.\n",
							"\n",
							"Let's go ahead and combine our distinct counts with columnar counts to see these values side-by-side."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT \n",
							"  count(user_id) AS total_ids,\n",
							"  count(DISTINCT user_id) AS unique_ids,\n",
							"  count(email) AS total_emails,\n",
							"  count(DISTINCT email) AS unique_emails,\n",
							"  count(updated) AS total_updates,\n",
							"  count(DISTINCT(updated)) AS unique_updates,\n",
							"  count(*) AS total_rows, \n",
							"  count(DISTINCT(*)) AS unique_non_null_rows\n",
							"FROM users_dirty"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Based on the above summary, we know:\n",
							"* All of our emails are unique\n",
							"* Our emails contain the largest number of null values\n",
							"* The **`updated`** column contains only 1 distinct value, but most are non-null"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"## Deduplicate Rows\n",
							"Based on the above behavior, what do you expect will happen if we use **`DISTINCT *`** to try to remove duplicate records?"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REPLACE TEMP VIEW users_deduped AS\n",
							"  SELECT DISTINCT(*) FROM users_dirty;\n",
							"\n",
							"SELECT * FROM users_deduped"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Note in the preview above that there appears to be null values, even though our **`COUNT(DISTINCT(*))`** ignored these nulls.\n",
							"\n",
							"How many rows do you expect passed through this **`DISTINCT`** command?"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT COUNT(*) FROM users_deduped"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Note that we now have a completely new number.\n",
							"\n",
							"Spark skips null values while counting values in a column or counting distinct values for a field, but does not omit rows with nulls from a **`DISTINCT`** query.\n",
							"\n",
							"Indeed, the reason we're seeing a new number that is 1 higher than previous counts is because we have 3 rows that are all nulls (here included as a single distinct row)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT * FROM users_dirty\n",
							"WHERE\n",
							"  user_id IS NULL AND\n",
							"  user_first_touch_timestamp IS NULL AND\n",
							"  email IS NULL AND\n",
							"  updated IS NULL"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"  \n",
							"## Deduplicate Based on Specific Columns\n",
							"\n",
							"Recall that **`user_id`** and **`user_first_touch_timestamp`** should form unique tuples, as they are both generated when a given user is first encountered.\n",
							"\n",
							"We can see that we have some null values in each of these fields; exclude nulls counting the distinct number of pairs for these fields will get us the correct count for distinct values in our table."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT COUNT(DISTINCT(user_id, user_first_touch_timestamp))\n",
							"FROM users_dirty\n",
							"WHERE user_id IS NOT NULL"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Here, we'll use these distinct pairs to remove unwanted rows from our data.\n",
							"\n",
							"The code below uses **`GROUP BY`** to remove duplicate records based on **`user_id`** and **`user_first_touch_timestamp`**.\n",
							"\n",
							"The **`max()`** aggregate function is used on the **`email`** column as a hack to capture non-null emails when multiple records are present; in this batch, all **`updated`** values were equivalent, but we need to use an aggregate function to keep this value in the result of our group by."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REPLACE TEMP VIEW deduped_users AS\n",
							"SELECT user_id, user_first_touch_timestamp, max(email) AS email, max(updated) AS updated\n",
							"FROM users_dirty\n",
							"WHERE user_id IS NOT NULL\n",
							"GROUP BY user_id, user_first_touch_timestamp;\n",
							"\n",
							"SELECT count(*) FROM deduped_users"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"## Validate Datasets\n",
							"We've visually confirmed that our counts are as expected, based our manual review.\n",
							" \n",
							"Below, we programmatically do some validation using simple filters and **`WHERE`** clauses.\n",
							"\n",
							"Validate that the **`user_id`** for each row is unique."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT max(row_count) <= 1 no_duplicate_ids FROM (\n",
							"  SELECT user_id, count(*) AS row_count\n",
							"  FROM deduped_users\n",
							"  GROUP BY user_id)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Confirm that each email is associated with at most one **`user_id`**."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT max(user_id_count) <= 1 at_most_one_id FROM (\n",
							"  SELECT email, count(user_id) AS user_id_count\n",
							"  FROM deduped_users\n",
							"  WHERE email IS NOT NULL\n",
							"  GROUP BY email)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"## Date Format and Regex\n",
							"Now that we've removed null fields and eliminated duplicates, we may wish to extract further value out of the data.\n",
							"\n",
							"The code below:\n",
							"- Correctly scales and casts the **`user_first_touch_timestamp`** to a valid timestamp\n",
							"- Extracts the calendar data and clock time for this timestamp in human readable format\n",
							"- Uses **`regexp_extract`** to extract the domains from the email column using regex"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT *,\n",
							"  date_format(first_touch, \"MMM d, yyyy\") AS first_touch_date,\n",
							"  date_format(first_touch, \"HH:mm:ss\") AS first_touch_time,\n",
							"  regexp_extract(email, \"(?<=@).+\", 0) AS email_domain\n",
							"FROM (\n",
							"  SELECT *,\n",
							"    CAST(user_first_touch_timestamp / 1e6 AS timestamp) AS first_touch \n",
							"  FROM deduped_users\n",
							")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"Run the following cell to delete the tables and files associated with this lesson."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"DA.cleanup()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 4-7 - Advanced SQL Transformations')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "04 - ETL with Spark SQL"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5c5bf0d7-8ea0-4a3b-85f7-62a7139f2b87"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Advanced SQL Transformations\n",
							"\n",
							"Querying tabular data stored in the data lakehouse with Spark SQL is easy, efficient, and fast.\n",
							"\n",
							"This gets more complicated as the data structure becomes less regular, when many tables need to be used in a single query, or when the shape of data needs to be changed dramatically. This notebook introduces a number of functions present in Spark SQL to help engineers complete even the most complicated transformations.\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lesson, you should be able to:\n",
							"- Use **`.`** and **`:`** syntax to query nested data\n",
							"- Work with JSON\n",
							"- Flatten and unpacking arrays and structs\n",
							"- Combine datasets using joins and set operators\n",
							"- Reshape data using pivot tables\n",
							"- Use higher order functions for working with arrays"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Run Setup\n",
							"\n",
							"The setup script will create the data and declare necessary values for the rest of this notebook to execute."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../Includes/Classroom-Setup-04.7"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Interacting with JSON Data\n",
							"\n",
							"The **`events_raw`** table was registered against data representing a Kafka payload.\n",
							"\n",
							"In most cases, Kafka data will be binary-encoded JSON values. We'll cast the **`key`** and **`value`** as strings below to look at these in a human-readable format."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REPLACE TEMP VIEW events_strings AS\n",
							"  SELECT string(key), string(value) \n",
							"  FROM events_raw;\n",
							"  \n",
							"SELECT * FROM events_strings"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Spark SQL has built-in functionality to directly interact with JSON data stored as strings. We can use the **`:`** syntax to traverse nested data structures."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT value:device, value:geo:city \n",
							"FROM events_strings"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Spark SQL also has the ability to parse JSON objects into struct types (a native Spark type with nested attributes).\n",
							"\n",
							"However, the **`from_json`** function requires a schema. To derive the schema of our current data, we'll start by executing a query we know will return a JSON value with no null fields."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT value \n",
							"FROM events_strings \n",
							"WHERE value:event_name = \"finalize\" \n",
							"ORDER BY key\n",
							"LIMIT 1"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Spark SQL also has a **`schema_of_json`** function to derive the JSON schema from an example. Here, we copy and paste an example JSON to the function and chain it into the **`from_json`** function to cast our **`value`** field to a struct type."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REPLACE TEMP VIEW parsed_events AS\n",
							"  SELECT from_json(value, schema_of_json('{\"device\":\"Linux\",\"ecommerce\":{\"purchase_revenue_in_usd\":1075.5,\"total_item_quantity\":1,\"unique_items\":1},\"event_name\":\"finalize\",\"event_previous_timestamp\":1593879231210816,\"event_timestamp\":1593879335779563,\"geo\":{\"city\":\"Houston\",\"state\":\"TX\"},\"items\":[{\"coupon\":\"NEWBED10\",\"item_id\":\"M_STAN_K\",\"item_name\":\"Standard King Mattress\",\"item_revenue_in_usd\":1075.5,\"price_in_usd\":1195.0,\"quantity\":1}],\"traffic_source\":\"email\",\"user_first_touch_timestamp\":1593454417513109,\"user_id\":\"UA000000106116176\"}')) AS json \n",
							"  FROM events_strings;\n",
							"  \n",
							"SELECT * FROM parsed_events"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Once a JSON string is unpacked to a struct type, Spark supports **`*`** (star) unpacking to flatten fields into columns."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REPLACE TEMP VIEW new_events_final AS\n",
							"  SELECT json.* \n",
							"  FROM parsed_events;\n",
							"  \n",
							"SELECT * FROM new_events_final"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Explore Data Structures\n",
							"\n",
							"Spark SQL has robust syntax for working with complex and nested data types.\n",
							"\n",
							"Start by looking at the fields in the **`events`** table."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DESCRIBE events"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"The **`ecommerce`** field is a struct that contains a double and 2 longs.\n",
							"\n",
							"We can interact with the subfields in this field using standard **`.`** syntax similar to how we might traverse nested data in JSON."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT ecommerce.purchase_revenue_in_usd \n",
							"FROM events\n",
							"WHERE ecommerce.purchase_revenue_in_usd IS NOT NULL"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"## Working with Arrays\n",
							"The **`items`** field in the **`events`** table is an array of structs.\n",
							"\n",
							"Spark SQL has a number of functions specifically to deal with arrays.\n",
							"\n",
							"For example, the **`size`** function provides a count of the number of elements in an array for each row.\n",
							"\n",
							"Let's use this to filter for event records with arrays containing 3 or more items."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT user_id, event_timestamp, event_name, items\n",
							"FROM events\n",
							"WHERE size(items) > 2"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"## Explode Arrays\n",
							"\n",
							"The **`explode`** function lets us put each element in an array on its own row.\n",
							"\n",
							"Let's use this to explode event records with 3 or more items into separate rows, one for each item in the array."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT user_id, event_timestamp, event_name, explode(items) AS item\n",
							"FROM events\n",
							"WHERE size(items) > 2"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"## Collect Arrays\n",
							"\n",
							"The **`collect_set`** function can collect unique values for a field, including fields within arrays.\n",
							"\n",
							"The **`flatten`** function allows multiple arrays to be combined into a single array.\n",
							"\n",
							"The **`array_distinct`** function removes duplicate elements from an array.\n",
							"\n",
							"Here, we combine these queries to create a simple table that shows the unique collection of actions and the items in a user's cart."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT user_id,\n",
							"  collect_set(event_name) AS event_history,\n",
							"  array_distinct(flatten(collect_set(items.item_id))) AS cart_history\n",
							"FROM events\n",
							"GROUP BY user_id"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"## Join Tables\n",
							"\n",
							"Spark SQL supports standard join operations (inner, outer, left, right, anti, cross, semi).\n",
							"\n",
							"Here we chain a join with a lookup table to an **`explode`** operation to grab the standard printed item name."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REPLACE VIEW sales_enriched AS\n",
							"SELECT *\n",
							"FROM (\n",
							"  SELECT *, explode(items) AS item \n",
							"  FROM sales) a\n",
							"INNER JOIN item_lookup b\n",
							"ON a.item.item_id = b.item_id;\n",
							"\n",
							"SELECT * FROM sales_enriched"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Set Operators\n",
							"Spark SQL supports **`UNION`**, **`MINUS`**, and **`INTERSECT`** set operators.\n",
							"\n",
							"**`UNION`** returns the collection of two queries. \n",
							"\n",
							"The query below returns the same results as if we inserted our **`new_events_final`** into the **`events`** table."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT * FROM events \n",
							"UNION \n",
							"SELECT * FROM new_events_final"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"**`INTERSECT`** returns all rows found in both relations."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT * FROM events \n",
							"INTERSECT \n",
							"SELECT * FROM new_events_final"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"The above query returns no results because our two datasets have no values in common.\n",
							"\n",
							"**`MINUS`** returns all the rows found in one dataset but not the other; we'll skip executing this here as our previous query demonstrates we have no values in common."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"\n",
							"## Pivot Tables\n",
							"The **`PIVOT`** clause is used for data perspective. We can get the aggregated values based on specific column values, which will be turned to multiple columns used in **`SELECT`** clause. The **`PIVOT`** clause can be specified after the table name or subquery.\n",
							"\n",
							"**`SELECT * FROM ()`**: The **`SELECT`** statement inside the parentheses is the input for this table.\n",
							"\n",
							"**`PIVOT`**: The first argument in the clause is an aggregate function and the column to be aggregated. Then, we specify the pivot column in the **`FOR`** subclause. The **`IN`** operator contains the pivot column values. \n",
							"\n",
							"Here we use **`PIVOT`** to create a new **`transactions`** table that flattens out the information contained in the **`sales`** table.\n",
							"\n",
							"This flattened data format can be useful for dashboarding, but also useful for applying machine learning algorithms for inference or prediction."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REPLACE TABLE transactions AS\n",
							"\n",
							"SELECT * FROM (\n",
							"  SELECT\n",
							"    email,\n",
							"    order_id,\n",
							"    transaction_timestamp,\n",
							"    total_item_quantity,\n",
							"    purchase_revenue_in_usd,\n",
							"    unique_items,\n",
							"    item.item_id AS item_id,\n",
							"    item.quantity AS quantity\n",
							"  FROM sales_enriched\n",
							") PIVOT (\n",
							"  sum(quantity) FOR item_id in (\n",
							"    'P_FOAM_K',\n",
							"    'M_STAN_Q',\n",
							"    'P_FOAM_S',\n",
							"    'M_PREM_Q',\n",
							"    'M_STAN_F',\n",
							"    'M_STAN_T',\n",
							"    'M_PREM_K',\n",
							"    'M_PREM_F',\n",
							"    'M_STAN_K',\n",
							"    'M_PREM_T',\n",
							"    'P_DOWN_S',\n",
							"    'P_DOWN_K'\n",
							"  )\n",
							");\n",
							"\n",
							"SELECT * FROM transactions"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Higher Order Functions\n",
							"Higher order functions in Spark SQL allow you to work directly with complex data types. When working with hierarchical data, records are frequently stored as array or map type objects. Higher-order functions allow you to transform data while preserving the original structure.\n",
							"\n",
							"Higher order functions include:\n",
							"- **`FILTER`** filters an array using the given lambda function.\n",
							"- **`EXIST`** tests whether a statement is true for one or more elements in an array. \n",
							"- **`TRANSFORM`** uses the given lambda function to transform all elements in an array.\n",
							"- **`REDUCE`** takes two lambda functions to reduce the elements of an array to a single value by merging the elements into a buffer, and the apply a finishing function on the final buffer."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Filter\n",
							"Remove items that are not king-sized from all records in our **`items`** column. We can use the **`FILTER`** function to create a new column that excludes that value from each array.\n",
							"\n",
							"**`FILTER (items, i -> i.item_id LIKE \"%K\") AS king_items`**\n",
							"\n",
							"In the statement above:\n",
							"- **`FILTER`** : the name of the higher-order function <br>\n",
							"- **`items`** : the name of our input array <br>\n",
							"- **`i`** : the name of the iterator variable. You choose this name and then use it in the lambda function. It iterates over the array, cycling each value into the function one at a time.<br>\n",
							"- **`->`** :  Indicates the start of a function <br>\n",
							"- **`i.item_id LIKE \"%K\"`** : This is the function. Each value is checked to see if it ends with the capital letter K. If it is, it gets filtered into the new column, **`king_items`**"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- filter for sales of only king sized items\n",
							"SELECT\n",
							"  order_id,\n",
							"  items,\n",
							"  FILTER (items, i -> i.item_id LIKE \"%K\") AS king_items\n",
							"FROM sales"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"You may write a filter that produces a lot of empty arrays in the created column. When that happens, it can be useful to use a **`WHERE`** clause to show only non-empty array values in the returned column. \n",
							"\n",
							"In this example, we accomplish that by using a subquery (a query within a query). They are useful for performing an operation in multiple steps. In this case, we're using it to create the named column that we will use with a **`WHERE`** clause."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REPLACE TEMP VIEW king_size_sales AS\n",
							"\n",
							"SELECT order_id, king_items\n",
							"FROM (\n",
							"  SELECT\n",
							"    order_id,\n",
							"    FILTER (items, i -> i.item_id LIKE \"%K\") AS king_items\n",
							"  FROM sales)\n",
							"WHERE size(king_items) > 0;\n",
							"  \n",
							"SELECT * FROM king_size_sales"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Transform\n",
							"Built-in functions are designed to operate on a single, simple data type within a cell; they cannot process array values. **`TRANSFORM`** can be particularly useful when you want to apply an existing function to each element in an array. \n",
							"\n",
							"Compute the total revenue from king-sized items per order.\n",
							"\n",
							"**`TRANSFORM(king_items, k -> CAST(k.item_revenue_in_usd * 100 AS INT)) AS item_revenues`**\n",
							"\n",
							"In the statement above, for each value in the input array, we extract the item's revenue value, multiply it by 100, and cast the result to integer. Note that we're using the same kind as references as in the previous command, but we name the iterator with a new variable, **`k`**."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- get total revenue from king items per order\n",
							"CREATE OR REPLACE TEMP VIEW king_item_revenues AS\n",
							"\n",
							"SELECT\n",
							"  order_id,\n",
							"  king_items,\n",
							"  TRANSFORM (\n",
							"    king_items,\n",
							"    k -> CAST(k.item_revenue_in_usd * 100 AS INT)\n",
							"  ) AS item_revenues\n",
							"FROM king_size_sales;\n",
							"\n",
							"SELECT * FROM king_item_revenues\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Summary\n",
							"Spark SQL offers a comprehensive set of native functionality for interacting with and manipulating highly nested data.\n",
							"\n",
							"While some syntax for this functionality may be unfamiliar to SQL users, leveraging built-in functions like higher order functions can prevent SQL engineers from needing to rely on custom logic when dealing with highly complex data structures."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"Run the following cell to delete the tables and files associated with this lesson."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%%pyspark\n",
							"DA.cleanup()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 4-8 - SQL UDFs and Control Flow')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "04 - ETL with Spark SQL"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "67f50faf-34e3-4394-a333-da5200fd38a1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# SQL UDFs and Control Flow\n",
							"\n",
							"Databricks added support for User Defined Functions (UDFs) registered natively in SQL starting in DBR 9.1.\n",
							"\n",
							"This feature allows users to register custom combinations of SQL logic as functions in a database, making these methods reusable anywhere SQL can be run on Databricks. These functions leverage Spark SQL directly, maintaining all of the optimizations of Spark when applying your custom logic to large datasets.\n",
							"\n",
							"In this notebook, we'll first have a simple introduction to these methods, and then explore how this logic can be combined with **`CASE`** / **`WHEN`** clauses to provide reusable custom control flow logic.\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lesson, you should be able to:\n",
							"* Define and registering SQL UDFs\n",
							"* Describe the security model used for sharing SQL UDFs\n",
							"* Use **`CASE`** / **`WHEN`** statements in SQL code\n",
							"* Leverage **`CASE`** / **`WHEN`** statements in SQL UDFs for custom control flow"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Setup\n",
							"Run the following cell to setup your environment."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../Includes/Classroom-Setup-04.8"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Create a Simple Dataset\n",
							"\n",
							"For this notebook, we'll consider the following dataset, registered here as a temporary view."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REPLACE TEMPORARY VIEW foods(food) AS VALUES\n",
							"(\"beef\"),\n",
							"(\"beans\"),\n",
							"(\"potatoes\"),\n",
							"(\"bread\");\n",
							"\n",
							"SELECT * FROM foods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## SQL UDFs\n",
							"At minimum, a SQL UDF requires a function name, optional parameters, the type to be returned, and some custom logic.\n",
							"\n",
							"Below, a simple function named **`yelling`** takes one parameter named **`text`**. It returns a string that will be in all uppercase letters with three exclamation points added to the end."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REPLACE FUNCTION yelling(text STRING)\n",
							"RETURNS STRING\n",
							"RETURN concat(upper(text), \"!!!\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Note that this function is applied to all values of the column in a parallel fashion within the Spark processing engine. SQL UDFs are an efficient way to define custom logic that is optimized for execution on Databricks."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT yelling(food) FROM foods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Scoping and Permissions of SQL UDFs\n",
							"\n",
							"Note that SQL UDFs will persist between execution environments (which can include notebooks, DBSQL queries, and jobs).\n",
							"\n",
							"We can describe the function to see where it was registered and basic information about expected inputs and what is returned."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DESCRIBE FUNCTION yelling"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"By describing extended, we can get even more information. \n",
							"\n",
							"Note that the **`Body`** field at the bottom of the function description shows the SQL logic used in the function itself."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DESCRIBE FUNCTION EXTENDED yelling"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"SQL UDFs exist as objects in the metastore and are governed by the same Table ACLs as databases, tables, or views.\n",
							"\n",
							"In order to use a SQL UDF, a user must have **`USAGE`** and **`SELECT`** permissions on the function."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## CASE/WHEN\n",
							"\n",
							"The standard SQL syntactic construct **`CASE`** / **`WHEN`** allows the evaluation of multiple conditional statements with alternative outcomes based on table contents.\n",
							"\n",
							"Again, everything is evaluated natively in Spark, and so is optimized for parallel execution."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT *,\n",
							"  CASE \n",
							"    WHEN food = \"beans\" THEN \"I love beans\"\n",
							"    WHEN food = \"potatoes\" THEN \"My favorite vegetable is potatoes\"\n",
							"    WHEN food <> \"beef\" THEN concat(\"Do you have any good recipes for \", food ,\"?\")\n",
							"    ELSE concat(\"I don't eat \", food)\n",
							"  END\n",
							"FROM foods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Simple Control Flow Functions\n",
							"\n",
							"Combining SQL UDFs with control flow in the form of **`CASE`** / **`WHEN`** clauses provides optimized execution for control flows within SQL workloads.\n",
							"\n",
							"Here, we demonstrate wrapping the previous logic in a function that will be reusable anywhere we can execute SQL."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE FUNCTION foods_i_like(food STRING)\n",
							"RETURNS STRING\n",
							"RETURN CASE \n",
							"  WHEN food = \"beans\" THEN \"I love beans\"\n",
							"  WHEN food = \"potatoes\" THEN \"My favorite vegetable is potatoes\"\n",
							"  WHEN food <> \"beef\" THEN concat(\"Do you have any good recipes for \", food ,\"?\")\n",
							"  ELSE concat(\"I don't eat \", food)\n",
							"END;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Using this method on our data provides the desired outcome."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"SELECT foods_i_like(food) FROM foods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"While the example provided here are simple string methods, these same basic principles can be used to add custom computations and logic for native execution in Spark SQL. \n",
							"\n",
							"Especially for enterprises that might be migrating users from systems with many defined procedures or custom-defined formulas, SQL UDFs can allow a handful of users to define the complex logic needed for common reporting and analytic queries."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"Run the following cell to delete the tables and files associated with this lesson."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%%pyspark\n",
							"DA.cleanup()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 4-9L - Reshaping Data Lab')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "04 - ETL with Spark SQL"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "50975d26-c26e-422b-8af7-c7ab2e1a4a90"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Reshaping Data Lab\n",
							"\n",
							"In this lab, you will create a **`clickpaths`** table that aggregates the number of times each user took a particular action in **`events`** and then join this information with the flattened view of **`transactions`** created in the previous notebook.\n",
							"\n",
							"You'll also explore a new higher order function to flag items recorded in **`sales`** based on information extracted from item names.\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lab, you should be able to:\n",
							"- Pivot and join tables to create clickpaths for each user\n",
							"- Apply higher order functions to flag types of products purchased"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Run Setup\n",
							"\n",
							"The setup script will create the data and declare necessary values for the rest of this notebook to execute."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../Includes/Classroom-Setup-04.9L"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Reshape Datasets to Create Click Paths\n",
							"This operation will join data from your **`events`** and **`transactions`** tables in order to create a record of all actions a user took on the site and what their final order looked like.\n",
							"\n",
							"The **`clickpaths`** table should contain all the fields from your **`transactions`** table, as well as a count of every **`event_name`** in its own column. Each user that completed a purchase should have a single row in the final table. Let's start by pivoting the **`events`** table to get counts for each **`event_name`**."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"### 1. Pivot **`events`** to count actions for each user\n",
							"We want to aggregate the number of times each user performed a specific event, specified in the **`event_name`** column. To do this, group by **`user_id`** and pivot on **`event_name`** to provide a count of every event type in its own column, resulting in the schema below.\n",
							"\n",
							"| field | type | \n",
							"| --- | --- | \n",
							"| user_id | STRING |\n",
							"| cart | BIGINT |\n",
							"| pillows | BIGINT |\n",
							"| login | BIGINT |\n",
							"| main | BIGINT |\n",
							"| careers | BIGINT |\n",
							"| guest | BIGINT |\n",
							"| faq | BIGINT |\n",
							"| down | BIGINT |\n",
							"| warranty | BIGINT |\n",
							"| finalize | BIGINT |\n",
							"| register | BIGINT |\n",
							"| shipping_info | BIGINT |\n",
							"| checkout | BIGINT |\n",
							"| mattresses | BIGINT |\n",
							"| add_item | BIGINT |\n",
							"| press | BIGINT |\n",
							"| email_coupon | BIGINT |\n",
							"| cc_info | BIGINT |\n",
							"| foam | BIGINT |\n",
							"| reviews | BIGINT |\n",
							"| original | BIGINT |\n",
							"| delivery | BIGINT |\n",
							"| premium | BIGINT |\n",
							"\n",
							"A list of the event names are provided below."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"CREATE OR REPLACE VIEW events_pivot\n",
							"<FILL_IN>\n",
							"(\"cart\", \"pillows\", \"login\", \"main\", \"careers\", \"guest\", \"faq\", \"down\", \"warranty\", \"finalize\", \n",
							"\"register\", \"shipping_info\", \"checkout\", \"mattresses\", \"add_item\", \"press\", \"email_coupon\", \n",
							"\"cc_info\", \"foam\", \"reviews\", \"original\", \"delivery\", \"premium\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"**NOTE**: We'll use Python to run checks occasionally throughout the lab. The helper functions below will return an error with a message on what needs to change if you have not followed instructions. No output means that you have completed this step."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%%pyspark\n",
							"def check_table_results(table_name, column_names, num_rows):\n",
							"    assert spark.table(table_name), f\"Table named **`{table_name}`** does not exist\"\n",
							"    assert spark.table(table_name).columns == column_names, \"Please name the columns in the order provided above\"\n",
							"    assert spark.table(table_name).count() == num_rows, f\"The table should have {num_rows} records\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Run the cell below to confirm the view was created correctly."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%%pyspark\n",
							"event_columns = ['user', 'cart', 'pillows', 'login', 'main', 'careers', 'guest', 'faq', 'down', 'warranty', 'finalize', 'register', 'shipping_info', 'checkout', 'mattresses', 'add_item', 'press', 'email_coupon', 'cc_info', 'foam', 'reviews', 'original', 'delivery', 'premium']\n",
							"check_table_results(\"events_pivot\", event_columns, 204586)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"### 2. Join event counts and transactions for all users\n",
							"\n",
							"Next, join **`events_pivot`** with **`transactions`** to create the table **`clickpaths`**. This table should have the same event name columns from the **`events_pivot`** table created above, followed by columns from the **`transactions`** table, as shown below.\n",
							"\n",
							"| field | type | \n",
							"| --- | --- | \n",
							"| user | STRING |\n",
							"| cart | BIGINT |\n",
							"| ... | ... |\n",
							"| user_id | STRING |\n",
							"| order_id | BIGINT |\n",
							"| transaction_timestamp | BIGINT |\n",
							"| total_item_quantity | BIGINT |\n",
							"| purchase_revenue_in_usd | DOUBLE |\n",
							"| unique_items | BIGINT |\n",
							"| P_FOAM_K | BIGINT |\n",
							"| M_STAN_Q | BIGINT |\n",
							"| P_FOAM_S | BIGINT |\n",
							"| M_PREM_Q | BIGINT |\n",
							"| M_STAN_F | BIGINT |\n",
							"| M_STAN_T | BIGINT |\n",
							"| M_PREM_K | BIGINT |\n",
							"| M_PREM_F | BIGINT |\n",
							"| M_STAN_K | BIGINT |\n",
							"| M_PREM_T | BIGINT |\n",
							"| P_DOWN_S | BIGINT |\n",
							"| P_DOWN_K | BIGINT |"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"CREATE OR REPLACE VIEW clickpaths AS\n",
							"<FILL_IN>"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Run the cell below to confirm the table was created correctly."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%%pyspark\n",
							"clickpath_columns = event_columns + ['user_id', 'order_id', 'transaction_timestamp', 'total_item_quantity', 'purchase_revenue_in_usd', 'unique_items', 'P_FOAM_K', 'M_STAN_Q', 'P_FOAM_S', 'M_PREM_Q', 'M_STAN_F', 'M_STAN_T', 'M_PREM_K', 'M_PREM_F', 'M_STAN_K', 'M_PREM_T', 'P_DOWN_S', 'P_DOWN_K']\n",
							"check_table_results(\"clickpaths\", clickpath_columns, 9085)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Flag Types of Products Purchased\n",
							"Here, you'll use the higher order function **`EXISTS`** with data from the **`sales`** table to create boolean columns **`mattress`** and **`pillow`** that indicate whether the item purchased was a mattress or pillow product.\n",
							"\n",
							"For example, if **`item_name`** from the **`items`** column ends with the string **`\"Mattress\"`**, the column value for **`mattress`** should be **`true`** and the value for **`pillow`** should be **`false`**. Here are a few examples of items and the resulting values.\n",
							"\n",
							"|  items  | mattress | pillow |\n",
							"| ------- | -------- | ------ |\n",
							"| **`[{..., \"item_id\": \"M_PREM_K\", \"item_name\": \"Premium King Mattress\", ...}]`** | true | false |\n",
							"| **`[{..., \"item_id\": \"P_FOAM_S\", \"item_name\": \"Standard Foam Pillow\", ...}]`** | false | true |\n",
							"| **`[{..., \"item_id\": \"M_STAN_F\", \"item_name\": \"Standard Full Mattress\", ...}]`** | true | false |\n",
							"\n",
							"See documentation for the <a href=\"https://docs.databricks.com/sql/language-manual/functions/exists.html\" target=\"_blank\">exists</a> function.  \n",
							"You can use the condition expression **`item_name LIKE \"%Mattress\"`** to check whether the string **`item_name`** ends with the word \"Mattress\"."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"CREATE OR REPLACE TABLE sales_product_flags AS\n",
							"<FILL_IN>\n",
							"EXISTS <FILL_IN>.item_name LIKE \"%Mattress\"\n",
							"EXISTS <FILL_IN>.item_name LIKE \"%Pillow\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Run the cell below to confirm the table was created correctly."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%%pyspark\n",
							"check_table_results(\"sales_product_flags\", ['items', 'mattress', 'pillow'], 10539)\n",
							"product_counts = spark.sql(\"SELECT sum(CAST(mattress AS INT)) num_mattress, sum(CAST(pillow AS INT)) num_pillow FROM sales_product_flags\").first().asDict()\n",
							"assert product_counts == {'num_mattress': 10015, 'num_pillow': 1386}, \"There should be 10015 rows where mattress is true, and 1386 where pillow is true\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"Run the following cell to delete the tables and files associated with this lesson."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%%pyspark\n",
							"DA.cleanup()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 5-1 - Python Basics')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "05 - OPTIONAL Python for Spark SQL"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "dca97af7-d53c-4cbe-b60a-070a7047df6f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Just Enough Python for Databricks SQL\n",
							"\n",
							"While Databricks SQL provides an ANSI-compliant flavor of SQL with many additional custom methods (including the entire Delta Lake SQL syntax), users migrating from some systems may run into missing features, especially around control flow and error handling.\n",
							"\n",
							"Databricks notebooks allow users to write SQL and Python and execute logic cell-by-cell. PySpark has extensive support for executing SQL queries, and can easily exchange data with tables and temporary views.\n",
							"\n",
							"Mastering just a handful of Python concepts will unlock powerful new design practices for engineers and analysts proficient in SQL. Rather than trying to teach the entire language, this lesson focuses on those features that can immediately be leveraged to write more extensible SQL programs on Databricks.\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lesson, you should be able to:\n",
							"* Print and manipulate multi-line Python strings\n",
							"* Define variables and functions\n",
							"* Use f-strings for variable substitution"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Strings\n",
							"Characters enclosed in single (**`'`**) or double (**`\"`**) quotes are considered strings."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"\"This is a string\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"To preview how a string will render, we can call **`print()`**."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"print(\"This is a string\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"By wrapping a string in triple quotes (**`\"\"\"`**), it's possible to use multiple lines."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"print(\"\"\"\n",
							"This \n",
							"is \n",
							"a \n",
							"multi-line \n",
							"string\n",
							"\"\"\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"This makes it easy to turn SQL queries into Python strings."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"print(\"\"\"\n",
							"SELECT *\n",
							"FROM test_table\n",
							"\"\"\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"When we execute SQL from a Python cell, we will pass a string as an argument to **`spark.sql()`**."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"SELECT 1 AS test\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"To render a query the way it would appear in a normal SQL notebook, we call **`display()`** on this function."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"display(spark.sql(\"SELECT 1 AS test\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"**NOTE**: Executing a cell with only a Python string in it will just print the string. Using **`print()`** with a string just renders it back to the notebook.\n",
							"\n",
							"To execute a string that contains SQL using Python, it must be passed within a call to **`spark.sql()`**."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Variables\n",
							"Python variables are assigned using the **`=`**.\n",
							"\n",
							"Python variable names need to start with a letter, and can only contain letters, numbers, and underscores. (Variable names starting with underscores are valid but typically reserved for special use cases.)\n",
							"\n",
							"Many Python programmers favor snake casing, which uses only lowercase letters and underscores for all variables.\n",
							"\n",
							"The cell below creates the variable **`my_string`**."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"my_string = \"This is a string\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Executing a cell with this variable will return its value."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"my_string"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"The output here is the same as if we typed **`\"This is a string\"`** into the cell and ran it.\n",
							"\n",
							"Note that the quotation marks aren't part of the string, as shown when we print it."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"print(my_string)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"This variable can be used the same way a string would be.\n",
							"\n",
							"String concatenation (joining to strings together) can be performed with a **`+`**."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"print(\"This is a new string and \" + my_string)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"We can join string variables with other string variables."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"new_string = \"This is a new string and \"\n",
							"print(new_string + my_string)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Functions\n",
							"Functions allow you to specify local variables as arguments and then apply custom logic. We define a function using the keyword **`def`** followed by the function name and, enclosed in parentheses, any variable arguments we wish to pass into the function. Finally, the function header has a **`:`** at the end.\n",
							"\n",
							"Note: In Python, indentation matters. You can see in the cell below that the logic of the function is indented in from the left margin. Any code that is indented to this level is part of the function.\n",
							"\n",
							"The function below takes one argument (**`arg`**) and then prints it."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"def print_string(arg):\n",
							"    print(arg)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"When we pass a string as the argument, it will be printed."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"print_string(\"foo\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"We can also pass a variable as an argument."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"print_string(my_string)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Oftentimes we want to return the results of our function for use elsewhere. For this we use the **`return`** keyword.\n",
							"\n",
							"The function below constructs a new string by concatenating our argument. Note that both functions and arguments can have arbitrary names, just like variables (and follow the same rules)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"def return_new_string(string_arg):\n",
							"    return \"The string passed to this function was \" + string_arg"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Running this function returns the output."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"return_new_string(\"foobar\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Assigning it to a variable captures the output for reuse elsewhere."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"function_output = return_new_string(\"foobar\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"This variable doesn't contain our function, just the results of our function (a string)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"function_output"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## F-strings\n",
							"By adding the letter **`f`** before a Python string, you can inject variables or evaluated Python code by inserting them inside curly braces (**`{}`**).\n",
							"\n",
							"Evaluate the cell below to see string variable substitution."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"f\"I can substitute {my_string} here\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"The following cell inserts the string returned by a function."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"f\"I can substitute functions like {return_new_string('foobar')} here\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Combine this with triple quotes and you can format a paragraph or list, like below."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"multi_line_string = f\"\"\"\n",
							"I can have many lines of text with variable substitution:\n",
							"  - A variable: {my_string}\n",
							"  - A function output: {return_new_string('foobar')}\n",
							"\"\"\"\n",
							"\n",
							"print(multi_line_string)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Or you could format a SQL query."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"table_name = \"users\"\n",
							"filter_clause = \"WHERE state = 'CA'\"\n",
							"\n",
							"query = f\"\"\"\n",
							"SELECT *\n",
							"FROM {table_name}\n",
							"{filter_clause}\n",
							"\"\"\"\n",
							"\n",
							"print(query)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 5-2 - Python Control Flow')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "05 - OPTIONAL Python for Spark SQL"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0291f178-831d-4b6d-b592-b841abf42361"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Just Enough Python for Databricks SQL\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lesson, you should be able to:\n",
							"* Leverage **`if`** / **`else`**\n",
							"* Describe how errors impact notebook execution\n",
							"* Write simple tests with **`assert`**\n",
							"* Use **`try`** / **`except`** to handle errors"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## if/else\n",
							"\n",
							"**`if`** / **`else`** clauses are common in many programming languages.\n",
							"\n",
							"Note that SQL has the **`CASE WHEN ... ELSE`** construct, which is similar.\n",
							"\n",
							"<strong>If you're seeking to evaluate conditions within your tables or queries, use **`CASE WHEN`**.</strong>\n",
							"\n",
							"Python control flow should be reserved for evaluating conditions outside of your query.\n",
							"\n",
							"More on this later. First, an example with **`\"beans\"`**."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"food = \"beans\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Working with **`if`** and **`else`** is all about evaluating whether or not certain conditions are true in your execution environment.\n",
							"\n",
							"Note that in Python, we have the following comparison operators:\n",
							"\n",
							"| Syntax | Operation |\n",
							"| --- | --- |\n",
							"| **`==`** | equals |\n",
							"| **`>`** | greater than |\n",
							"| **`<`** | less than |\n",
							"| **`>=`** | greater than or equal |\n",
							"| **`<=`** | less than or equal |\n",
							"| **`!=`** | not equal |\n",
							"\n",
							"If you read the sentence below out loud, you will be describing the control flow of your program."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"if food == \"beans\":\n",
							"    print(f\"I love {food}\")\n",
							"else:\n",
							"    print(f\"I don't eat {food}\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"As expected, because the variable **`food`** is the string literal **`\"beans\"`**, the **`if`** statement evaluated to **`True`** and the first print statement evaluated.\n",
							"\n",
							"Let's assign a different value to the variable."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"food = \"beef\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Now the first condition will evaluate as **`False`**. \n",
							"\n",
							"What do you think will happen when you run the following cell?"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"if food == \"beans\":\n",
							"    print(f\"I love {food}\")\n",
							"else:\n",
							"    print(f\"I don't eat {food}\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Note that each time we assign a new value to a variable, this completely erases the old variable."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"food = \"potatoes\"\n",
							"print(food)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"The Python keyword **`elif`** (short for **`else`** + **`if`**) allows us to evaluate multiple conditions.\n",
							"\n",
							"Note that conditions are evaluated from top to bottom. Once a condition evaluates to true, no further conditions will be evaluated.\n",
							"\n",
							"**`if`** / **`else`** control flow patterns:\n",
							"1. Must contain an **`if`** clause\n",
							"1. Can contain any number of **`elif`** clauses\n",
							"1. Can contain at most one **`else`** clause"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"if food == \"beans\":\n",
							"    print(f\"I love {food}\")\n",
							"elif food == \"potatoes\":\n",
							"    print(f\"My favorite vegetable is {food}\")\n",
							"elif food != \"beef\":\n",
							"    print(f\"Do you have any good recipes for {food}?\")\n",
							"else:\n",
							"    print(f\"I don't eat {food}\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"By encapsulating the above logic in a function, we can reuse this logic and formatting with arbitrary arguments rather than referencing globally-defined variables."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"def foods_i_like(food):\n",
							"    if food == \"beans\":\n",
							"        print(f\"I love {food}\")\n",
							"    elif food == \"potatoes\":\n",
							"        print(f\"My favorite vegetable is {food}\")\n",
							"    elif food != \"beef\":\n",
							"        print(f\"Do you have any good recipes for {food}?\")\n",
							"    else:\n",
							"        print(f\"I don't eat {food}\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Here, we pass the string **`\"bread\"`** to the function."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"foods_i_like(\"bread\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"As we evaluate the function, we locally assign the string **`\"bread\"`** to the **`food`** variable, and the logic behaves as expected.\n",
							"\n",
							"Note that we don't overwrite the value of the **`food`** variable as previously defined in the notebook."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"food"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## try/except\n",
							"\n",
							"While **`if`** / **`else`** clauses allow us to define conditional logic based on evaluating conditional statements, **`try`** / **`except`** focuses on providing robust error handling.\n",
							"\n",
							"Let's begin by considering a simple function."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"def three_times(number):\n",
							"    return number * 3"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Let's assume that the desired use of this function is to multiply an integer value by 3.\n",
							"\n",
							"The below cell demonstrates this behavior."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"three_times(2)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Note what happens if a string is passed to the function."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"three_times(\"2\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"In this case, we don't get an error, but we also do not get the desired outcome.\n",
							"\n",
							"**`assert`** statements allow us to run simple tests of Python code. If an **`assert`** statement evaluates to true, nothing happens. \n",
							"\n",
							"If it evaluates to false, an error is raised.\n",
							"\n",
							"Run the following cell to assert that the number **`2`** is an integer"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"assert type(2) == int"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Uncomment the following cell and then run it to assert that the string **`\"2\"`** is an integer.\n",
							"\n",
							"It should throw an **`AssertionError`**."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# assert type(\"2\") == int"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"As expected, the string **`\"2\"`** is not an integer.\n",
							"\n",
							"Python strings have a property to report whether or not they can be safely cast as numeric value as seen below."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"assert \"2\".isnumeric()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"String numbers are common; you may see them as results from an API query, raw records in a JSON or CSV file, or returned by a SQL query.\n",
							"\n",
							"**`int()`** and **`float()`** are two common methods for casting values to numeric types. \n",
							"\n",
							"An **`int`** will always be a whole number, while a **`float`** will always have a decimal."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"int(\"2\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"While Python will gladly cast a string containing numeric characters to a numeric type, it will not allow you to change other strings to numbers.\n",
							"\n",
							"Uncomment the following cell and give it a try:"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# int(\"two\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Note that errors will stop the execution of a notebook script; all cells after an error will be skipped when a notebook is scheduled as a production job.\n",
							"\n",
							"If we enclose code that might throw an error in a **`try`** statement, we can define alternate logic when an error is encountered.\n",
							"\n",
							"Below is a simple function that demonstrates this."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"def try_int(num_string):\n",
							"    try:\n",
							"        int(num_string)\n",
							"        result = f\"{num_string} is a number.\"\n",
							"    except:\n",
							"        result = f\"{num_string} is not a number!\"\n",
							"        \n",
							"    print(result)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"When a numeric string is passed, the function will return the result as an integer."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"try_int(\"2\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"When a non-numeric string is passed, an informative message is printed out.\n",
							"\n",
							"**NOTE**: An error is **not** raised, even though an error occurred, and no value was returned. Implementing logic that suppresses errors can lead to logic silently failing."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"try_int(\"two\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Below, our earlier function is updated to include logic for handling errors to return an informative message."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"def three_times(number):\n",
							"    try:\n",
							"        return int(number) * 3\n",
							"    except ValueError as e:\n",
							"        print(f\"You passed the string variable '{number}'.\\n\")\n",
							"        print(f\"Try passing an integer instead.\")\n",
							"        return None"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Now our function can process numbers passed as strings."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"three_times(\"2\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"And prints an informative message when a string is passed."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"three_times(\"two\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Note that as implemented, this logic would only be useful for interactive execution of this logic (the message isn't currently being logged anywhere, and the code will not return the data in the desired format; human intervention would be required to act upon the printed message)."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Applying Python Control Flow for SQL Queries\n",
							"\n",
							"While the above examples demonstrate the basic principles of using these designs in Python, the goal of this lesson is to learn how to apply these concepts to executing SQL logic on Databricks.\n",
							"\n",
							"Let's revisit converting a SQL cell to execute in Python.\n",
							"\n",
							"**NOTE**: The following setup script ensures an isolated execution environment."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"CREATE OR REPLACE TEMP VIEW demo_tmp_vw(id, name, value) AS VALUES\n",
							"  (1, \"Yve\", 1.0),\n",
							"  (2, \"Omar\", 2.5),\n",
							"  (3, \"Elia\", 3.3);"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Run the SQL cell below to preview the contents of this temp view."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"SELECT * FROM demo_tmp_vw"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Running SQL in a Python cell simply requires passing the string query to **`spark.sql()`**."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"query = \"SELECT * FROM demo_tmp_vw\"\n",
							"spark.sql(query)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"But recall that executing a query with **`spark.sql()`** returns the results as a DataFrame rather than displaying them; below, the code is augmented to capture the result and display it."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"query = \"SELECT * FROM demo_tmp_vw\"\n",
							"result = spark.sql(query)\n",
							"display(result)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Using a simple **`if`** clause with a function allows us to execute arbitrary SQL queries, optionally displaying the results, and always returning the resultant DataFrame."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"def simple_query_function(query, preview=True):\n",
							"    query_result = spark.sql(query)\n",
							"    if preview:\n",
							"        display(query_result)\n",
							"    return query_result"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"result = simple_query_function(query)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Below, we execute a different query and set preview to **`False`**, as the purpose of the query is to create a temp view rather than return a preview of data."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"new_query = \"CREATE OR REPLACE TEMP VIEW id_name_tmp_vw AS SELECT id, name FROM demo_tmp_vw\"\n",
							"\n",
							"simple_query_function(new_query, preview=False)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"We now have a simple extensible function that could be further parameterized depending on the needs of our organization.\n",
							"\n",
							"For example, suppose we want to protect our company from malicious SQL, like the query below."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"injection_query = \"SELECT * FROM demo_tmp_vw; DROP DATABASE prod_db CASCADE; SELECT * FROM demo_tmp_vw\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"We can use the **`find()`** method to test for multiple SQL statements by looking for a semicolon."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"injection_query.find(\";\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"If it's not found it will return **`-1`**"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"injection_query.find(\"x\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"With that knowledge, we can define a simple search for a semicolon in the query string and raise a custom error message if it was found (not **`-1`**)"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"def injection_check(query):\n",
							"    semicolon_index = query.find(\";\")\n",
							"    if semicolon_index >= 0:\n",
							"        raise ValueError(f\"Query contains semi-colon at index {semicolon_index}\\nBlocking execution to avoid SQL injection attack\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"**NOTE**: The example shown here is not sophisticated, but seeks to demonstrate a general principle. \n",
							"\n",
							"Always be wary of allowing untrusted users to pass text that will be passed to SQL queries. \n",
							"\n",
							"Also note that only one query can be executed using **`spark.sql()`**, so text with a semi-colon will always throw an error."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Uncomment the following cell and give it a try:"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# injection_check(injection_query)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"If we add this method to our earlier query function, we now have a more robust function that will assess each query for potential threats before execution."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"def secure_query_function(query, preview=True):\n",
							"    injection_check(query)\n",
							"    query_result = spark.sql(query)\n",
							"    if preview:\n",
							"        display(query_result)\n",
							"    return query_result"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"As expected, we see normal performance with a safe query."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"secure_query_function(query)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"But prevent execution when bad logic is run."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 5-3L - Python for SQL Lab')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "05 - OPTIONAL Python for Spark SQL"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "86f725c3-4911-4181-8a9d-5d655323ed38"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Just Enough Python for Databricks SQL Lab\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lab, you should be able to:\n",
							"* Review basic Python code and describe expected outcomes of code execution\n",
							"* Reason through control flow statements in Python functions\n",
							"* Add parameters to a SQL query by wrapping it in a Python function"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../Includes/Classroom-Setup-05.3L"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Reviewing Python Basics\n",
							"\n",
							"In the previous notebook, we briefly explored using **`spark.sql()`** to execute arbitrary SQL commands from Python.\n",
							"\n",
							"Look at the following 3 cells. Before executing each cell, identify:\n",
							"1. The expected output of cell execution\n",
							"1. What logic is being executed\n",
							"1. Changes to the resultant state of the environment\n",
							"\n",
							"Then execute the cells, compare the results to your expectations, and see the explanations below."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"course = \"dewd\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(f\"SELECT '{course}' AS course_name\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.sql(f\"SELECT '{course}' AS course_name\")\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"1. **Cmd 5** assigns a string to a variable. When a variable assignment is successful, no output is displayed to the notebook. A new variable is added to the current execution environment.\n",
							"1. **Cmd 6** executes a SQL query and displays the schema for the DataFrame alongside the word **`DataFrame`**. In this case, the SQL query is just to select a string, so no changes to our environment occur. \n",
							"1. **Cmd 7** executes the same SQL query and displays the output of the DataFrame. This combination of **`display()`** and **`spark.sql()`** most closely mirrors executing logic in a **`%sql`** cell; the results will always be printed in a formatted table, assuming results are returned by the query; some queries will instead manipulate tables or databases, in which case the word **`OK`** will print to show successful execution. In this case, no changes to our environment occur from running this code."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Setting Up a Development Environment\n",
							"\n",
							"Throughout this course, we use logic similar to the following cell to capture information about the user currently executing the notebook and create an isolated development database.\n",
							"\n",
							"The **`re`** library is the <a href=\"https://docs.python.org/3/library/re.html\" target=\"_blank\">standard Python library for regex</a>.\n",
							"\n",
							"Databricks SQL has a special method to capture the username of the **`current_user()`**; and the **`.first()[0]`** code is a quick hack to capture the first row of the first column of a query executed with **`spark.sql()`** (in this case, we do this safely knowing that there will only be 1 row and 1 column).\n",
							"\n",
							"All other logic below is just string formatting."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import re\n",
							"\n",
							"username = spark.sql(\"SELECT current_user()\").first()[0]\n",
							"clean_username = re.sub(\"[^a-zA-Z0-9]\", \"_\", username)\n",
							"db_name = f\"dbacademy_{clean_username}_{course}_5_3l\"\n",
							"working_dir = f\"dbfs:/user/{username}/dbacademy/{course}/5.3l\"\n",
							"\n",
							"print(f\"username:    {username}\")\n",
							"print(f\"db_name:     {db_name}\")\n",
							"print(f\"working_dir: {working_dir}\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Below, we add a simple control flow statement to this logic to create and use this user-specific database. \n",
							"\n",
							"Optionally, we will reset this database and drop all of the contents on repeat execution. (Note the the default value for the parameter **`reset`** is **`True`**)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"def create_database(course, reset=True):\n",
							"    import re\n",
							"\n",
							"    username = spark.sql(\"SELECT current_user()\").first()[0]\n",
							"    clean_username = re.sub(\"[^a-zA-Z0-9]\", \"_\", username)\n",
							"    db_name = f\"dbacademy_{clean_username}_{course}_5_3l\"\n",
							"    working_dir = f\"dbfs:/user/{username}/dbacademy/{course}/5.3l\"\n",
							"\n",
							"    print(f\"username:    {username}\")\n",
							"    print(f\"db_name:     {db_name}\")\n",
							"    print(f\"working_dir: {working_dir}\")\n",
							"\n",
							"    if reset:\n",
							"        spark.sql(f\"DROP DATABASE IF EXISTS {db_name} CASCADE\")\n",
							"        dbutils.fs.rm(working_dir, True)\n",
							"        \n",
							"    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db_name} LOCATION '{working_dir}/{db_name}.db'\")\n",
							"    spark.sql(f\"USE {db_name}\")\n",
							"    \n",
							"create_database(course)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"While this logic as defined is geared toward isolating students in shared workspaces for instructional purposes, the same basic design could be leveraged for testing new logic in an isolated environment before pushing to production."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Handling Errors Gracefully\n",
							"\n",
							"Review the logic in the function below.\n",
							"\n",
							"Note that we've just declared a new database that currently contains no tables."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"def query_or_make_demo_table(table_name):\n",
							"    try:\n",
							"        display(spark.sql(f\"SELECT * FROM {table_name}\"))\n",
							"        print(f\"Displayed results for the table {table_name}\")\n",
							"        \n",
							"    except:\n",
							"        spark.sql(f\"CREATE TABLE {table_name} (id INT, name STRING, value DOUBLE, state STRING)\")\n",
							"        spark.sql(f\"\"\"INSERT INTO {table_name}\n",
							"                      VALUES (1, \"Yve\", 1.0, \"CA\"),\n",
							"                             (2, \"Omar\", 2.5, \"NY\"),\n",
							"                             (3, \"Elia\", 3.3, \"OH\"),\n",
							"                             (4, \"Rebecca\", 4.7, \"TX\"),\n",
							"                             (5, \"Ameena\", 5.3, \"CA\"),\n",
							"                             (6, \"Ling\", 6.6, \"NY\"),\n",
							"                             (7, \"Pedro\", 7.1, \"KY\")\"\"\")\n",
							"        \n",
							"        display(spark.sql(f\"SELECT * FROM {table_name}\"))\n",
							"        print(f\"Created the table {table_name}\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Try to identify the following before executing the next cell:\n",
							"1. The expected output of cell execution\n",
							"1. What logic is being executed\n",
							"1. Changes to the resultant state of the environment"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"query_or_make_demo_table(\"demo_table\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Now answer the same three questions before running the same query below."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"query_or_make_demo_table(\"demo_table\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"- On the first execution, the table **`demo_table`** did not yet exist. As such, the attempt to return the contents of the table created an error, which resulted in our **`except`** block of logic executing. This block:\n",
							"  1. Created the table\n",
							"  1. Inserted values\n",
							"  1. Printed or displayed the contents of the table\n",
							"- On the second execution, the table **`demo_table`** already exists, and so the first query in the **`try`** block executes without error. As a result, we just display the results of the query without modifying anything in our environment."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Adapting SQL to Python\n",
							"Let's consider the following SQL query against our demo table created above."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"SELECT id, value \n",
							"FROM demo_table\n",
							"WHERE state = \"CA\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"which can also be expressed using the PySpark API and the **`display`** function as seen here:"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"results = spark.sql(\"SELECT id, value FROM demo_table WHERE state = 'CA'\")\n",
							"display(results)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Let's use this simple example to practice creating a Python function that adds optional functionality.\n",
							"\n",
							"Our target function will:\n",
							"* Be based upon a query that only includes the **`id`** and **`value`** columns from the a table named **`demo_table`**\n",
							"* Will allow filtering of that query by **`state`** where the the default behavior is to include all states\n",
							"* Will optionally render the results of the query using the **`display`** function where the default behavior is to not render\n",
							"* Will return:\n",
							"  * The query result object (a PySpark DataFrame) if **`render_results`** is False\n",
							"  * The **`None`** value  if **`render_results`** is True\n",
							"\n",
							"Stretch Goal:\n",
							"* Add an assert statement to verify that the value passed for the **`state`** parameter contains two, uppercase letters\n",
							"\n",
							"Some starter logic has been provided below:"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# TODO\n",
							"def preview_values(state=<FILL-IN>, render_results=<FILL-IN>):\n",
							"    query = <FILL-IN>\n",
							"\n",
							"    if state is not None:\n",
							"        <FILL-IN>\n",
							"\n",
							"    if render_results\n",
							"        <FILL-IN>\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"The assert statements below can be used to check whether or not your function works as intended."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import pyspark.sql.dataframe\n",
							"\n",
							"assert type(preview_values()) == pyspark.sql.dataframe.DataFrame, \"Function should return the results as a DataFrame\"\n",
							"assert preview_values().columns == [\"id\", \"value\"], \"Query should only return **`id`** and **`value`** columns\"\n",
							"\n",
							"assert preview_values(render_results=True) is None, \"Function should not return None when rendering\"\n",
							"assert preview_values(render_results=False) is not None, \"Function should return DataFrame when not rendering\"\n",
							"\n",
							"assert preview_values(state=None).count() == 7, \"Function should allow no state\"\n",
							"assert preview_values(state=\"NY\").count() == 2, \"Function should allow filtering by state\"\n",
							"assert preview_values(state=\"CA\").count() == 2, \"Function should allow filtering by state\"\n",
							"assert preview_values(state=\"OH\").count() == 1, \"Function should allow filtering by state\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 6-1 - Incremental Data Ingestion with Auto Loader')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "06 - Incremental Data Processing"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d25ee043-44c3-41b7-875b-5f8d9d40cf7d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Incremental Data Ingestion with Auto Loader\n",
							"\n",
							"Incremental ETL is important since it allows us to deal solely with new data that has been encountered since the last ingestion. Reliably processing only the new data reduces redundant processing and helps enterprises reliably scale data pipelines.\n",
							"\n",
							"The first step for any successful data lakehouse implementation is ingesting into a Delta Lake table from cloud storage. \n",
							"\n",
							"Historically, ingesting files from a data lake into a database has been a complicated process.\n",
							"\n",
							"Databricks Auto Loader provides an easy-to-use mechanism for incrementally and efficiently processing new data files as they arrive in cloud file storage. In this notebook, you'll see Auto Loader in action.\n",
							"\n",
							"![](https://files.training.databricks.com/images/autoloader-detection-modes.png)\n",
							"\n",
							"Due to the benefits and scalability that Auto Loader delivers, Databricks recommends its use as general **best practice** when ingesting data from cloud object storage.\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lesson, you should be able to:\n",
							"* Execute Auto Loader code to incrementally ingest data from cloud storage to Delta Lake\n",
							"* Describe what happens when a new file arrives in a directory configured for Auto Loader\n",
							"* Query a table fed by a streaming Auto Loader query\n",
							"\n",
							"## Dataset Used\n",
							"This demo uses simplified artificially generated medical data representing heart rate recordings delivered in the JSON format. \n",
							"\n",
							"| Field | Type |\n",
							"| --- | --- |\n",
							"| device_id | int |\n",
							"| mrn | long |\n",
							"| time | double |\n",
							"| heartrate | double |"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Getting Started\n",
							"\n",
							"Run the following cell to reset the demo and configure required variables and help functions."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../Includes/Classroom-Setup-06.1"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Using Auto Loader\n",
							"\n",
							"In the cell below, a function is defined to demonstrate using Databricks Auto Loader with the PySpark API. This code includes both a Structured Streaming read and write.\n",
							"\n",
							"The following notebook will provide a more robust overview of Structured Streaming. If you wish to learn more about Auto Loader options, refer to the <a href=\"https://docs.databricks.com/spark/latest/structured-streaming/auto-loader.html\" target=\"_blank\">documentation</a>.\n",
							"\n",
							"Note that when using Auto Loader with automatic <a href=\"https://docs.databricks.com/spark/latest/structured-streaming/auto-loader-schema.html\" target=\"_blank\">schema inference and evolution</a>, the 4 arguments shown here should allow ingestion of most datasets. These arguments are explained below.\n",
							"\n",
							"| argument | what it is | how it's used |\n",
							"| --- | --- | --- |\n",
							"| **`data_source`** | The directory of the source data | Auto Loader will detect new files as they arrive in this location and queue them for ingestion; passed to the **`.load()`** method |\n",
							"| **`source_format`** | The format of the source data |  While the format for all Auto Loader queries will be **`cloudFiles`**, the format of the source data should always be specified for the **`cloudFiles.format`** option |\n",
							"| **`table_name`** | The name of the target table | Spark Structured Streaming supports writing directly to Delta Lake tables by passing a table name as a string to the **`.table()`** method. Note that you can either append to an existing table or create a new table |\n",
							"| **`checkpoint_directory`** | The location for storing metadata about the stream | This argument is passed to the **`checkpointLocation`** and **`cloudFiles.schemaLocation`** options. Checkpoints keep track of streaming progress, while the schema location tracks updates to the fields in the source dataset |\n",
							"\n",
							"**NOTE**: The code below has been streamlined to demonstrate Auto Loader functionality. We'll see in later lessons that additional transformations can be applied to source data before saving them to Delta Lake."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"def autoload_to_table(data_source, source_format, table_name, checkpoint_directory):\n",
							"    query = (spark.readStream\n",
							"                  .format(\"cloudFiles\")\n",
							"                  .option(\"cloudFiles.format\", source_format)\n",
							"                  .option(\"cloudFiles.schemaLocation\", checkpoint_directory)\n",
							"                  .load(data_source)\n",
							"                  .writeStream\n",
							"                  .option(\"checkpointLocation\", checkpoint_directory)\n",
							"                  .option(\"mergeSchema\", \"true\")\n",
							"                  .table(table_name))\n",
							"    return query"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"In the following cell, we use the previously defined function and some path variables defined in the setup script to begin an Auto Loader stream.\n",
							"\n",
							"Here, we're reading from a source directory of JSON files."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"query = autoload_to_table(data_source = f\"{DA.paths.working_dir}/tracker\",\n",
							"                          source_format = \"json\",\n",
							"                          table_name = \"target_table\",\n",
							"                          checkpoint_directory = f\"{DA.paths.checkpoints}/target_table\")\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Because Auto Loader uses Spark Structured Streaming to load data incrementally, the code above doesn't appear to finish executing.\n",
							"\n",
							"We can think of this as a **continuously active query**. This means that as soon as new data arrives in our data source, it will be processed through our logic and loaded into our target table. We'll explore this in just a second."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Helper Function for Streaming Lessons\n",
							"\n",
							"Our notebook-based lessons combine streaming functions with batch and streaming queries against the results of those operations. These notebooks are for instructional purposes and intended for interactive, cell-by-cell execution. This pattern is not intended for production.\n",
							"\n",
							"Below, we define a helper function that prevents our notebook from executing the next cell just long enough to ensure data has been written out by a given streaming query. This code should not be necessary in a production job."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"def block_until_stream_is_ready(query, min_batches=2):\n",
							"    import time\n",
							"    while len(query.recentProgress) < min_batches:\n",
							"        time.sleep(5) # Give it a couple of seconds\n",
							"\n",
							"    print(f\"The stream has processed {len(query.recentProgress)} batchs\")\n",
							"\n",
							"block_until_stream_is_ready(query)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Query Target Table\n",
							"\n",
							"Once data has been ingested to Delta Lake with Auto Loader, users can interact with it the same way they would any table."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"SELECT * FROM target_table"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Note that the **`_rescued_data`** column is added by Auto Loader automatically to capture any data that might be malformed and not fit into the table otherwise.\n",
							"\n",
							"While Auto Loader captured the field names for our data correctly, note that it encoded all fields as **`STRING`** type. Because JSON is a text-based format, this is the safest and most permissive type, ensuring that the least amount of data is dropped or ignored at ingestion due to type mismatch."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"DESCRIBE TABLE target_table"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Use the cell below to define a temporary view that summarizes the recordings in our target table.\n",
							"\n",
							"We'll use this view below to demonstrate how new data is automatically ingested with Auto Loader."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"CREATE OR REPLACE TEMP VIEW device_counts AS\n",
							"  SELECT device_id, count(*) total_recordings\n",
							"  FROM target_table\n",
							"  GROUP BY device_id;\n",
							"  \n",
							"SELECT * FROM device_counts"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Land New Data\n",
							"\n",
							"As mentioned previously, Auto Loader is configured to incrementally process files from a directory in cloud object storage into a Delta Lake table.\n",
							"\n",
							"We have configured and are currently executing a query to process JSON files from the location specified by **`source_path`** into a table named **`target_table`**. Let's review the contents of the **`source_path`** directory."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"files = dbutils.fs.ls(f\"{DA.paths.working_dir}/tracker\")\n",
							"display(files)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"At present, you should see a single JSON file listed in this location.\n",
							"\n",
							"The method in the cell below was configured in our setup script to allow us to model an external system writing data to this directory. Each time you execute the cell below, a new file will land in the **`source_path`** directory."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.data_factory.load()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"List the contents of the **`source_path`** again using the cell below. You should see an additional JSON file for each time you ran the previous cell."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"files = dbutils.fs.ls(f\"{DA.paths.working_dir}/tracker\")\n",
							"display(files)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Tracking Ingestion Progress\n",
							"\n",
							"Historically, many systems have been configured to either reprocess all records in a source directory to calculate current results or require data engineers to implement custom logic to identify new data that's arrived since the last time a table was updated.\n",
							"\n",
							"With Auto Loader, your table has already been updated.\n",
							"\n",
							"Run the query below to confirm that new data has been ingested."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"SELECT * FROM device_counts"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"The Auto Loader query we configured earlier automatically detects and processes records from the source directory into the target table. There is a slight delay as records are ingested, but an Auto Loader query executing with default streaming configuration should update results in near real time.\n",
							"\n",
							"The query below shows the table history. A new table version should be indicated for each **`STREAMING UPDATE`**. These update events coincide with new batches of data arriving at the source."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"DESCRIBE HISTORY target_table"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Clean Up\n",
							"Feel free to continue landing new data and exploring the table results with the cells above.\n",
							"\n",
							"When you're finished, run the following cell to stop all active streams and remove created resources before continuing."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.cleanup()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 6-2 - Reasoning about Incremental Data')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "06 - Incremental Data Processing"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "30637448-e607-444e-af8f-82e24e7bf3ce"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Reasoning about Incremental Data\n",
							"\n",
							"Spark Structured Streaming extends the functionality of Apache Spark to allow for simplified configuration and bookkeeping when processing incremental datasets. In the past, much of the emphasis for streaming with big data has focused on reducing latency to provide near real time analytic insights. While Structured Streaming provides exceptional performance in achieving these goals, this lesson will focus more on the applications of incremental data processing.\n",
							"\n",
							"While incremental processing is not absolutely necessary to work successfully in the data lakehouse, our experience helping some of the world's largest companies derive insights from the world's largest datasets has led to the conclusion that many workloads can benefit substantially from an incremental processing approach. Many of the core features at the heart of Databricks have been optimized specifically to handle these ever-growing datasets.\n",
							"\n",
							"Consider the following datasets and use cases:\n",
							"* Data scientists need secure, de-identified, versioned access to frequently updated records in an operational database\n",
							"* Credit card transactions need to be compared to past customer behavior to identify and flag fraud\n",
							"* A multi-national retailer seeks to serve custom product recommendations using purchase history\n",
							"* Log files from distributed systems need to be analayzed to detect and respond to instabilities\n",
							"* Clickstream data from millions of online shoppers needs to be leveraged for A/B testing of UX\n",
							"\n",
							"The above are just a small sample of datasets that grow incrementally and infinitely over time.\n",
							"\n",
							"In this lesson, we'll explore the basics of working with Spark Structured Streaming to allow incremental processing of data. In the next lesson, we'll talk more about how this incremental processing model simplifies data processing in the data lakehouse.\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lesson, you should be able to:\n",
							"* Describe the programming model used by Spark Structured Streaming\n",
							"* Configure required options to perform a streaming read on a source\n",
							"* Describe the requirements for end-to-end fault tolerance\n",
							"* Configure required options to perform a streaming write to a sink"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Getting Started\n",
							"\n",
							"Run the following cell to configure our \"classroom.\""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../Includes/Classroom-Setup-06.2"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Treating Infinite Data as a Table\n",
							"\n",
							"The magic behind Spark Structured Streaming is that it allows users to interact with ever-growing data sources as if they were just a static table of records.\n",
							"\n",
							"<img src=\"http://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png\" width=\"800\"/>\n",
							"\n",
							"In the graphic above, a **data stream** describes any data source that grows over time. New data in a data stream might correspond to:\n",
							"* A new JSON log file landing in cloud storage\n",
							"* Updates to a database captured in a CDC feed\n",
							"* Events queued in a pub/sub messaging feed\n",
							"* A CSV file of sales closed the previous day\n",
							"\n",
							"Many organizations have traditionally taken an approach of reprocessing the entire source dataset each time they want to update their results. Another approach would be to write custom logic to only capture those files or records that have been added since the last time an update was run.\n",
							"\n",
							"Structured Streaming lets us define a query against the data source and automatically detect new records and propagate them through previously defined logic. \n",
							"\n",
							"**Spark Structured Streaming is optimized on Databricks to integrate closely with Delta Lake and Auto Loader.**"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Basic Concepts\n",
							"\n",
							"- The developer defines an **input table** by configuring a streaming read against a **source**. The syntax for doing this is similar to working with static data.\n",
							"- A **query** is defined against the input table. Both the DataFrames API and Spark SQL can be used to easily define transformations and actions against the input table.\n",
							"- This logical query on the input table generates the **results table**. The results table contains the incremental state information of the stream.\n",
							"- The **output** of a streaming pipeline will persist updates to the results table by writing to an external **sink**. Generally, a sink will be a durable system such as files or a pub/sub messaging bus.\n",
							"- New rows are appended to the input table for each **trigger interval**. These new rows are essentially analogous to micro-batch transactions and will be automatically propagated through the results table to the sink.\n",
							"\n",
							"<img src=\"http://spark.apache.org/docs/latest/img/structured-streaming-model.png\" width=\"800\"/>\n",
							"\n",
							"\n",
							"For more information, see the analogous section in the <a href=\"http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#basic-concepts\" target=\"_blank\">Structured Streaming Programming Guide</a> (from which several images have been borrowed)."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## End-to-end Fault Tolerance\n",
							"\n",
							"Structured Streaming ensures end-to-end exactly-once fault-tolerance guarantees through _checkpointing_ (discussed below) and <a href=\"https://en.wikipedia.org/wiki/Write-ahead_logging\" target=\"_blank\">Write Ahead Logs</a>.\n",
							"\n",
							"Structured Streaming sources, sinks, and the underlying execution engine work together to track the progress of stream processing. If a failure occurs, the streaming engine attempts to restart and/or reprocess the data.\n",
							"For best practices on recovering from a failed streaming query see <a href=\"https://docs.databricks.com/spark/latest/structured-streaming/production.html#recover-from-query-failures\" target=\"_blank\">docs</a>.\n",
							"\n",
							"This approach _only_ works if the streaming source is replayable; replayable sources include cloud-based object storage and pub/sub messaging services.\n",
							"\n",
							"At a high level, the underlying streaming mechanism relies on a couple of approaches:\n",
							"\n",
							"* First, Structured Streaming uses checkpointing and write-ahead logs to record the offset range of data being processed during each trigger interval.\n",
							"* Next, the streaming sinks are designed to be _idempotent_ - that is, multiple writes of the same data (as identified by the offset) do _not_ result in duplicates being written to the sink.\n",
							"\n",
							"Taken together, replayable data sources and idempotent sinks allow Structured Streaming to ensure **end-to-end, exactly-once semantics** under any failure condition."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Reading a Stream\n",
							"\n",
							"The **`spark.readStream()`** method returns a **`DataStreamReader`** used to configure and query the stream.\n",
							"\n",
							"In the previous lesson, we saw code configured for incrementally reading with Auto Loader. Here, we'll show how easy it is to incrementally read a Delta Lake table.\n",
							"\n",
							"The code uses the PySpark API to incrementally read a Delta Lake table named **`bronze`** and register a streaming temp view named **`streaming_tmp_vw`**.\n",
							"\n",
							"**NOTE**: A number of optional configurations (not shown here) can be set when configuring incremental reads, the most important of which allows you to <a href=\"https://docs.databricks.com/delta/delta-streaming.html#limit-input-rate\" target=\"_blank\">limit the input rate</a>."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"(spark.readStream\n",
							"    .table(\"bronze\")\n",
							"    .createOrReplaceTempView(\"streaming_tmp_vw\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"When we execute a query on a streaming temporary view, we'll continue to update the results of the query as new data arrives in the source.\n",
							"\n",
							"Think of a query executed against a streaming temp view as an **always-on incremental query**.\n",
							"\n",
							"**NOTE**: Generally speaking, unless a human is actively monitoring the output of a query during development or live dashboarding, we won't return streaming results to a notebook."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"SELECT * FROM streaming_tmp_vw"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"You will recognize the data as being the same as the Delta table written out in our previous lesson.\n",
							"\n",
							"Before continuing, click **`Stop Execution`** at the top of the notebook, **`Cancel`** immediately under the cell, or run the following cell to stop all active streaming queries."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"for s in spark.streams.active:\n",
							"    print(\"Stopping \" + s.id)\n",
							"    s.stop()\n",
							"    s.awaitTermination()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Working with Streaming Data\n",
							"We can execute most transformation against streaming temp views the same way we would with static data. Here, we'll run a simple aggregation to get counts of records for each **`device_id`**.\n",
							"\n",
							"Because we are querying a streaming temp view, this becomes a streaming query that executes indefinitely, rather than completing after retrieving a single set of results. For streaming queries like this, Databricks Notebooks include interactive dashboards that allow users to monitor streaming performance. Explore this below.\n",
							"\n",
							"One important note regarding this example: this is merely displaying an aggregation of input as seen by the stream. **None of these records are being persisted anywhere at this point.**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"SELECT device_id, count(device_id) AS total_recordings\n",
							"FROM streaming_tmp_vw\n",
							"GROUP BY device_id"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Before continuing, click **`Stop Execution`** at the top of the notebook, **`Cancel`** immediately under the cell, or run the following cell to stop all active streaming queries."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"for s in spark.streams.active:\n",
							"    print(\"Stopping \" + s.id)\n",
							"    s.stop()\n",
							"    s.awaitTermination()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Unsupported Operations\n",
							"\n",
							"Most operations on a streaming DataFrame are identical to a static DataFrame. There are <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#unsupported-operations\" target=\"_blank\">some exceptions to this</a>.\n",
							"\n",
							"Consider the model of the data as a constantly appending table. Sorting is one of a handful of operations that is either too complex or logically not possible to do when working with streaming data.\n",
							"\n",
							"A full discussion of these exceptions is out of scope for this course. Note that advanced streaming methods like windowing and watermarking can be used to add additional functionality to incremental workloads.\n",
							"\n",
							"Uncomment and run the following cell how this failure may appear:"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# %sql\n",
							"# SELECT * \n",
							"# FROM streaming_tmp_vw\n",
							"# ORDER BY time"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Persisting Streaming Results\n",
							"\n",
							"In order to persist incremental results, we need to pass our logic back to the PySpark Structured Streaming DataFrames API.\n",
							"\n",
							"Above, we created a temp view from a PySpark streaming DataFrame. If we create another temp view from the results of a query against a streaming temp view, we'll again have a streaming temp view."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"CREATE OR REPLACE TEMP VIEW device_counts_tmp_vw AS (\n",
							"  SELECT device_id, COUNT(device_id) AS total_recordings\n",
							"  FROM streaming_tmp_vw\n",
							"  GROUP BY device_id\n",
							")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Writing a Stream\n",
							"\n",
							"To persist the results of a streaming query, we must write them out to durable storage. The **`DataFrame.writeStream`** method returns a **`DataStreamWriter`** used to configure the output.\n",
							"\n",
							"When writing to Delta Lake tables, we typically will only need to worry about 3 settings, discussed here.\n",
							"\n",
							"### Checkpointing\n",
							"\n",
							"Databricks creates checkpoints by storing the current state of your streaming job to cloud storage.\n",
							"\n",
							"Checkpointing combines with write ahead logs to allow a terminated stream to be restarted and continue from where it left off.\n",
							"\n",
							"Checkpoints cannot be shared between separate streams. A checkpoint is required for every streaming write to ensure processing guarantees.\n",
							"\n",
							"### Output Modes\n",
							"\n",
							"Streaming jobs have output modes similar to static/batch workloads. <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes\" target=\"_blank\">More details here</a>.\n",
							"\n",
							"| Mode   | Example | Notes |\n",
							"| ------------- | ----------- | --- |\n",
							"| **Append** | **`.outputMode(\"append\")`**     | **This is the default.** Only newly appended rows are incrementally appended to the target table with each batch |\n",
							"| **Complete** | **`.outputMode(\"complete\")`** | The Results Table is recalculated each time a write is triggered; the target table is overwritten with each batch |\n",
							"\n",
							"\n",
							"### Trigger Intervals\n",
							"\n",
							"When defining a streaming write, the **`trigger`** method specifies when the system should process the next set of data..\n",
							"\n",
							"\n",
							"| Trigger Type                           | Example | Behavior |\n",
							"|----------------------------------------|----------|----------|\n",
							"| Unspecified                 |  | **This is the default.** This is equivalent to using **`processingTime=\"500ms\"`** |\n",
							"| Fixed interval micro-batches      | **`.trigger(processingTime=\"2 minutes\")`** | The query will be executed in micro-batches and kicked off at the user-specified intervals |\n",
							"| Triggered micro-batch               | **`.trigger(once=True)`** | The query will execute a single micro-batch to process all the available data and then stop on its own |\n",
							"| Triggered micro-batches       | **`.trigger(availableNow=True)`** | The query will execute multiple micro-batches to process all the available data and then stop on its own |\n",
							"\n",
							"Triggers are specified when defining how data will be written to a sink and control the frequency of micro-batches. By default, Spark will automatically detect and process all data in the source that has been added since the last trigger.\n",
							"\n",
							"**NOTE:** **`Trigger.AvailableNow`**</a> is a new trigger type that is available in DBR 10.1 for Scala only and available in DBR 10.2 and above for Python and Scala."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Pulling It All Together\n",
							"\n",
							"The code below demonstrates using **`spark.table()`** to load data from a streaming temp view back to a DataFrame. Note that Spark will always load streaming views as a streaming DataFrame and static views as static DataFrames (meaning that incremental processing must be defined with read logic to support incremental writing).\n",
							"\n",
							"In this first query, we'll demonstrate using **`trigger(availableNow=True)`** to perform incremental batch processing."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"(spark.table(\"device_counts_tmp_vw\")                               \n",
							"    .writeStream                                                \n",
							"    .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/silver\")\n",
							"    .outputMode(\"complete\")\n",
							"    .trigger(availableNow=True)\n",
							"    .table(\"device_counts\")\n",
							"    .awaitTermination() # This optional method blocks execution of the next cell until the incremental batch write has succeeded\n",
							")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Below, we change our trigger method to change this query from a triggered incremental batch to an always-on query triggered every 4 seconds.\n",
							"\n",
							"**NOTE**: As we start this query, no new records exist in our source table. We'll add new data shortly."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"query = (spark.table(\"device_counts_tmp_vw\")                               \n",
							"              .writeStream                                                \n",
							"              .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/silver\")\n",
							"              .outputMode(\"complete\")\n",
							"              .trigger(processingTime='4 seconds')\n",
							"              .table(\"device_counts\"))\n",
							"\n",
							"# Like before, wait until our stream has processed some data\n",
							"DA.block_until_stream_is_ready(query)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Querying the Output\n",
							"Now let's query the output we've written from SQL. Because the result is a table, we only need to deserialize the data to return the results.\n",
							"\n",
							"Because we are now querying a table (not a streaming DataFrame), the following will **not** be a streaming query."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"SELECT *\n",
							"FROM device_counts"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Land New Data\n",
							"\n",
							"As in our previous lesson, we have configured a helper function to process new records into our source table.\n",
							"\n",
							"Run the cell below to land another batch of data."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.data_factory.load()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Query the target table again to see the updated counts for each **`device_id`**."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"SELECT *\n",
							"FROM device_counts"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Clean Up\n",
							"Feel free to continue landing new data and exploring the table results with the cells above.\n",
							"\n",
							"When you're finished, run the following cell to stop all active streams and remove created resources before continuing."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.cleanup()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 6-3L - Using Auto Loader and Structured Streaming with Spark SQL Lab')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "06 - Incremental Data Processing"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "4adc6ef5-4a3c-4563-828d-1cd6688b40fe"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Using Auto Loader and Structured Streaming with Spark SQL\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lab, you should be able to:\n",
							"* Ingest data using Auto Loader\n",
							"* Aggregate streaming data\n",
							"* Stream data to a Delta table"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Setup\n",
							"Run the following script to setup necessary variables and clear out past runs of this notebook. Note that re-executing this cell will allow you to start the lab over."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../Includes/Classroom-Setup-06.3L"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Configure Streaming Read\n",
							"\n",
							"This lab uses a collection of customer-related CSV data from the **retail-org/customers** dataset.\n",
							"\n",
							"Read this data using <a href=\"https://docs.databricks.com/spark/latest/structured-streaming/auto-loader.html\" target=\"_blank\">Auto Loader</a> using its schema inference (use **`customers_checkpoint_path`** to store the schema info). Create a streaming temporary view called **`customers_raw_temp`**."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# TODO\n",
							"dataset_source = f\"{DA.paths.datasets}/retail-org/customers/\"\n",
							"customers_checkpoint_path = f\"{DA.paths.checkpoints}/customers\"\n",
							"\n",
							"(spark\n",
							"  .readStream\n",
							"  <FILL-IN>\n",
							"  .load(dataset_source)\n",
							"  .createOrReplaceTempView(\"customers_raw_temp\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import Row\n",
							"assert Row(tableName=\"customers_raw_temp\", isTemporary=True) in spark.sql(\"show tables\").select(\"tableName\", \"isTemporary\").collect(), \"Table not present or not temporary\"\n",
							"assert spark.table(\"customers_raw_temp\").dtypes ==  [('customer_id', 'string'),\n",
							" ('tax_id', 'string'),\n",
							" ('tax_code', 'string'),\n",
							" ('customer_name', 'string'),\n",
							" ('state', 'string'),\n",
							" ('city', 'string'),\n",
							" ('postcode', 'string'),\n",
							" ('street', 'string'),\n",
							" ('number', 'string'),\n",
							" ('unit', 'string'),\n",
							" ('region', 'string'),\n",
							" ('district', 'string'),\n",
							" ('lon', 'string'),\n",
							" ('lat', 'string'),\n",
							" ('ship_to_address', 'string'),\n",
							" ('valid_from', 'string'),\n",
							" ('valid_to', 'string'),\n",
							" ('units_purchased', 'string'),\n",
							" ('loyalty_segment', 'string'),\n",
							" ('_rescued_data', 'string')], \"Incorrect Schema\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Define a streaming aggregation\n",
							"\n",
							"Using CTAS syntax, define a new streaming view called **`customer_count_by_state_temp`** that counts the number of customers per **`state`**, in a field called **`customer_count`**."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"-- TODO\n",
							"\n",
							"CREATE OR REPLACE TEMPORARY VIEW customer_count_by_state_temp AS\n",
							"SELECT\n",
							"  <FILL-IN>"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"assert Row(tableName=\"customer_count_by_state_temp\", isTemporary=True) in spark.sql(\"show tables\").select(\"tableName\", \"isTemporary\").collect(), \"Table not present or not temporary\"\n",
							"assert spark.table(\"customer_count_by_state_temp\").dtypes == [('state', 'string'), ('customer_count', 'bigint')], \"Incorrect Schema\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Write aggregated data to a Delta table\n",
							"\n",
							"Stream data from the **`customer_count_by_state_temp`** view to a Delta table called **`customer_count_by_state`**."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# TODO\n",
							"customers_count_checkpoint_path = f\"{DA.paths.checkpoints}/customers_count\"\n",
							"\n",
							"query = (spark\n",
							"  <FILL-IN>"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA.block_until_stream_is_ready(query)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"assert Row(tableName=\"customer_count_by_state\", isTemporary=False) in spark.sql(\"show tables\").select(\"tableName\", \"isTemporary\").collect(), \"Table not present or not temporary\"\n",
							"assert spark.table(\"customer_count_by_state\").dtypes == [('state', 'string'), ('customer_count', 'bigint')], \"Incorrect Schema\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Query the results\n",
							"\n",
							"Query the **`customer_count_by_state`** table (this will not be a streaming query). Plot the results as a bar graph and also using the map plot."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"-- TODO"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Wrapping Up\n",
							"\n",
							"Run the following cell to remove the database and all data associated with this lab."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.cleanup()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"By completing this lab, you should now feel comfortable:\n",
							"* Using PySpark to configure Auto Loader for incremental data ingestion\n",
							"* Using Spark SQL to aggregate streaming data\n",
							"* Streaming data to a Delta table"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 7-1 - Incremental Multi-Hop in the Lakehouse')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "07 - Multi-Hop Architecture"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "445413c5-78f6-47ca-9f8a-0dd2fd377c8a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Incremental Multi-Hop in the Lakehouse\n",
							"\n",
							"Now that we have a better understanding of how to work with incremental data processing by combining Structured Streaming APIs and Spark SQL, we can explore the tight integration between Structured Streaming and Delta Lake.\n",
							"\n",
							"\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lesson, you should be able to:\n",
							"* Describe Bronze, Silver, and Gold tables\n",
							"* Create a Delta Lake multi-hop pipeline"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Incremental Updates in the Lakehouse\n",
							"\n",
							"Delta Lake allows users to easily combine streaming and batch workloads in a unified multi-hop pipeline. Each stage of the pipeline represents a state of our data valuable to driving core use cases within the business. Because all data and metadata lives in object storage in the cloud, multiple users and applications can access data in near-real time, allowing analysts to access the freshest data as it's being processed.\n",
							"\n",
							"![](https://files.training.databricks.com/images/sslh/multi-hop-simple.png)\n",
							"\n",
							"- **Bronze** tables contain raw data ingested from various sources (JSON files, RDBMS data,  IoT data, to name a few examples).\n",
							"\n",
							"- **Silver** tables provide a more refined view of our data. We can join fields from various bronze tables to enrich streaming records, or update account statuses based on recent activity.\n",
							"\n",
							"- **Gold** tables provide business level aggregates often used for reporting and dashboarding. This would include aggregations such as daily active website users, weekly sales per store, or gross revenue per quarter by department. \n",
							"\n",
							"The end outputs are actionable insights, dashboards and reports of business metrics.\n",
							"\n",
							"By considering our business logic at all steps of the ETL pipeline, we can ensure that storage and compute costs are optimized by reducing unnecessary duplication of data and limiting ad hoc querying against full historic data.\n",
							"\n",
							"Each stage can be configured as a batch or streaming job, and ACID transactions ensure that we succeed or fail completely."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Datasets Used\n",
							"\n",
							"This demo uses simplified artificially generated medical data. The schema of our two datasets is represented below. Note that we will be manipulating these schema during various steps.\n",
							"\n",
							"#### Recordings\n",
							"The main dataset uses heart rate recordings from medical devices delivered in the JSON format. \n",
							"\n",
							"| Field | Type |\n",
							"| --- | --- |\n",
							"| device_id | int |\n",
							"| mrn | long |\n",
							"| time | double |\n",
							"| heartrate | double |\n",
							"\n",
							"#### PII\n",
							"These data will later be joined with a static table of patient information stored in an external system to identify patients by name.\n",
							"\n",
							"| Field | Type |\n",
							"| --- | --- |\n",
							"| mrn | long |\n",
							"| name | string |"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Getting Started\n",
							"\n",
							"Run the following cell to configure the lab environment."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../Includes/Classroom-Setup-07.1"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Data Simulator\n",
							"Databricks Auto Loader can automatically process files as they land in your cloud object stores. \n",
							"\n",
							"To simulate this process, you will be asked to run the following operation several times throughout the course."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.data_factory.load()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Bronze Table: Ingesting Raw JSON Recordings\n",
							"\n",
							"Below, we configure a read on a raw JSON source using Auto Loader with schema inference.\n",
							"\n",
							"Note that while you need to use the Spark DataFrame API to set up an incremental read, once configured you can immediately register a temp view to leverage Spark SQL for streaming transformations on your data.\n",
							"\n",
							"**NOTE**: For a JSON data source, Auto Loader will default to inferring each column as a string. Here, we demonstrate specifying the data type for the **`time`** column using the **`cloudFiles.schemaHints`** option. Note that specifying improper types for a field will result in null values."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"(spark.readStream\n",
							"    .format(\"cloudFiles\")\n",
							"    .option(\"cloudFiles.format\", \"json\")\n",
							"    .option(\"cloudFiles.schemaHints\", \"time DOUBLE\")\n",
							"    .option(\"cloudFiles.schemaLocation\", f\"{DA.paths.checkpoints}/bronze\")\n",
							"    .load(DA.paths.data_landing_location)\n",
							"    .createOrReplaceTempView(\"recordings_raw_temp\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Here, we'll enrich our raw data with additional metadata describing the source file and the time it was ingested. This additional metadata can be ignored during downstream processing while providing useful information for troubleshooting errors if corrupt data is encountered."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"CREATE OR REPLACE TEMPORARY VIEW recordings_bronze_temp AS (\n",
							"  SELECT *, current_timestamp() receipt_time, input_file_name() source_file\n",
							"  FROM recordings_raw_temp\n",
							")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"The code below passes our enriched raw data back to PySpark API to process an incremental write to a Delta Lake table."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"(spark.table(\"recordings_bronze_temp\")\n",
							"      .writeStream\n",
							"      .format(\"delta\")\n",
							"      .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/bronze\")\n",
							"      .outputMode(\"append\")\n",
							"      .table(\"bronze\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Trigger another file arrival with the following cell and you'll see the changes immediately detected by the streaming query you've written."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.data_factory.load()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"### Load Static Lookup Table\n",
							"The ACID guarantees that Delta Lake brings to your data are managed at the table level, ensuring that only fully successfully commits are reflected in your tables. If you choose to merge these data with other data sources, be aware of how those sources version data and what sort of consistency guarantees they have.\n",
							"\n",
							"In this simplified demo, we are loading a static CSV file to add patient data to our recordings. In production, we could use Databricks' <a href=\"https://docs.databricks.com/spark/latest/structured-streaming/auto-loader.html\" target=\"_blank\">Auto Loader</a> feature to keep an up-to-date view of these data in our Delta Lake."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"(spark.read\n",
							"      .format(\"csv\")\n",
							"      .schema(\"mrn STRING, name STRING\")\n",
							"      .option(\"header\", True)\n",
							"      .load(f\"{DA.paths.datasets}/healthcare/patient/patient_info.csv\")\n",
							"      .createOrReplaceTempView(\"pii\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"SELECT * FROM pii"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Silver Table: Enriched Recording Data\n",
							"As a second hop in our silver level, we will do the follow enrichments and checks:\n",
							"- Our recordings data will be joined with the PII to add patient names\n",
							"- The time for our recordings will be parsed to the format **`'yyyy-MM-dd HH:mm:ss'`** to be human-readable\n",
							"- We will exclude heart rates that are <= 0, as we know that these either represent the absence of the patient or an error in transmission"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"(spark.readStream\n",
							"  .table(\"bronze\")\n",
							"  .createOrReplaceTempView(\"bronze_tmp\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"CREATE OR REPLACE TEMPORARY VIEW recordings_w_pii AS (\n",
							"  SELECT device_id, a.mrn, b.name, cast(from_unixtime(time, 'yyyy-MM-dd HH:mm:ss') AS timestamp) time, heartrate\n",
							"  FROM bronze_tmp a\n",
							"  INNER JOIN pii b\n",
							"  ON a.mrn = b.mrn\n",
							"  WHERE heartrate > 0)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"(spark.table(\"recordings_w_pii\")\n",
							"      .writeStream\n",
							"      .format(\"delta\")\n",
							"      .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/recordings_enriched\")\n",
							"      .outputMode(\"append\")\n",
							"      .table(\"recordings_enriched\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Trigger another new file and wait for it propagate through both previous queries."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"SELECT COUNT(*) FROM recordings_enriched"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA.data_factory.load()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Gold Table: Daily Averages\n",
							"\n",
							"Here we read a stream of data from **`recordings_enriched`** and write another stream to create an aggregate gold table of daily averages for each patient."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"(spark.readStream\n",
							"  .table(\"recordings_enriched\")\n",
							"  .createOrReplaceTempView(\"recordings_enriched_temp\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"CREATE OR REPLACE TEMP VIEW patient_avg AS (\n",
							"  SELECT mrn, name, mean(heartrate) avg_heartrate, date_trunc(\"DD\", time) date\n",
							"  FROM recordings_enriched_temp\n",
							"  GROUP BY mrn, name, date_trunc(\"DD\", time))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Note that we're using **`.trigger(availableNow=True)`** below. This provides us the ability to continue to use the strengths of Structured Streaming while triggering this job one-time to process all available data in micro-batches. To recap, these strengths include:\n",
							"- exactly once end-to-end fault tolerant processing\n",
							"- automatic detection of changes in upstream data sources\n",
							"\n",
							"If we know the approximate rate at which our data grows, we can appropriately size the cluster we schedule for this job to ensure fast, cost-effective processing. The customer will be able to evaluate how much updating this final aggregate view of their data costs and make informed decisions about how frequently this operation needs to be run.\n",
							"\n",
							"Downstream processes subscribing to this table do not need to re-run any expensive aggregations. Rather, files just need to be de-serialized and then queries based on included fields can quickly be pushed down against this already-aggregated source."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"(spark.table(\"patient_avg\")\n",
							"      .writeStream\n",
							"      .format(\"delta\")\n",
							"      .outputMode(\"complete\")\n",
							"      .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/daily_avg\")\n",
							"      .trigger(availableNow=True)\n",
							"      .table(\"daily_patient_avg\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"#### Important Considerations for complete Output with Delta\n",
							"\n",
							"When using **`complete`** output mode, we rewrite the entire state of our table each time our logic runs. While this is ideal for calculating aggregates, we **cannot** read a stream from this directory, as Structured Streaming assumes data is only being appended in the upstream logic.\n",
							"\n",
							"**NOTE**: Certain options can be set to change this behavior, but have other limitations attached. For more details, refer to <a href=\"https://docs.databricks.com/delta/delta-streaming.html#ignoring-updates-and-deletes\" target=\"_blank\">Delta Streaming: Ignoring Updates and Deletes</a>.\n",
							"\n",
							"The gold Delta table we have just registered will perform a static read of the current state of the data each time we run the following query."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"SELECT * FROM daily_patient_avg"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Note the above table includes all days for all users. If the predicates for our ad hoc queries match the data encoded here, we can push down our predicates to files at the source and very quickly generate more limited aggregate views."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"SELECT * \n",
							"FROM daily_patient_avg\n",
							"WHERE date BETWEEN \"2020-01-17\" AND \"2020-01-31\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Process Remaining Records\n",
							"The following cell will land additional files for the rest of 2020 in your source directory. You'll be able to see these process through the first 3 tables in your Delta Lake, but will need to re-run your final query to update your **`daily_patient_avg`** table, since this query uses the trigger available now syntax."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.data_factory.load(continuous=True)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Wrapping Up\n",
							"\n",
							"Finally, make sure all streams are stopped."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.cleanup()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Summary\n",
							"\n",
							"Delta Lake and Structured Streaming combine to provide near real-time analytic access to data in the lakehouse."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Additional Topics & Resources\n",
							"\n",
							"* <a href=\"https://docs.databricks.com/delta/delta-streaming.html\" target=\"_blank\">Table Streaming Reads and Writes</a>\n",
							"* <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\" target=\"_blank\">Structured Streaming Programming Guide</a>\n",
							"* <a href=\"https://www.youtube.com/watch?v=rl8dIzTpxrI\" target=\"_blank\">A Deep Dive into Structured Streaming</a> by Tathagata Das. This is an excellent video describing how Structured Streaming works.\n",
							"* <a href=\"https://databricks.com/glossary/lambda-architecture\" target=\"_blank\">Lambda Architecture</a>\n",
							"* <a href=\"https://bennyaustin.wordpress.com/2010/05/02/kimball-and-inmon-dw-models/#\" target=\"_blank\">Data Warehouse Models</a>\n",
							"* <a href=\"http://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html\" target=\"_blank\">Create a Kafka Source Stream</a>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 7-2L - Propagating Incremental Updates with Structured Streaming and Delta Lake Lab')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "07 - Multi-Hop Architecture"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ebdc2755-9332-4849-9e46-c15284e69282"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Propagating Incremental Updates with Structured Streaming and Delta Lake\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lab, you should be able to:\n",
							"* Apply your knowledge of structured streaming and Auto Loader to implement a simple multi-hop architecture"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Setup\n",
							"Run the following script to setup necessary variables and clear out past runs of this notebook. Note that re-executing this cell will allow you to start the lab over."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../Includes/Classroom-Setup-07.2L"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Ingest data\n",
							"\n",
							"This lab uses a collection of customer-related CSV data from the **retail-org/customers** dataset.\n",
							"\n",
							"Read this data using Auto Loader using its schema inference (use **`customers_checkpoint_path`** to store the schema info). Stream the raw data to a Delta table called **`bronze`**."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# TODO\n",
							"customers_checkpoint_path = f\"{DA.paths.checkpoints}/customers\"\n",
							"dataset_source = f\"{DA.paths.datasets}/retail-org/customers/\"\n",
							"\n",
							"query = (spark\n",
							"  .readStream\n",
							"  <FILL-IN>\n",
							"  .load(dataset_source)\n",
							"  .writeStream\n",
							"  <FILL-IN>\n",
							"  .table(\"bronze\")\n",
							")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA.block_until_stream_is_ready(query)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"Run the cell below to check your work."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"assert spark.table(\"bronze\"), \"Table named `bronze` does not exist\"\n",
							"assert spark.sql(f\"SHOW TABLES\").filter(f\"tableName == 'bronze'\").first()[\"isTemporary\"] == False, \"Table is temporary\"\n",
							"assert spark.table(\"bronze\").dtypes ==  [('customer_id', 'string'), ('tax_id', 'string'), ('tax_code', 'string'), ('customer_name', 'string'), ('state', 'string'), ('city', 'string'), ('postcode', 'string'), ('street', 'string'), ('number', 'string'), ('unit', 'string'), ('region', 'string'), ('district', 'string'), ('lon', 'string'), ('lat', 'string'), ('ship_to_address', 'string'), ('valid_from', 'string'), ('valid_to', 'string'), ('units_purchased', 'string'), ('loyalty_segment', 'string'), ('_rescued_data', 'string')], \"Incorrect Schema\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Let's create a streaming temporary view into the bronze table, so that we can perform transforms using SQL."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"(spark\n",
							"  .readStream\n",
							"  .table(\"bronze\")\n",
							"  .createOrReplaceTempView(\"bronze_temp\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Clean and enhance data\n",
							"\n",
							"Using CTAS syntax, define a new streaming view called **`bronze_enhanced_temp`** that does the following:\n",
							"* Skips records with a null **`postcode`** (set to zero)\n",
							"* Inserts a column called **`receipt_time`** containing a current timestamp\n",
							"* Inserts a column called **`source_file`** containing the input filename"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"-- TODO\n",
							"CREATE OR REPLACE TEMPORARY VIEW bronze_enhanced_temp AS\n",
							"SELECT\n",
							"  <FILL-IN>"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"Run the cell below to check your work."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"assert spark.table(\"bronze_enhanced_temp\"), \"Table named `bronze_enhanced_temp` does not exist\"\n",
							"assert spark.sql(f\"SHOW TABLES\").filter(f\"tableName == 'bronze_enhanced_temp'\").first()[\"isTemporary\"] == True, \"Table is not temporary\"\n",
							"assert spark.table(\"bronze_enhanced_temp\").dtypes ==  [('customer_id', 'string'), ('tax_id', 'string'), ('tax_code', 'string'), ('customer_name', 'string'), ('state', 'string'), ('city', 'string'), ('postcode', 'string'), ('street', 'string'), ('number', 'string'), ('unit', 'string'), ('region', 'string'), ('district', 'string'), ('lon', 'string'), ('lat', 'string'), ('ship_to_address', 'string'), ('valid_from', 'string'), ('valid_to', 'string'), ('units_purchased', 'string'), ('loyalty_segment', 'string'), ('_rescued_data', 'string'), ('receipt_time', 'timestamp'), ('source_file', 'string')], \"Incorrect Schema\"\n",
							"assert spark.table(\"bronze_enhanced_temp\").isStreaming, \"Not a streaming table\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Silver table\n",
							"\n",
							"Stream the data from **`bronze_enhanced_temp`** to a table called **`silver`**."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# TODO\n",
							"silver_checkpoint_path = f\"{DA.paths.checkpoints}/silver\"\n",
							"\n",
							"query = (spark.table(\"bronze_enhanced_temp\")\n",
							"  <FILL-IN>\n",
							"  .table(\"silver\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA.block_until_stream_is_ready(query)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"Run the cell below to check your work."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"assert spark.table(\"silver\"), \"Table named `silver` does not exist\"\n",
							"assert spark.sql(f\"SHOW TABLES\").filter(f\"tableName == 'silver'\").first()[\"isTemporary\"] == False, \"Table is temporary\"\n",
							"assert spark.table(\"silver\").dtypes ==  [('customer_id', 'string'), ('tax_id', 'string'), ('tax_code', 'string'), ('customer_name', 'string'), ('state', 'string'), ('city', 'string'), ('postcode', 'string'), ('street', 'string'), ('number', 'string'), ('unit', 'string'), ('region', 'string'), ('district', 'string'), ('lon', 'string'), ('lat', 'string'), ('ship_to_address', 'string'), ('valid_from', 'string'), ('valid_to', 'string'), ('units_purchased', 'string'), ('loyalty_segment', 'string'), ('_rescued_data', 'string'), ('receipt_time', 'timestamp'), ('source_file', 'string')], \"Incorrect Schema\"\n",
							"assert spark.table(\"silver\").filter(\"postcode <= 0\").count() == 0, \"Null postcodes present\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"Let's create a streaming temporary view into the silver table, so that we can perform business-level aggregation using SQL."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"(spark\n",
							"  .readStream\n",
							"  .table(\"silver\")\n",
							"  .createOrReplaceTempView(\"silver_temp\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Gold tables\n",
							"\n",
							"Using CTAS syntax, define a new streaming view called **`customer_count_temp`** that counts customers per state in a column named **`customer_count`**."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"-- TODO\n",
							"CREATE OR REPLACE TEMPORARY VIEW customer_count_temp AS\n",
							"SELECT \n",
							"<FILL-IN>"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"Run the cell below to check your work."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"assert spark.table(\"customer_count_temp\"), \"Table named `customer_count_temp` does not exist\"\n",
							"assert spark.sql(f\"SHOW TABLES\").filter(f\"tableName == 'customer_count_temp'\").first()[\"isTemporary\"] == True, \"Table is not temporary\"\n",
							"assert spark.table(\"customer_count_temp\").dtypes ==  [('state', 'string'), ('customer_count', 'bigint')], \"Incorrect Schema\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Finally, stream the data from the **`customer_count_temp`** view to a Delta table called **`gold_customer_count_by_state`**."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# TODO\n",
							"customers_count_checkpoint_path = f\"{DA.paths.checkpoints}/customers_counts\"\n",
							"\n",
							"query = (spark\n",
							"  .table(\"customer_count_temp\")\n",
							"  .writeStream\n",
							"  <FILL-IN>\n",
							"  .table(\"gold_customer_count_by_state\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA.block_until_stream_is_ready(query)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"Run the cell below to check your work."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"assert spark.table(\"gold_customer_count_by_state\"), \"Table named `gold_customer_count_by_state` does not exist\"\n",
							"assert spark.sql(f\"show tables\").filter(f\"tableName == 'gold_customer_count_by_state'\").first()[\"isTemporary\"] == False, \"Table is temporary\"\n",
							"assert spark.table(\"gold_customer_count_by_state\").dtypes ==  [('state', 'string'), ('customer_count', 'bigint')], \"Incorrect Schema\"\n",
							"assert spark.table(\"gold_customer_count_by_state\").count() == 51, \"Incorrect number of rows\" "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Query the results\n",
							"\n",
							"Query the **`gold_customer_count_by_state`** table (this will not be a streaming query). Plot the results as a bar graph and also using the map plot."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"SELECT * FROM gold_customer_count_by_state"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Wrapping Up\n",
							"\n",
							"Run the following cell to remove the database and all data associated with this lab."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.cleanup()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"By completing this lab, you should now feel comfortable:\n",
							"* Using PySpark to configure Auto Loader for incremental data ingestion\n",
							"* Using Spark SQL to aggregate streaming data\n",
							"* Streaming data to a Delta table"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 8-1-1 - DLT UI Walkthrough')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "08 - Delta Live Tables/DE 8.1 - DLT"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "4bd90a4a-4f95-4f5a-a952-d727f9b404c4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Using the Delta Live Tables UI\n",
							"\n",
							"This demo will explore the DLT UI. \n",
							"\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lesson, you should be able to:\n",
							"* Deploy a DLT pipeline\n",
							"* Explore the resultant DAG\n",
							"* Execute an update of the pipeline\n",
							"* Look at metrics"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Run Setup\n",
							"\n",
							"The following cell is configured to reset this demo."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../../Includes/Classroom-Setup-08.1.1"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Execute the following cell to print out values that will be used during the following configuration steps."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.print_pipeline_config()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Create and Configure a Pipeline\n",
							"\n",
							"In this section you will create a pipeline using a notebook provided with the courseware. We'll explore the contents of the notebook in the following lesson.\n",
							"\n",
							"1. Click the **Workflows** button on the sidebar.\n",
							"1. Select the **Delta Live Tables** tab.\n",
							"1. Click **Create Pipeline**.\n",
							"1. Leave **Product Edition** as **Advanced**.\n",
							"1. Fill in a **Pipeline Name** - because these names must be unique, we suggest using the **`Pipeline Name`** provided by the cell above.\n",
							"1. For **Notebook Libraries**, use the navigator to locate and select the notebook specified above.\n",
							"   * Even though this document is a standard Databricks Notebook, the SQL syntax is specialized to DLT table declarations.\n",
							"   * We will be exploring the syntax in the exercise that follows.\n",
							"1. Under **Configuration**, add two configuration parameters:\n",
							"   * Click **Add configuration**, set the \"key\" to **spark.master** and the \"value\" to **local[\\*]**.\n",
							"   * Click **Add configuration**, set the \"key\" to **datasets_path** and the \"value\" to the value provided in the cell above.\n",
							"1. In the **Target** field, enter the database name provided in the cell above.<br/>\n",
							"This should follow the pattern **`da_<name>_<hash>_dewd_dlt_demo_81`**\n",
							"   * This field is optional; if not specified, then tables will not be registered to a metastore, but will still be available in the DBFS. Refer to the <a href=\"https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-user-guide.html#publish-tables\" target=\"_blank\">documentation</a> for more information on this option.\n",
							"1. In the **Storage location** field, enter the path provided in the cell above.\n",
							"   * This optional field allows the user to specify a location to store logs, tables, and other information related to pipeline execution. \n",
							"   * If not specified, DLT will automatically generate a directory.\n",
							"1. For **Pipeline Mode**, select **Triggered**\n",
							"   * This field specifies how the pipeline will be run.\n",
							"   * **Triggered** pipelines run once and then shut down until the next manual or scheduled update.\n",
							"   * **Continuous** pipelines run continuously, ingesting new data as it arrives. Choose the mode based on latency and cost requirements.\n",
							"1. For **Pipeline Mode**, select **Triggered**.\n",
							"1. Uncheck the **Enable autoscaling** box.\n",
							"1. Set the number of **`workers`** to **`0`** (zero).\n",
							"   * Along with the **spark.master** config above, this will create a **Single Node** clusters.\n",
							"1. Enable **Photon Acceleration**.\n",
							"\n",
							"The fields **Enable autoscaling**, **Min Workers** and **Max Workers** control the worker configuration for the underlying cluster processing the pipeline. \n",
							"\n",
							"Notice the DBU estimate provided, similar to that provided when configuring interactive clusters.\n",
							"\n",
							"Finally, click **Create**."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.validate_pipeline_config()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Run a Pipeline\n",
							"\n",
							"With a pipeline created, you will now run the pipeline.\n",
							"\n",
							"1. Select **Development** to run the pipeline in development mode. \n",
							"  * Development mode provides for more expeditious iterative development by reusing the cluster (as opposed to creating a new cluster for each run) and disabling retries so that you can readily identify and fix errors.\n",
							"  * Refer to the <a href=\"https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-user-guide.html#optimize-execution\" target=\"_blank\">documentation</a> for more information on this feature.\n",
							"2. Click **Start**.\n",
							"\n",
							"The initial run will take several minutes while a cluster is provisioned. \n",
							"\n",
							"Subsequent runs will be appreciably quicker."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Exploring the DAG\n",
							"\n",
							"As the pipeline completes, the execution flow is graphed. \n",
							"\n",
							"Selecting the tables reviews the details.\n",
							"\n",
							"Select **sales_orders_cleaned**. Notice the results reported in the **Data Quality** section. Because this flow has data expectations declared, those metrics are tracked here. No records are dropped because the constraint is declared in a way that allows violating records to be included in the output. This will be covered in more details in the next exercise."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 8-1-2 - SQL for Delta Live Tables')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "08 - Delta Live Tables/DE 8.1 - DLT"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "1c8277b5-2df0-436a-8ee2-cea4f0551d94"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# SQL for Delta Live Tables\n",
							"\n",
							"In the last lesson, we walked through the process of scheduling this notebook as a Delta Live Table (DLT) pipeline. Now we'll explore the contents of this notebook to better understand the syntax used by Delta Live Tables.\n",
							"\n",
							"This notebook uses SQL to declare Delta Live Tables that together implement a simple multi-hop architecture based on a Databricks-provided example dataset loaded by default into Databricks workspaces.\n",
							"\n",
							"At its simplest, you can think of DLT SQL as a slight modification to traditional CTAS statements. DLT tables and views will always be preceded by the **`LIVE`** keyword.\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lesson, you should be able to:\n",
							"* Define tables and views with Delta Live Tables\n",
							"* Use SQL to incrementally ingest raw data with Auto Loader\n",
							"* Perform incremental reads on Delta tables with SQL\n",
							"* Update code and redeploy a pipeline"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Declare Bronze Layer Tables\n",
							"\n",
							"Below we declare two tables implementing the bronze layer. This represents data in its rawest form, but captured in a format that can be retained indefinitely and queried with the performance and benefits that Delta Lake has to offer."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"### sales_orders_raw\n",
							"\n",
							"**`sales_orders_raw`** ingests JSON data incrementally from our **retail-org/sales_orders** dataset.\n",
							"\n",
							"Incremental processing via <a herf=\"https://docs.databricks.com/spark/latest/structured-streaming/auto-loader.html\" target=\"_blank\">Auto Loader</a> (which uses the same processing model as Structured Streaming), requires the addition of the **`STREAMING`** keyword in the declaration as seen below. The **`cloud_files()`** method enables Auto Loader to be used natively with SQL. This method takes the following positional parameters:\n",
							"* The source location, as mentioned above\n",
							"* The source data format, which is JSON in this case\n",
							"* An arbitrarily sized array of optional reader options. In this case, we set **`cloudFiles.inferColumnTypes`** to **`true`**\n",
							"\n",
							"The following declaration also demonstrates the declaration of additional table metadata (a comment and properties in this case) that would be visible to anyone exploring the data catalog."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REFRESH STREAMING LIVE TABLE sales_orders_raw\n",
							"COMMENT \"The raw sales orders, ingested from retail-org/sales_orders.\"\n",
							"AS SELECT * FROM cloud_files(\"${datasets_path}/retail-org/sales_orders\", \"json\", map(\"cloudFiles.inferColumnTypes\", \"true\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"### customers\n",
							"\n",
							"**`customers`** presents CSV customer data found in **retail-org/customers**.\n",
							"\n",
							"This table will soon be used in a join operation to look up customer data based on sales records."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REFRESH STREAMING LIVE TABLE customers\n",
							"COMMENT \"The customers buying finished products, ingested from retail-org/customers.\"\n",
							"AS SELECT * FROM cloud_files(\"${datasets_path}/retail-org/customers/\", \"csv\");"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Declare Silver Layer Tables\n",
							"\n",
							"Now we declare tables implementing the silver layer. This layer represents a refined copy of data from the bronze layer, with the intention of optimizing downstream applications. At this level we apply operations like data cleansing and enrichment."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"### sales_orders_cleaned\n",
							"\n",
							"Here we declare our first silver table, which enriches the sales transaction data with customer information in addition to implementing quality control by rejecting records with a null order number.\n",
							"\n",
							"This declaration introduces a number of new concepts.\n",
							"\n",
							"#### Quality Control\n",
							"\n",
							"The **`CONSTRAINT`** keyword introduces quality control. Similar in function to a traditional **`WHERE`** clause, **`CONSTRAINT`** integrates with DLT, enabling it to collect metrics on constraint violations. Constraints provide an optional **`ON VIOLATION`** clause, specifying an action to take on records that violate the constraint. The three modes currently supported by DLT include:\n",
							"\n",
							"| **`ON VIOLATION`** | Behavior |\n",
							"| --- | --- |\n",
							"| **`FAIL UPDATE`** | Pipeline failure when constraint is violated |\n",
							"| **`DROP ROW`** | Discard records that violate constraints |\n",
							"| Omitted | Records violating constraints will be included (but violations will be reported in metrics) |\n",
							"\n",
							"#### References to DLT Tables and Views\n",
							"References to other DLT tables and views will always include the **`live.`** prefix. A target database name will automatically be substituted at runtime, allowing for easily migration of pipelines between DEV/QA/PROD environments.\n",
							"\n",
							"#### References to Streaming Tables\n",
							"\n",
							"References to streaming DLT tables use the **`STREAM()`**, supplying the table name as an argument."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REFRESH STREAMING LIVE TABLE sales_orders_cleaned(\n",
							"  CONSTRAINT valid_order_number EXPECT (order_number IS NOT NULL) ON VIOLATION DROP ROW\n",
							")\n",
							"COMMENT \"The cleaned sales orders with valid order_number(s).\"\n",
							"AS\n",
							"  SELECT f.customer_id, f.customer_name, f.number_of_line_items, \n",
							"         timestamp(from_unixtime((cast(f.order_datetime as long)))) as order_datetime, \n",
							"         date(from_unixtime((cast(f.order_datetime as long)))) as order_date, \n",
							"         f.order_number, f.ordered_products, c.state, c.city, c.lon, c.lat, c.units_purchased, c.loyalty_segment\n",
							"  FROM STREAM(LIVE.sales_orders_raw) f\n",
							"  LEFT JOIN LIVE.customers c\n",
							"    ON c.customer_id = f.customer_id\n",
							"    AND c.customer_name = f.customer_name"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Declare Gold Table\n",
							"\n",
							"At the most refined level of the architecture, we declare a table delivering an aggregation with business value, in this case a collection of sales order data based in a specific region. In aggregating, the report generates counts and totals of orders by date and customer."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REFRESH LIVE TABLE sales_order_in_la\n",
							"COMMENT \"Sales orders in LA.\"\n",
							"AS\n",
							"  SELECT city, order_date, customer_id, customer_name, ordered_products_explode.curr, \n",
							"         sum(ordered_products_explode.price) as sales, \n",
							"         sum(ordered_products_explode.qty) as quantity, \n",
							"         count(ordered_products_explode.id) as product_count\n",
							"  FROM (SELECT city, order_date, customer_id, customer_name, explode(ordered_products) as ordered_products_explode\n",
							"        FROM LIVE.sales_orders_cleaned \n",
							"        WHERE city = 'Los Angeles')\n",
							"  GROUP BY order_date, city, customer_id, customer_name, ordered_products_explode.curr"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Explore Results\n",
							"\n",
							"Explore the DAG (Directed Acyclic Graph) representing the entities involved in the pipeline and the relationships between them. Click on each to view a summary, which includes:\n",
							"* Run status\n",
							"* Metadata summary\n",
							"* Schema\n",
							"* Data quality metrics\n",
							"\n",
							"Refer to this <a href=\"$./DE 8.3 - Pipeline Results\" target=\"_blank\">companion notebook</a> to inspect tables and logs."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Update Pipeline\n",
							"\n",
							"Uncomment the following cell to declare another gold table. Similar to the previous gold table declaration, this filters for the **`city`** of Chicago. \n",
							"\n",
							"Re-run your pipeline to examine the updated results. \n",
							"\n",
							"Does it run as expected? \n",
							"\n",
							"Can you identify any issues?"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"-- CREATE OR REFRESH LIVE TABLE sales_order_in_chicago\n",
							"-- COMMENT \"Sales orders in Chicago.\"\n",
							"-- AS\n",
							"--   SELECT city, order_date, customer_id, customer_name, ordered_products_explode.curr, \n",
							"--          sum(ordered_products_explode.price) as sales, \n",
							"--          sum(ordered_products_explode.qty) as quantity, \n",
							"--          count(ordered_products_explode.id) as product_count\n",
							"--   FROM (SELECT city, order_date, customer_id, customer_name, explode(ordered_products) as ordered_products_explode\n",
							"--         FROM sales_orders_cleaned \n",
							"--         WHERE city = 'Chicago')\n",
							"--   GROUP BY order_date, city, customer_id, customer_name, ordered_products_explode.curr"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 8-1-3 - Pipeline Results')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "08 - Delta Live Tables/DE 8.1 - DLT"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "abd5d13b-72be-4421-9b8f-fdd4b39cbd7a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Exploring the Results of a DLT Pipeline\n",
							"\n",
							"This notebook explores the execution results of a DLT pipeline."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../../Includes/Classroom-Setup-08.1.3"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"files = dbutils.fs.ls(DA.paths.storage_location)\n",
							"display(files)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"The **`system`** directory captures events associated with the pipeline."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"files = dbutils.fs.ls(f\"{DA.paths.storage_location}/system/events\")\n",
							"display(files)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"These event logs are stored as a Delta table. Let's query the table."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%%sql\n",
							"SELECT * FROM delta.`${da.paths.storage_location}/system/events`"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Let's view the contents of the *tables* directory."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"files = dbutils.fs.ls(f\"{DA.paths.storage_location}/tables\")\n",
							"display(files)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Let's query the gold table."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%%sql\n",
							"SELECT * FROM ${da.db_name}.sales_order_in_la"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							" \n",
							"Run the following cell to delete the tables and files associated with this lesson."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.cleanup()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 8-2-1L - Lab Instructions')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "08 - Delta Live Tables/DE 8.2 - DLT Lab"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "606f0fb1-f88e-4123-9b63-df503da82713"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Lab: Migrating SQL Notebooks to Delta Live Tables\n",
							"\n",
							"This notebook describes the overall structure for the lab exercise, configures the environment for the lab, provides simulated data streaming, and performs cleanup once you are done. A notebook like this is not typically needed in a production pipeline scenario.\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lab, you should be able to:\n",
							"* Convert existing data pipelines to Delta Live Tables"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Datasets Used\n",
							"\n",
							"This demo uses simplified artificially generated medical data. The schema of our two datasets is represented below. Note that we will be manipulating these schema during various steps.\n",
							"\n",
							"#### Recordings\n",
							"The main dataset uses heart rate recordings from medical devices delivered in the JSON format. \n",
							"\n",
							"| Field | Type |\n",
							"| --- | --- |\n",
							"| device_id | int |\n",
							"| mrn | long |\n",
							"| time | double |\n",
							"| heartrate | double |\n",
							"\n",
							"#### PII\n",
							"These data will later be joined with a static table of patient information stored in an external system to identify patients by name.\n",
							"\n",
							"| Field | Type |\n",
							"| --- | --- |\n",
							"| mrn | long |\n",
							"| name | string |"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Getting Started\n",
							"\n",
							"Begin by running the following cell to configure the lab environment."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../../Includes/Classroom-Setup-08.2.1L"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Land Initial Data\n",
							"Seed the landing zone with more data before proceeding.\n",
							"\n",
							"You will re-run this command to land additional data later."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.data_factory.load()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Execute the following cell to print out values that will be used during the following configuration steps."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.print_pipeline_config()    "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Create and Configure a Pipeline\n",
							"\n",
							"1. Click the **Workflows** button on the sidebar.\n",
							"1. Select the **Delta Live Tables** tab.\n",
							"1. Click **Create Pipeline**.\n",
							"1. Leave **Product Edition** as **Advanced**.\n",
							"1. Fill in a **Pipeline Name** - because these names must be unique, we suggest using the **Pipeline Name** provided in the cell above.\n",
							"1. For **Notebook Libraries**, use the navigator to locate and select the notebook specified above.\n",
							"1. Under **Configuration**, add three configuration parameters:\n",
							"   * Click **Add configuration**, set the \"key\" to **spark.master** and the \"value\" to **local[\\*]**.\n",
							"   * Click **Add configuration**, set the \"key\" to **datasets_path** and the \"value\" to the value provided in the cell above.\n",
							"   * Click **Add configuration**, set the \"key\" to **source** and the \"value\" to the value provided in the cell above.\n",
							"1. In the **Target** field, enter the database name provided in the cell above.<br/>\n",
							"This should follow the pattern **`da_<name_<hash>_dewd_dlt_lab_82`**\n",
							"1. In the **Storage location** field, enter the path provided in the cell above.\n",
							"1. Enter the location printed next to **`Storage Location`** below in the **Storage Location** field.\n",
							"1. For **Pipeline Mode**, select **Triggered**.\n",
							"1. Uncheck the **Enable autoscaling** box.\n",
							"1. Set the number of **`workers`** to **`0`** (zero).\n",
							"1. Enable **Photon Acceleration**.\n",
							"\n",
							"Finally, click **Create**."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.validate_pipeline_config()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Open and Complete DLT Pipeline Notebook\n",
							"\n",
							"You will perform your work in the companion notebook [DE 8.2.2L - Migrating a SQL Pipeline to DLT Lab]($./DE 8.2.2L - Migrating a SQL Pipeline to DLT Lab),<br/>\n",
							"which you will ultimately deploy as a pipeline.\n",
							"\n",
							"Open the Notebook and, following the guidelines provided therein, fill in the cells where prompted to<br/>\n",
							"implement a multi-hop architecture similar to the one we worked with in the previous section."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Run your Pipeline\n",
							"\n",
							"Select **Development** mode, which accelerates the development lifecycle by reusing the same cluster across runs.<br/>\n",
							"It will also turn off automatic retries when jobs fail.\n",
							"\n",
							"Click **Start** to begin the first update to your table.\n",
							"\n",
							"Delta Live Tables will automatically deploy all the necessary infrastructure and resolve the dependencies between all datasets.\n",
							"\n",
							"**NOTE**: The first table update may take several minutes as relationships are resolved and infrastructure deploys."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Troubleshooting Code in Development Mode\n",
							"\n",
							"Don't despair if your pipeline fails the first time. Delta Live Tables is in active development, and error messages are improving all the time.\n",
							"\n",
							"Because relationships between tables are mapped as a DAG, error messages will often indicate that a dataset isn't found.\n",
							"\n",
							"Let's consider our DAG below:\n",
							"\n",
							"<img src=\"https://files.training.databricks.com/images/dlt-dag.png\">\n",
							"\n",
							"If the error message **`Dataset not found: 'recordings_parsed'`** is raised, there may be several culprits:\n",
							"1. The logic defining **`recordings_parsed`** is invalid\n",
							"1. There is an error reading from **`recordings_bronze`**\n",
							"1. A typo exists in either **`recordings_parsed`** or **`recordings_bronze`**\n",
							"\n",
							"The safest way to identify the culprit is to iteratively add table/view definitions back into your DAG starting from your initial ingestion tables. You can simply comment out later table/view definitions and uncomment these between runs."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 8-2-2L - Migrating a SQL Pipeline to DLT Lab')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "08 - Delta Live Tables/DE 8.2 - DLT Lab"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "2ab317b8-6967-4e68-a647-98a4925827d0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Lab: Migrating a SQL Pipeline to Delta Live Tables\n",
							"\n",
							"This notebook will be completed by you to implement a DLT pipeline using SQL. \n",
							"\n",
							"It is **not intended** to be executed interactively, but rather to be deployed as a pipeline once you have completed your changes.\n",
							"\n",
							"To aid in completion of this Notebook, please refer to the <a href=\"https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-language-ref.html#sql\" target=\"_blank\">DLT syntax documentation</a>."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Declare Bronze Table\n",
							"\n",
							"Declare a bronze table, **`recordings_bronze`**, that ingests JSON data incrementally (using Auto Loader) from the simulated cloud source. The source location is already supplied as an argument; using this value is illustrated in the cell below.\n",
							"\n",
							"As we did previously, include two additional columns:\n",
							"* **`receipt_time`** that records a timestamp as returned by **`current_timestamp()`** \n",
							"* **`source_file`** that is obtained by **`input_file_name()`**"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"CREATE <FILL-IN>\n",
							"AS SELECT <FILL-IN>\n",
							"  FROM cloud_files(\"${source}\", \"json\", map(\"cloudFiles.schemaHints\", \"time DOUBLE, mrn INTEGER\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"### PII File\n",
							"\n",
							"Using a similar CTAS syntax, create a live **table** into the CSV data found in the *healthcare/patient* dataset.\n",
							"\n",
							"To properly configure Auto Loader for this source, you will need to specify the following additional parameters:\n",
							"\n",
							"| option | value |\n",
							"| --- | --- |\n",
							"| **`header`** | **`true`** |\n",
							"| **`cloudFiles.inferColumnTypes`** | **`true`** |\n",
							"\n",
							"<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"/> Auto Loader configurations for CSV can be found <a href=\"https://docs.databricks.com/spark/latest/structured-streaming/auto-loader-csv.html\" target=\"_blank\">here</a>."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"CREATE <FILL-IN> pii\n",
							"AS SELECT *\n",
							"  FROM cloud_files(\"${datasets_path}/healthcare/patient\", \"csv\", map(<FILL-IN>))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Declare Silver Tables\n",
							"\n",
							"Our silver table, **`recordings_enriched`**, will consist of the following fields:\n",
							"\n",
							"| Field | Type |\n",
							"| --- | --- |\n",
							"| **`device_id`** | **`INTEGER`** |\n",
							"| **`mrn`** | **`LONG`** |\n",
							"| **`heartrate`** | **`DOUBLE`** |\n",
							"| **`time`** | **`TIMESTAMP`** (example provided below) |\n",
							"| **`name`** | **`STRING`** |\n",
							"\n",
							"This query should also enrich the data through an inner join with the **`pii`** table on the common **`mrn`** field to obtain the name.\n",
							"\n",
							"Implement quality control by applying a constraint to drop records with an invalid **`heartrate`** (that is, not greater than zero)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"CREATE OR REFRESH STREAMING LIVE TABLE recordings_enriched\n",
							"  (<FILL-IN add a constraint to drop records when heartrate ! > 0>)\n",
							"AS SELECT \n",
							"  CAST(<FILL-IN>) device_id, \n",
							"  <FILL-IN mrn>, \n",
							"  <FILL-IN heartrate>, \n",
							"  CAST(FROM_UNIXTIME(DOUBLE(time), 'yyyy-MM-dd HH:mm:ss') AS TIMESTAMP) time \n",
							"  FROM STREAM(live.recordings_bronze)\n",
							"  <FILL-IN specify an inner join with the pii table on the mrn field>"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Gold Table\n",
							"\n",
							"Create a gold table, **`daily_patient_avg`**, that aggregates **`recordings_enriched`** by **`mrn`**, **`name`**, and **`date`** and delivers the following columns:\n",
							"\n",
							"| Column name | Value |\n",
							"| --- | --- |\n",
							"| **`mrn`** | **`mrn`** from source |\n",
							"| **`name`** | **`name`** from source |\n",
							"| **`avg_heartrate`** | Average **`heartrate`** from the grouping |\n",
							"| **`date`** | Date extracted from **`time`** |"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"-- TODO\n",
							"CREATE <FILL-IN> daily_patient_avg\n",
							"  COMMENT <FILL-IN insert comment here>\n",
							"AS SELECT <FILL-IN>"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 8-2-3L - Lab Conclusion')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "08 - Delta Live Tables/DE 8.2 - DLT Lab"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ba380024-926b-4a8e-8a3c-3a5efa27849c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Lab: Conclusion\n",
							"Running the following cell to configure the lab environment:"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../../Includes/Classroom-Setup-08.2.3L"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Display Results\n",
							"\n",
							"Assuming your pipeline runs successfully, display the contents of the gold table.\n",
							"\n",
							"**NOTE**: Because we specified a value for **Target**, tables are published to the specified database. Without a **Target** specification, we would need to query the table based on its underlying location in DBFS (relative to the **Storage Location**)."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"SELECT * FROM ${da.db_name}.daily_patient_avg"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Trigger another file arrival with the following cell. \n",
							"\n",
							"Feel free to run it a couple more times if desired. \n",
							"\n",
							"Following this, run the pipeline again and view the results. \n",
							"\n",
							"Feel free to re-run the cell above to gain an updated view of the **`daily_patient_avg`** table."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.data_factory.load()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Wrapping Up\n",
							"\n",
							"Ensure that you delete your pipeline from the DLT UI, and run the following cell to clean up the files and tables that were created as part of the lab setup and execution."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.cleanup()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Summary\n",
							"\n",
							"In this lab, you learned to convert an existing data pipeline to a Delta Live Tables SQL pipeline, and deployed that pipeline using the DLT UI."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Additional Topics & Resources\n",
							"\n",
							"* <a href=\"https://docs.databricks.com/data-engineering/delta-live-tables/index.html\" target=\"_blank\">Delta Live Tables Documentation</a>\n",
							"* <a href=\"https://youtu.be/6Q8qPZ7c1O0\" target=\"_blank\">Delta Live Tables Demo</a>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 9-1-1 - Task Orchestration with Databricks Jobs')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "09 - Task Orchestration with Jobs/DE 9.1 - Scheduling Tasks with the Jobs UI"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "cf3a79e6-0f9e-4c81-8019-dc8b44b564d8"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Orchestrating Jobs with Databricks Workflows\n",
							"\n",
							"New updates to the Databricks Jobs UI have added the ability to schedule multiple tasks as part of a job, allowing Databricks Jobs to fully handle orchestration for most production workloads.\n",
							"\n",
							"Here, we'll start by reviewing the steps for scheduling a notebook task as a triggered standalone job, and then add a dependent task using a DLT pipeline. \n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lesson, you should be able to:\n",
							"* Schedule a notebook task in a Databricks Workflow Job\n",
							"* Describe job scheduling options and differences between cluster types\n",
							"* Review Job Runs to track progress and see results\n",
							"* Schedule a DLT pipeline task in a Databricks Workflow Job\n",
							"* Configure linear dependencies between tasks using the Databricks Workflows UI"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../../Includes/Classroom-Setup-09.1.1"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Create and configure a pipeline\n",
							"The pipeline we create here is nearly identical to the one in the previous unit.\n",
							"\n",
							"We will use it as part of a scheduled job in this lesson.\n",
							"\n",
							"Execute the following cell to print out the values that will be used during the following configuration steps."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.print_pipeline_config()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Create and configure a Pipeline\n",
							"\n",
							"Steps:\n",
							"1. Click the **Workflows** button on the sidebar.\n",
							"1. Select the **Delta Live Tables** tab.\n",
							"1. Click **Create Pipeline**.\n",
							"1. Fill in a **Pipeline Name** - because these names must be unique, we suggest using the **Pipeline Name** provided in the cell above.\n",
							"1. For **Notebook Libraries**, use the navigator to locate and select the companion notebook provided in the cell above.\n",
							"1. Under **Configuration**, add the two configuration parameters:\n",
							"   * Click **Add configuration**, set the \"key\" to **spark.master** and the \"value\" to **local[\\*]**.\n",
							"   * Click **Add configuration**, set the \"key\" to **datasets_path** and the \"value\" to the value provided in the cell above.\n",
							"1. In the **Target** field, enter the database name provided in the cell above.<br/>\n",
							"This should follow the pattern **`da_<name>_<hash>_dewd_jobs_demo_91`**\n",
							"1. In the **Storage location** field, copy the directory as printed above.\n",
							"1. For **Pipeline Mode**, select **Triggered**\n",
							"1. Uncheck the **Enable autoscaling** box\n",
							"1. Set the number of **`workers`** to **`0`** (zero).\n",
							"1. Enable **Photon Acceleration**.\n",
							"\n",
							"Finally, click **Create**.\n",
							"\n",
							"<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"> **Note**: we won't be executing this pipline directly as it will be executed by our job later in this lesson,<br/>\n",
							"but if you want to test it real quick, you can click the **Start** button now."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.validate_pipeline_config()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"\n",
							"## Schedule a Notebook Job\n",
							"\n",
							"When using the Jobs UI to orchestrate a workload with multiple tasks, you'll always begin by scheduling a single task.\n",
							"\n",
							"Before we start run the following cell to get the values used in this step."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.print_job_config_task_reset()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"Here, we'll start by scheduling the next notebook.\n",
							"\n",
							"Steps:\n",
							"1. Click the **Workflows** button on the sidebar.\n",
							"1. Select the **Jobs** tab.\n",
							"1. Click the **Create Job** button.\n",
							"1. Configure the task:\n",
							"    1. Enter **Reset** for the task name\n",
							"    1. For **Type**, select **Notebook**\n",
							"    1. For **Path**, select the **Reset Notebook Path** value provided in the cell above\n",
							"    1. From the **Cluster** dropdown, under **Existing All Purpose Clusters**, select your cluster\n",
							"    1. Click **Create**\n",
							"1. In the top-left of the screen, rename the job (not the task) from **`Reset`** (the defaulted value) to the **Job Name** value provided in the cell above.\n",
							"1. Click the blue **Run now** button in the top right to start the job.\n",
							"\n",
							"<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"> **Note**: When selecting your all-purpose cluster, you will get a warning about how this will be billed as all-purpose compute. Production jobs should always be scheduled against new job clusters appropriately sized for the workload, as this is billed at a much lower rate."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.validate_job_v1_config()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Cron Scheduling of Databricks Jobs\n",
							"\n",
							"Note that on the right hand side of the Jobs UI, directly under the **Job Details** section is a section labeled **Schedule**.\n",
							"\n",
							"Click on the **Edit schedule** button to explore scheduling options.\n",
							"\n",
							"Changing the **Schedule type** field from **Manual** to **Scheduled** will bring up a cron scheduling UI.\n",
							"\n",
							"This UI provides extensive options for setting up chronological scheduling of your Jobs. Settings configured with the UI can also be output in cron syntax, which can be edited if custom configuration not available with the UI is needed.\n",
							"\n",
							"At this time, we'll leave our job set to the **Manual (Paused)** scheduling type."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Review Run\n",
							"\n",
							"As currently configured, our single notebook provides identical performance to the legacy Databricks Jobs UI, which only allowed a single notebook to be scheduled.\n",
							"\n",
							"To Review the Job Run\n",
							"1. Select the **Runs** tab in the top-left of the screen (you should currently be on the **Tasks** tab)\n",
							"1. Find your job. If **the job is still running**, it will be under the **Active runs** section. If **the job finished running**, it will be under the **Completed runs** section\n",
							"1. Open the Output details by clicking on the timestamp field under the **Start time** column\n",
							"1. If **the job is still running**, you will see the active state of the notebook with a **Status** of **`Pending`** or **`Running`** in the right side panel. If **the job has completed**, you will see the full execution of the notebook with a **Status** of **`Succeeded`** or **`Failed`** in the right side panel\n",
							"  \n",
							"The notebook employs the magic command **`%run`** to call an additional notebook using a relative path. Note that while not covered in this course, <a href=\"https://docs.databricks.com/repos.html#work-with-non-notebook-files-in-a-databricks-repo\" target=\"_blank\">new functionality added to Databricks Repos allows loading Python modules using relative paths</a>.\n",
							"\n",
							"The actual outcome of the scheduled notebook is to reset the environment for our new job and pipeline."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Schedule a DLT Pipeline as a Task\n",
							"\n",
							"In this step, we'll add a DLT pipeline to execute after the success of the task we configured at the start of this lesson.\n",
							"\n",
							"Steps:\n",
							"1. At the top left of your screen, you'll see the **Runs** tab is currently selected; click the **Tasks** tab.\n",
							"1. Click the large blue circle with a **+** at the center bottom of the screen to add a new task\n",
							"1. Configure the task:\n",
							"    1. Enter **DLT** for the task name\n",
							"    1. For **Type**, select  **Delta Live Tables pipeline**\n",
							"    1. For **Pipeline**, select the DLT pipeline you configured previously in this exercise<br/>\n",
							"    Note: The pipeline will start with **DLT-Job-Demo-91** and will end with your email address.\n",
							"    1. The **Depends on** field defaults to your previously defined task, **Reset** - leave this value as-is.\n",
							"    1. Click the blue **Create task** button\n",
							"\n",
							"You should now see a screen with 2 boxes and a downward arrow between them. \n",
							"\n",
							"Your **`Reset`** task will be at the top, leading into your **`DLT`** task. \n",
							"\n",
							"This visualization represents the dependencies between these tasks.\n",
							"\n",
							"Click **Run now** to execute your job.\n",
							"\n",
							"**NOTE**: You may need to wait a few minutes as infrastructure for your job and pipeline is deployed."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.validate_job_v2_config()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Review Multi-Task Run Results\n",
							"\n",
							"Select the **Runs** tab again and then the most recent run under **Active runs** or **Completed runs** depending on if the job has completed or not.\n",
							"\n",
							"The visualizations for tasks will update in real time to reflect which tasks are actively running, and will change colors if task failures occur. \n",
							"\n",
							"Clicking on a task box will render the scheduled notebook in the UI. \n",
							"\n",
							"You can think of this as just an additional layer of orchestration on top of the previous Databricks Jobs UI, if that helps; note that if you have workloads scheduling jobs with the CLI or REST API, <a href=\"https://docs.databricks.com/dev-tools/api/latest/jobs.html\" target=\"_blank\">the JSON structure used to configure and get results about jobs has seen similar updates to the UI</a>.\n",
							"\n",
							"**NOTE**: At this time, DLT pipelines scheduled as tasks do not directly render results in the Runs GUI; instead, you will be directed back to the DLT Pipeline GUI for the scheduled Pipeline."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 9-1-2 - Reset')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "09 - Task Orchestration with Jobs/DE 9.1 - Scheduling Tasks with the Jobs UI"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "2bcf993c-3a30-4be7-b279-87d4f247fcf6"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../../Includes/Classroom-Setup-09.1.2"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 9-1-3 - DLT Job')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "09 - Task Orchestration with Jobs/DE 9.1 - Scheduling Tasks with the Jobs UI"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "cdccf3ce-9533-4748-a5ff-05697bb9b979"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ../../Includes/Classroom-Setup-09.1.3"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REFRESH STREAMING LIVE TABLE sales_orders_raw\n",
							"COMMENT \"The raw sales orders, ingested from retail-org/sales_orders.\"\n",
							"AS\n",
							"SELECT * FROM cloud_files(\"${datasets_path}/retail-org/sales_orders/\", \"json\", map(\"cloudFiles.inferColumnTypes\", \"true\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REFRESH STREAMING LIVE TABLE customers\n",
							"COMMENT \"The customers buying finished products, ingested from retail-org/customers.\"\n",
							"AS SELECT * FROM cloud_files(\"${datasets_path}/retail-org/customers/\", \"csv\");"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REFRESH STREAMING LIVE TABLE sales_orders_cleaned(\n",
							"  CONSTRAINT valid_order_number EXPECT (order_number IS NOT NULL) ON VIOLATION DROP ROW\n",
							")\n",
							"COMMENT \"The cleaned sales orders with valid order_number(s) and partitioned by order_datetime.\"\n",
							"AS\n",
							"SELECT f.customer_id, f.customer_name, f.number_of_line_items, \n",
							"  TIMESTAMP(from_unixtime((cast(f.order_datetime as long)))) as order_datetime, \n",
							"  DATE(from_unixtime((cast(f.order_datetime as long)))) as order_date, \n",
							"  f.order_number, f.ordered_products, c.state, c.city, c.lon, c.lat, c.units_purchased, c.loyalty_segment\n",
							"  FROM STREAM(LIVE.sales_orders_raw) f\n",
							"  LEFT JOIN LIVE.customers c\n",
							"      ON c.customer_id = f.customer_id\n",
							"     AND c.customer_name = f.customer_name"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REFRESH LIVE TABLE sales_order_in_la\n",
							"COMMENT \"Sales orders in LA.\"\n",
							"AS\n",
							"SELECT city, order_date, customer_id, customer_name, ordered_products_explode.curr, SUM(ordered_products_explode.price) as sales, SUM(ordered_products_explode.qty) as quantity, COUNT(ordered_products_explode.id) as product_count\n",
							"FROM (\n",
							"  SELECT city, order_date, customer_id, customer_name, EXPLODE(ordered_products) as ordered_products_explode\n",
							"  FROM LIVE.sales_orders_cleaned \n",
							"  WHERE city = 'Los Angeles'\n",
							"  )\n",
							"GROUP BY order_date, city, customer_id, customer_name, ordered_products_explode.curr"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REFRESH LIVE TABLE sales_order_in_chicago\n",
							"COMMENT \"Sales orders in Chicago.\"\n",
							"AS\n",
							"SELECT city, order_date, customer_id, customer_name, ordered_products_explode.curr, SUM(ordered_products_explode.price) as sales, SUM(ordered_products_explode.qty) as quantity, COUNT(ordered_products_explode.id) as product_count\n",
							"FROM (\n",
							"  SELECT city, order_date, customer_id, customer_name, EXPLODE(ordered_products) as ordered_products_explode\n",
							"  FROM LIVE.sales_orders_cleaned \n",
							"  WHERE city = 'Chicago'\n",
							"  )\n",
							"GROUP BY order_date, city, customer_id, customer_name, ordered_products_explode.curr\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 9-2-1L - Lab Instructions')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "09 - Task Orchestration with Jobs/DE 9.2L - Jobs Lab"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "cbfb5e69-171e-47a9-902b-2faee942892b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Lab: Orchestrating Jobs with Databricks\n",
							"\n",
							"In this lab, you'll be configuring a multi-task job comprising of:\n",
							"* A notebook that lands a new batch of data in a storage directory\n",
							"* A Delta Live Table pipeline that processes this data through a series of tables\n",
							"* A notebook that queries the gold table produced by this pipeline as well as various metrics output by DLT\n",
							"\n",
							"## Learning Objectives\n",
							"By the end of this lab, you should be able to:\n",
							"* Schedule a notebook as a task in a Databricks Job\n",
							"* Schedule a DLT pipeline as a task in a Databricks Job\n",
							"* Configure linear dependencies between tasks using the Databricks Workflows UI"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ../../Includes/Classroom-Setup-09.2.1L"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Land Initial Data\n",
							"Seed the landing zone with some data before proceeding. You will re-run this command to land additional data later."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.data_factory.load()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Create and Configure a Pipeline\n",
							"\n",
							"The pipeline we create here is nearly identical to the one in the previous unit.\n",
							"\n",
							"We will use it as part of a scheduled job in this lesson.\n",
							"\n",
							"Execute the following cell to print out the values that will be used during the following configuration steps."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.print_pipeline_config()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Steps:\n",
							"1. Click the **Workflows** button on the sidebar.\n",
							"1. Select the **Delta Live Tables** tab.\n",
							"1. Click **Create Pipeline**.\n",
							"1. Fill in a **Pipeline Name** - because these names must be unique, we suggest using the **Pipeline Name** provided in the cell above.\n",
							"1. For **Notebook Libraries**, use the navigator to locate and select the companion notebook provided in the cell above.\n",
							"1. Under **Configuration**, add the three configuration parameters:\n",
							"   * Click **Add configuration**, set the \"key\" to **spark.master** and the \"value\" to **local[\\*]**.\n",
							"   * Click **Add configuration**, set the \"key\" to **datasets_path** and the \"value\" to the value provided in the cell above.\n",
							"   * Click **Add configuration**, set the \"key\" to **source** and the \"value\" to the value provided in the cell above.\n",
							"1. In the **Target** field, enter the database name provided in the cell above.<br/>\n",
							"This should follow the pattern **`da_<name>_<hash>_dewd_jobs_lab_92`**\n",
							"1. In the **Storage location** field, copy the directory as printed above.\n",
							"1. For **Pipeline Mode**, select **Triggered**\n",
							"1. Uncheck the **Enable autoscaling** box\n",
							"1. Set the number of **`workers`** to **`0`** (zero).\n",
							"1. Enable **Photon Acceleration**.\n",
							"\n",
							"Finally, click **Create**.\n",
							"\n",
							"<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"> **Note**: we won't be executing this pipline directly as it will be executed by our job later in this lesson,<br/>\n",
							"but if you want to test it real quick, you can click the **Start** button now."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.validate_pipeline_config()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Schedule a Notebook Job\n",
							"\n",
							"When using the Jobs UI to orchestrate a workload with multiple tasks, you'll always begin by scheduling a single task.\n",
							"\n",
							"Before we start run the following cell to get the values used in this step."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.print_job_config()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Here, we'll start by scheduling the next notebook.\n",
							"\n",
							"Steps:\n",
							"1. Click the **Workflows** button on the sidebar\n",
							"1. Select the **Jobs** tab.\n",
							"1. Click the blue **Create Job** button\n",
							"1. Configure the task:\n",
							"    1. Enter **Batch-Job** for the task name\n",
							"    1. For **Type**, select **Notebook**\n",
							"    1. For **Path**, select the **Batch Notebook Path** value provided in the cell above\n",
							"    1. From the **Cluster** dropdown, under **Existing All Purpose Clusters**, select your cluster\n",
							"    1. Click **Create**\n",
							"1. In the top-left of the screen, rename the job (not the task) from **`Batch-Job`** (the defaulted value) to the **Job Name** value provided in the cell above.\n",
							"1. Click the blue **Run now** button in the top right to start the job.\n",
							"\n",
							"<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"> **Note**: When selecting your all purpose cluster, you will get a warning about how this will be billed as all purpose compute. Production jobs should always be scheduled against new job clusters appropriately sized for the workload, as this is billed at a much lower rate."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Schedule a DLT Pipeline as a Task\n",
							"\n",
							"In this step, we'll add a DLT pipeline to execute after the success of the task we configured at the start of this lesson.\n",
							"\n",
							"Steps:\n",
							"1. At the top left of your screen, you'll see the **Runs** tab is currently selected; click the **Tasks** tab.\n",
							"1. Click the large blue circle with a **+** at the center bottom of the screen to add a new task\n",
							"1. Configure the task:\n",
							"    1. Enter **DLT** for the task name\n",
							"    1. For **Type**, select  **Delta Live Tables pipeline**\n",
							"    1. For **Pipeline**, select the DLT pipeline you configured previously in this exercise<br/>\n",
							"    Note: The pipeline will start with **DLT-Job-Lab-92** and will end with your email address.\n",
							"    1. The **Depends on** field defaults to your previously defined task, **Batch-Job** - leave this value as-is.\n",
							"    1. Click the blue **Create task** button\n",
							"\n",
							"You should now see a screen with 2 boxes and a downward arrow between them. \n",
							"\n",
							"Your **`Batch-Job`** task will be at the top, leading into your **`DLT`** task."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"## Schedule an Additional Notebook Task\n",
							"\n",
							"An additional notebook has been provided which queries some of the DLT metrics and the gold table defined in the DLT pipeline. \n",
							"\n",
							"We'll add this as a final task in our job.\n",
							"\n",
							"Steps:\n",
							"1. Click the large blue circle with a **+** at the center bottom of the screen to add a new task\n",
							"Steps:\n",
							"1. Configure the task:\n",
							"    1. Enter **Query-Results** for the task name\n",
							"    1. For **Type**, select **Notebook**\n",
							"    1. For **Path**, select the **Query Notebook Path** value provided in the cell above\n",
							"    1. From the **Cluster** dropdown, under **Existing All Purpose Clusters**, select your cluster\n",
							"    1. The **Depends on** field defaults to your previously defined task, **DLT** - leave this value as-is.\n",
							"    1. Click the blue **Create task** button\n",
							"    \n",
							"Click the blue **Run now** button in the top right of the screen to run this job.\n",
							"\n",
							"From the **Runs** tab, you will be able to click on the start time for this run under the **Active runs** section and visually track task progress.\n",
							"\n",
							"Once all your tasks have succeeded, review the contents of each task to confirm expected behavior."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.validate_job_config()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 9-2-2L - Batch Job')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "09 - Task Orchestration with Jobs/DE 9.2L - Jobs Lab"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "72d25f10-61d9-4867-9086-6b8194197186"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ../../Includes/Classroom-Setup-09.2.2L"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA.data_factory.load()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 9-2-3L - DLT Job')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "09 - Task Orchestration with Jobs/DE 9.2L - Jobs Lab"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "35663411-7830-4e25-b860-e7f629a15760"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ../../Includes/Classroom-Setup-09.2.3L"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REFRESH STREAMING LIVE TABLE recordings_bronze\n",
							"AS SELECT current_timestamp() receipt_time, input_file_name() source_file, *\n",
							"  FROM cloud_files(\"${source}\", \"json\", map(\"cloudFiles.schemaHints\", \"time DOUBLE\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REFRESH STREAMING LIVE TABLE pii\n",
							"AS SELECT *\n",
							"  FROM cloud_files(\"${datasets_path}/healthcare/patient\", \"csv\", map(\"header\", \"true\", \"cloudFiles.inferColumnTypes\", \"true\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REFRESH STREAMING LIVE TABLE recordings_enriched\n",
							"  (CONSTRAINT positive_heartrate EXPECT (heartrate > 0) ON VIOLATION DROP ROW)\n",
							"AS SELECT \n",
							"  CAST(a.device_id AS INTEGER) device_id, \n",
							"  CAST(a.mrn AS LONG) mrn, \n",
							"  CAST(a.heartrate AS DOUBLE) heartrate, \n",
							"  CAST(from_unixtime(a.time, 'yyyy-MM-dd HH:mm:ss') AS TIMESTAMP) time,\n",
							"  b.name\n",
							"  FROM STREAM(live.recordings_bronze) a\n",
							"  INNER JOIN STREAM(live.pii) b\n",
							"  ON a.mrn = b.mrn"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"CREATE OR REFRESH STREAMING LIVE TABLE daily_patient_avg\n",
							"  COMMENT \"Daily mean heartrates by patient\"\n",
							"AS SELECT mrn, name, MEAN(heartrate) avg_heartrate, DATE(time) `date`\n",
							"  FROM STREAM(live.recordings_enriched)\n",
							"  GROUP BY mrn, name, DATE(time)\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DE 9-2-4L - Query Results Job')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "09 - Task Orchestration with Jobs/DE 9.2L - Jobs Lab"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "437e758b-733c-4abd-adf5-75e5febb2478"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ../../Includes/Classroom-Setup-09.2.4L"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"# Exploring the Results of a DLT Pipeline\n",
							"\n",
							"Run the following cell to enumerate the output of your storage location:"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"files = dbutils.fs.ls(f\"{DA.paths.working_dir}/storage\")\n",
							"display(files)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"The **system** directory captures events associated with the pipeline."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"files = dbutils.fs.ls(f\"{DA.paths.working_dir}/storage/system/events\")\n",
							"display(files)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"These event logs are stored as a Delta table. \n",
							"\n",
							"Let's query the table."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"SELECT * FROM delta.`${da.paths.working_dir}/storage/system/events`"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Let's view the contents of the *tables* directory."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"files = dbutils.fs.ls(f\"{DA.paths.working_dir}/storage/tables\")\n",
							"display(files)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"\n",
							"Let's query the gold table."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"SELECT * FROM ${da.db_name}.daily_patient_avg"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA.cleanup()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Print-Dataset-Copyrights')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "f9d142fa-de68-4865-a5a4-6e35ac360041"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# DA = DBAcademyHelper(**helper_arguments)\n",
							"DA = DBAcademyHelper(**helper_arguments)\n",
							"DA.init(install_datasets=True, create_db=False)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA.print_copyrights()\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Reset')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b2d17583-9db6-4fd8-bd10-2cf09e62297b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments)\n",
							"DA.init(install_datasets=False, create_db=False)\n",
							"\n",
							"DA.cleanup_databases()    # Remove any databases created by this course\n",
							"DA.cleanup_working_dir()  # Remove any files created in the workspace\n",
							"DA.cleanup_datasets()     # Remove the local datasets forcing a reinstall\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Workspace-Setup')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "78a87998-eefa-4030-9d21-cf4b999cfd58"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
							"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
							"</div>"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"# Workspace Setup\n",
							"This notebook should be run by instructors to prepare the workspace for a class.\n",
							"\n",
							"The key changes this notebook makes includes:\n",
							"* Updating user-specific grants such that they can create databases/schemas against the current catalog when they are not workspace-admins.\n",
							"* Configures three cluster policies:\n",
							"    * **Student's All-Purpose Policy** - which should be used on clusters running standard notebooks.\n",
							"    * **Student's Jobs-Only Policy** - which should be used on workflows/jobs\n",
							"    * **Student's DLT-Only Policy** - which should be used on DLT piplines (automatically applied)\n",
							"* Create or update the shared **Starter Warehouse** for use in Databricks SQL exercises\n",
							"* Create the Instance Pool **Student's Pool** for use by students and the \"student\" and \"jobs\" policies."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%run ./_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Get Class Config\n",
							"The three variables defined by these widgets are used to configure our environment as a means of controlling class cost."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Setup the widgets to collect required parameters.\n",
							"from dbacademy_helper.workspace_helper import ALL_USERS # no other option for this course\n",
							"dbutils.widgets.dropdown(\"configure_for\", ALL_USERS, [ALL_USERS], \"Configure Workspace For\")\n",
							"\n",
							"# students_count is the reasonable estiamte to the maximum number of students\n",
							"dbutils.widgets.text(\"students_count\", \"\", \"Number of Students\")\n",
							"\n",
							"# event_name is the name assigned to this event/class or alternatively its class number\n",
							"dbutils.widgets.text(\"event_name\", \"\", \"Event Name/Class Number\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"# Init Script & Install Datasets\n",
							"The main affect of this call is to pre-install the datasets.\n",
							"\n",
							"It has the side effect of create our DA object which includes our REST client."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments) # Create the DA object\n",
							"DA.reset_environment()                   # Reset by removing databases and files from other lessons\n",
							"DA.init(install_datasets=True,           # Initialize, install and validate the datasets\n",
							"        create_db=False)                 # Continue initialization, create the user-db\n",
							"DA.conclude_setup()                      # Conclude setup by advertising environmental changes"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"## Create Class Instance Pools\n",
							"The following cell configures the instance pool used for this class"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"instance_pool_id = DA.workspace.clusters.create_instance_pools()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"## Create The Three Class-Specific Cluster Policies\n",
							"The following cells create the various cluster policies used by the class"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.workspace.clusters.create_all_purpose_policy(instance_pool_id)\n",
							"DA.workspace.clusters.create_jobs_policy(instance_pool_id)\n",
							"DA.workspace.clusters.create_dlt_policy(instance_pool_id)\n",
							"None"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"## Create Class-Shared Databricks SQL Warehouse/Endpoint\n",
							"Creates a single wharehouse to be used by all students.\n",
							"\n",
							"The configuration is derived from the number of students specified above."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.workspace.warehouses.create_shared_sql_warehouse()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"## Configure User Entitlements\n",
							"\n",
							"This task simply adds the \"**databricks-sql-access**\" entitlement to the \"**users**\" group ensuring that they can access the Databricks SQL view."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"DA.workspace.add_entitlement_workspace_access()\n",
							"DA.workspace.add_entitlement_databricks_sql_access()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"\n",
							"## Update Grants\n",
							"This operation executes **`GRANT CREATE ON CATALOG TO users`** to ensure that students can create databases as required by this course when they are not admins.\n",
							"\n",
							"Note: The implementation requires this to execute in another job and as such can take about three minutes to complete."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Ensures that all users can create databases on the current catalog \n",
							"# for cases wherein the user/student is not an admin.\n",
							"job_id = DA.workspace.databases.configure_permissions(\"Configure-Permissions\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA.client.jobs().delete_by_id(job_id)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA.setup_completed()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
							"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
							"<br/>\n",
							"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/_remote_files')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "6c8d4084-605e-4e3a-8a30-80eeca82add9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"remote_files = [\"/ecommerce/\", \"/ecommerce/README.md\", \"/ecommerce/delta/\", \"/ecommerce/delta/clickpaths/\", \"/ecommerce/delta/clickpaths/_delta_log/\", \"/ecommerce/delta/clickpaths/_delta_log/.s3-optimization-0\", \"/ecommerce/delta/clickpaths/_delta_log/.s3-optimization-1\", \"/ecommerce/delta/clickpaths/_delta_log/.s3-optimization-2\", \"/ecommerce/delta/clickpaths/_delta_log/00000000000000000000.crc\", \"/ecommerce/delta/clickpaths/_delta_log/00000000000000000000.json\", \"/ecommerce/delta/clickpaths/_delta_log/00000000000000000001.crc\", \"/ecommerce/delta/clickpaths/_delta_log/00000000000000000001.json\", \"/ecommerce/delta/clickpaths/part-00000-0337a7e6-2db7-4ac9-b087-8710b1b87e65-c000.snappy.parquet\", \"/ecommerce/delta/clickpaths/part-00000-70d28e19-29d1-4630-83f7-263f381dd4b2-c000.snappy.parquet\", \"/ecommerce/delta/clickpaths/part-00000-93b217c6-0f9e-4afe-83f1-118ef1460534-c000.snappy.parquet\", \"/ecommerce/delta/events/\", \"/ecommerce/delta/events/_delta_log/\", \"/ecommerce/delta/events/_delta_log/.s3-optimization-0\", \"/ecommerce/delta/events/_delta_log/.s3-optimization-1\", \"/ecommerce/delta/events/_delta_log/.s3-optimization-2\", \"/ecommerce/delta/events/_delta_log/00000000000000000000.crc\", \"/ecommerce/delta/events/_delta_log/00000000000000000000.json\", \"/ecommerce/delta/events/_delta_log/00000000000000000001.crc\", \"/ecommerce/delta/events/_delta_log/00000000000000000001.json\", \"/ecommerce/delta/events/part-00000-6841f2e7-d92f-406b-abbf-f9110aec7e14-c000.snappy.parquet\", \"/ecommerce/delta/events/part-00000-c93338ab-b7de-413a-97bc-2ef21fc42096-c000.snappy.parquet\", \"/ecommerce/delta/events/part-00000-f7a71761-b4e6-4209-a86b-c3b3ef379c0f-c000.snappy.parquet\", \"/ecommerce/delta/events_hist/\", \"/ecommerce/delta/events_hist/_delta_log/\", \"/ecommerce/delta/events_hist/_delta_log/.s3-optimization-0\", \"/ecommerce/delta/events_hist/_delta_log/.s3-optimization-1\", \"/ecommerce/delta/events_hist/_delta_log/.s3-optimization-2\", \"/ecommerce/delta/events_hist/_delta_log/00000000000000000000.crc\", \"/ecommerce/delta/events_hist/_delta_log/00000000000000000000.json\", \"/ecommerce/delta/events_hist/_delta_log/00000000000000000001.crc\", \"/ecommerce/delta/events_hist/_delta_log/00000000000000000001.json\", \"/ecommerce/delta/events_hist/part-00000-98bd00c6-5f72-48bb-a1df-890e719630f3-c000.snappy.parquet\", \"/ecommerce/delta/events_hist/part-00000-9d919c90-1ba1-495b-a6a6-a873c6168335-c000.snappy.parquet\", \"/ecommerce/delta/events_hist/part-00000-e0531e7b-f3b7-49c5-95a0-72e5ee5a3937-c000.snappy.parquet\", \"/ecommerce/delta/events_raw/\", \"/ecommerce/delta/events_raw/_delta_log/\", \"/ecommerce/delta/events_raw/_delta_log/.s3-optimization-0\", \"/ecommerce/delta/events_raw/_delta_log/.s3-optimization-1\", \"/ecommerce/delta/events_raw/_delta_log/.s3-optimization-2\", \"/ecommerce/delta/events_raw/_delta_log/00000000000000000000.crc\", \"/ecommerce/delta/events_raw/_delta_log/00000000000000000000.json\", \"/ecommerce/delta/events_raw/_delta_log/00000000000000000001.crc\", \"/ecommerce/delta/events_raw/_delta_log/00000000000000000001.json\", \"/ecommerce/delta/events_raw/part-00000-59dc39c3-23d2-4b58-a6b8-82455f9674f0-c000.snappy.parquet\", \"/ecommerce/delta/events_update/\", \"/ecommerce/delta/events_update/_delta_log/\", \"/ecommerce/delta/events_update/_delta_log/.s3-optimization-0\", \"/ecommerce/delta/events_update/_delta_log/.s3-optimization-1\", \"/ecommerce/delta/events_update/_delta_log/.s3-optimization-2\", \"/ecommerce/delta/events_update/_delta_log/00000000000000000000.crc\", \"/ecommerce/delta/events_update/_delta_log/00000000000000000000.json\", \"/ecommerce/delta/events_update/_delta_log/00000000000000000001.crc\", \"/ecommerce/delta/events_update/_delta_log/00000000000000000001.json\", \"/ecommerce/delta/events_update/part-00000-70b394f0-2afc-47c3-b491-2761cd8bfea8-c000.snappy.parquet\", \"/ecommerce/delta/events_update/part-00000-743be31f-a839-469b-a62e-4d335c751281-c000.snappy.parquet\", \"/ecommerce/delta/events_update/part-00000-f39f7494-b5d1-4c2b-8a46-d32de434f03e-c000.snappy.parquet\", \"/ecommerce/delta/item_lookup/\", \"/ecommerce/delta/item_lookup/_delta_log/\", \"/ecommerce/delta/item_lookup/_delta_log/.s3-optimization-0\", \"/ecommerce/delta/item_lookup/_delta_log/.s3-optimization-1\", \"/ecommerce/delta/item_lookup/_delta_log/.s3-optimization-2\", \"/ecommerce/delta/item_lookup/_delta_log/00000000000000000000.crc\", \"/ecommerce/delta/item_lookup/_delta_log/00000000000000000000.json\", \"/ecommerce/delta/item_lookup/part-00000-7fdc003c-7137-40a6-85e2-47bdbce39c6d-c000.snappy.parquet\", \"/ecommerce/delta/item_lookup/part-00001-f426f81c-02dd-4ce5-aab8-bf55f32073f7-c000.snappy.parquet\", \"/ecommerce/delta/item_lookup/part-00002-a31427cf-d560-4f21-acfd-84674c71a819-c000.snappy.parquet\", \"/ecommerce/delta/item_lookup/part-00003-8ed5ca81-8d5b-49dd-8abf-15e4af10d85a-c000.snappy.parquet\", \"/ecommerce/delta/sales/\", \"/ecommerce/delta/sales/_delta_log/\", \"/ecommerce/delta/sales/_delta_log/.s3-optimization-0\", \"/ecommerce/delta/sales/_delta_log/.s3-optimization-1\", \"/ecommerce/delta/sales/_delta_log/.s3-optimization-2\", \"/ecommerce/delta/sales/_delta_log/00000000000000000000.crc\", \"/ecommerce/delta/sales/_delta_log/00000000000000000000.json\", \"/ecommerce/delta/sales/_delta_log/00000000000000000001.crc\", \"/ecommerce/delta/sales/_delta_log/00000000000000000001.json\", \"/ecommerce/delta/sales/part-00000-23d9c9bc-3585-4996-9894-d3f3891a6937-c000.snappy.parquet\", \"/ecommerce/delta/sales/part-00000-952ddf1f-aa41-4bfa-959c-1d84feb6bd95-c000.snappy.parquet\", \"/ecommerce/delta/sales/part-00000-eb64a6d8-3b60-4257-9488-4863895ef390-c000.snappy.parquet\", \"/ecommerce/delta/sales_hist/\", \"/ecommerce/delta/sales_hist/_delta_log/\", \"/ecommerce/delta/sales_hist/_delta_log/.s3-optimization-0\", \"/ecommerce/delta/sales_hist/_delta_log/.s3-optimization-1\", \"/ecommerce/delta/sales_hist/_delta_log/.s3-optimization-2\", \"/ecommerce/delta/sales_hist/_delta_log/00000000000000000000.crc\", \"/ecommerce/delta/sales_hist/_delta_log/00000000000000000000.json\", \"/ecommerce/delta/sales_hist/_delta_log/00000000000000000001.crc\", \"/ecommerce/delta/sales_hist/_delta_log/00000000000000000001.json\", \"/ecommerce/delta/sales_hist/part-00000-271c7ee7-fd1b-4c42-b55a-3c3e9af1d7ec-c000.snappy.parquet\", \"/ecommerce/delta/sales_hist/part-00000-3ecfcbb1-f6f5-4297-a3d5-41f83e1dbfbc-c000.snappy.parquet\", \"/ecommerce/delta/sales_hist/part-00000-dd20e0ac-54f4-40e4-b36c-6b93a7c7510a-c000.snappy.parquet\", \"/ecommerce/delta/transactions/\", \"/ecommerce/delta/transactions/_delta_log/\", \"/ecommerce/delta/transactions/_delta_log/.s3-optimization-0\", \"/ecommerce/delta/transactions/_delta_log/.s3-optimization-1\", \"/ecommerce/delta/transactions/_delta_log/.s3-optimization-2\", \"/ecommerce/delta/transactions/_delta_log/00000000000000000000.crc\", \"/ecommerce/delta/transactions/_delta_log/00000000000000000000.json\", \"/ecommerce/delta/transactions/_delta_log/00000000000000000001.crc\", \"/ecommerce/delta/transactions/_delta_log/00000000000000000001.json\", \"/ecommerce/delta/transactions/part-00000-94144360-0635-4a96-b128-7c44125367e4-c000.snappy.parquet\", \"/ecommerce/delta/users/\", \"/ecommerce/delta/users/_delta_log/\", \"/ecommerce/delta/users/_delta_log/.s3-optimization-0\", \"/ecommerce/delta/users/_delta_log/.s3-optimization-1\", \"/ecommerce/delta/users/_delta_log/.s3-optimization-2\", \"/ecommerce/delta/users/_delta_log/00000000000000000000.crc\", \"/ecommerce/delta/users/_delta_log/00000000000000000000.json\", \"/ecommerce/delta/users/_delta_log/00000000000000000001.crc\", \"/ecommerce/delta/users/_delta_log/00000000000000000001.json\", \"/ecommerce/delta/users/part-00000-120d779b-e65e-47ec-a6fc-78e3c3414714-c000.snappy.parquet\", \"/ecommerce/delta/users_hist/\", \"/ecommerce/delta/users_hist/_delta_log/\", \"/ecommerce/delta/users_hist/_delta_log/.s3-optimization-0\", \"/ecommerce/delta/users_hist/_delta_log/.s3-optimization-1\", \"/ecommerce/delta/users_hist/_delta_log/.s3-optimization-2\", \"/ecommerce/delta/users_hist/_delta_log/00000000000000000000.crc\", \"/ecommerce/delta/users_hist/_delta_log/00000000000000000000.json\", \"/ecommerce/delta/users_hist/_delta_log/00000000000000000001.crc\", \"/ecommerce/delta/users_hist/_delta_log/00000000000000000001.json\", \"/ecommerce/delta/users_hist/part-00000-39b04022-8452-4cd1-a36f-de4ff26db24d-c000.snappy.parquet\", \"/ecommerce/delta/users_update/\", \"/ecommerce/delta/users_update/_delta_log/\", \"/ecommerce/delta/users_update/_delta_log/.s3-optimization-0\", \"/ecommerce/delta/users_update/_delta_log/.s3-optimization-1\", \"/ecommerce/delta/users_update/_delta_log/.s3-optimization-2\", \"/ecommerce/delta/users_update/_delta_log/00000000000000000000.crc\", \"/ecommerce/delta/users_update/_delta_log/00000000000000000000.json\", \"/ecommerce/delta/users_update/_delta_log/00000000000000000001.crc\", \"/ecommerce/delta/users_update/_delta_log/00000000000000000001.json\", \"/ecommerce/delta/users_update/part-00000-cd8a3013-62e4-4876-837b-8bb161a3111d-c000.snappy.parquet\", \"/ecommerce/raw/\", \"/ecommerce/raw/events-historical/\", \"/ecommerce/raw/events-historical/_SUCCESS\", \"/ecommerce/raw/events-historical/_committed_2791280217547372541\", \"/ecommerce/raw/events-historical/_committed_5148781236890632344\", \"/ecommerce/raw/events-historical/_committed_7228901775923393996\", \"/ecommerce/raw/events-historical/_committed_7345950505771384516\", \"/ecommerce/raw/events-historical/_committed_860462176816979339\", \"/ecommerce/raw/events-historical/_committed_vacuum7162366710152874219\", \"/ecommerce/raw/events-historical/_started_5148781236890632344\", \"/ecommerce/raw/events-historical/_started_7228901775923393996\", \"/ecommerce/raw/events-historical/_started_7345950505771384516\", \"/ecommerce/raw/events-historical/_started_860462176816979339\", \"/ecommerce/raw/events-historical/part-00000-tid-7228901775923393996-ef636a8a-296c-44e6-93d5-b6b52cb67adb-10-1-c000.snappy.parquet\", \"/ecommerce/raw/events-historical/part-00000-tid-7345950505771384516-67ae00f3-c8fe-4471-bc58-99042a3f0679-5895-1-c000.snappy.parquet\", \"/ecommerce/raw/events-historical/part-00001-tid-7228901775923393996-ef636a8a-296c-44e6-93d5-b6b52cb67adb-11-1-c000.snappy.parquet\", \"/ecommerce/raw/events-historical/part-00001-tid-7345950505771384516-67ae00f3-c8fe-4471-bc58-99042a3f0679-5896-1-c000.snappy.parquet\", \"/ecommerce/raw/events-historical/part-00002-tid-7228901775923393996-ef636a8a-296c-44e6-93d5-b6b52cb67adb-12-1-c000.snappy.parquet\", \"/ecommerce/raw/events-historical/part-00002-tid-7345950505771384516-67ae00f3-c8fe-4471-bc58-99042a3f0679-5897-1-c000.snappy.parquet\", \"/ecommerce/raw/events-historical/part-00003-tid-7228901775923393996-ef636a8a-296c-44e6-93d5-b6b52cb67adb-13-1-c000.snappy.parquet\", \"/ecommerce/raw/events-historical/part-00003-tid-7345950505771384516-67ae00f3-c8fe-4471-bc58-99042a3f0679-5898-1-c000.snappy.parquet\", \"/ecommerce/raw/events-kafka/\", \"/ecommerce/raw/events-kafka/000.json\", \"/ecommerce/raw/events-kafka/001.json\", \"/ecommerce/raw/events-kafka/002.json\", \"/ecommerce/raw/events-kafka/003.json\", \"/ecommerce/raw/events-kafka/004.json\", \"/ecommerce/raw/events-kafka/005.json\", \"/ecommerce/raw/events-kafka/006.json\", \"/ecommerce/raw/events-kafka/007.json\", \"/ecommerce/raw/events-kafka/008.json\", \"/ecommerce/raw/events-kafka/009.json\", \"/ecommerce/raw/events-kafka/010.json\", \"/ecommerce/raw/item-lookup/\", \"/ecommerce/raw/item-lookup/_SUCCESS\", \"/ecommerce/raw/item-lookup/_committed_2654461418592852096\", \"/ecommerce/raw/item-lookup/_started_2654461418592852096\", \"/ecommerce/raw/item-lookup/part-00000-tid-2654461418592852096-b933f2da-5dea-4d2d-b0ef-c9bac0905ab0-7615-1-c000.snappy.parquet\", \"/ecommerce/raw/item-lookup/part-00001-tid-2654461418592852096-b933f2da-5dea-4d2d-b0ef-c9bac0905ab0-7616-1-c000.snappy.parquet\", \"/ecommerce/raw/item-lookup/part-00002-tid-2654461418592852096-b933f2da-5dea-4d2d-b0ef-c9bac0905ab0-7617-1-c000.snappy.parquet\", \"/ecommerce/raw/item-lookup/part-00003-tid-2654461418592852096-b933f2da-5dea-4d2d-b0ef-c9bac0905ab0-7618-1-c000.snappy.parquet\", \"/ecommerce/raw/sales-30m/\", \"/ecommerce/raw/sales-30m/_SUCCESS\", \"/ecommerce/raw/sales-30m/_committed_4198604805198165031\", \"/ecommerce/raw/sales-30m/_committed_5364580745385244803\", \"/ecommerce/raw/sales-30m/_committed_5484765126949525607\", \"/ecommerce/raw/sales-30m/_committed_6144999417711249384\", \"/ecommerce/raw/sales-30m/_committed_9074289989443701117\", \"/ecommerce/raw/sales-30m/_committed_vacuum7490578173802527130\", \"/ecommerce/raw/sales-30m/_started_4198604805198165031\", \"/ecommerce/raw/sales-30m/_started_5364580745385244803\", \"/ecommerce/raw/sales-30m/_started_5484765126949525607\", \"/ecommerce/raw/sales-30m/_started_9074289989443701117\", \"/ecommerce/raw/sales-30m/part-00000-tid-5364580745385244803-e6225119-ab40-4518-af4a-6f7a43fa54ce-16-1-c000.snappy.parquet\", \"/ecommerce/raw/sales-30m/part-00000-tid-9074289989443701117-f9b615ab-8207-4281-8383-d6be3dc9f910-5900-1-c000.snappy.parquet\", \"/ecommerce/raw/sales-30m/part-00001-tid-5364580745385244803-e6225119-ab40-4518-af4a-6f7a43fa54ce-17-1-c000.snappy.parquet\", \"/ecommerce/raw/sales-30m/part-00001-tid-9074289989443701117-f9b615ab-8207-4281-8383-d6be3dc9f910-5901-1-c000.snappy.parquet\", \"/ecommerce/raw/sales-30m/part-00002-tid-5364580745385244803-e6225119-ab40-4518-af4a-6f7a43fa54ce-18-1-c000.snappy.parquet\", \"/ecommerce/raw/sales-30m/part-00002-tid-9074289989443701117-f9b615ab-8207-4281-8383-d6be3dc9f910-5902-1-c000.snappy.parquet\", \"/ecommerce/raw/sales-30m/part-00003-tid-5364580745385244803-e6225119-ab40-4518-af4a-6f7a43fa54ce-19-1-c000.snappy.parquet\", \"/ecommerce/raw/sales-30m/part-00003-tid-9074289989443701117-f9b615ab-8207-4281-8383-d6be3dc9f910-5903-1-c000.snappy.parquet\", \"/ecommerce/raw/sales-csv/\", \"/ecommerce/raw/sales-csv/000.csv\", \"/ecommerce/raw/sales-csv/001.csv\", \"/ecommerce/raw/sales-csv/002.csv\", \"/ecommerce/raw/sales-csv/003.csv\", \"/ecommerce/raw/sales-historical/\", \"/ecommerce/raw/sales-historical/_SUCCESS\", \"/ecommerce/raw/sales-historical/_committed_115710843736190548\", \"/ecommerce/raw/sales-historical/_committed_1668519538481149696\", \"/ecommerce/raw/sales-historical/_committed_4575620701854220343\", \"/ecommerce/raw/sales-historical/_committed_646693144485676518\", \"/ecommerce/raw/sales-historical/_committed_6786832248242696253\", \"/ecommerce/raw/sales-historical/_committed_vacuum6912201994486791117\", \"/ecommerce/raw/sales-historical/_started_115710843736190548\", \"/ecommerce/raw/sales-historical/_started_4575620701854220343\", \"/ecommerce/raw/sales-historical/_started_646693144485676518\", \"/ecommerce/raw/sales-historical/_started_6786832248242696253\", \"/ecommerce/raw/sales-historical/part-00000-tid-115710843736190548-19f23c58-6d33-488c-97f1-0cf6f5638a33-5905-1-c000.snappy.parquet\", \"/ecommerce/raw/sales-historical/part-00000-tid-6786832248242696253-ddac04b5-56f2-4641-81c0-2ca457594ddb-22-1-c000.snappy.parquet\", \"/ecommerce/raw/sales-historical/part-00001-tid-115710843736190548-19f23c58-6d33-488c-97f1-0cf6f5638a33-5906-1-c000.snappy.parquet\", \"/ecommerce/raw/sales-historical/part-00001-tid-6786832248242696253-ddac04b5-56f2-4641-81c0-2ca457594ddb-23-1-c000.snappy.parquet\", \"/ecommerce/raw/sales-historical/part-00002-tid-115710843736190548-19f23c58-6d33-488c-97f1-0cf6f5638a33-5907-1-c000.snappy.parquet\", \"/ecommerce/raw/sales-historical/part-00002-tid-6786832248242696253-ddac04b5-56f2-4641-81c0-2ca457594ddb-24-1-c000.snappy.parquet\", \"/ecommerce/raw/sales-historical/part-00003-tid-115710843736190548-19f23c58-6d33-488c-97f1-0cf6f5638a33-5908-1-c000.snappy.parquet\", \"/ecommerce/raw/sales-historical/part-00003-tid-6786832248242696253-ddac04b5-56f2-4641-81c0-2ca457594ddb-25-1-c000.snappy.parquet\", \"/ecommerce/raw/users-30m/\", \"/ecommerce/raw/users-30m/_SUCCESS\", \"/ecommerce/raw/users-30m/_committed_1642804274388629440\", \"/ecommerce/raw/users-30m/_started_1642804274388629440\", \"/ecommerce/raw/users-30m/part-00000-tid-1642804274388629440-4ef42c1a-d5e2-4534-a526-620366f00b43-7583-1-c000.snappy.parquet\", \"/ecommerce/raw/users-30m/part-00001-tid-1642804274388629440-4ef42c1a-d5e2-4534-a526-620366f00b43-7584-1-c000.snappy.parquet\", \"/ecommerce/raw/users-30m/part-00002-tid-1642804274388629440-4ef42c1a-d5e2-4534-a526-620366f00b43-7585-1-c000.snappy.parquet\", \"/ecommerce/raw/users-30m/part-00003-tid-1642804274388629440-4ef42c1a-d5e2-4534-a526-620366f00b43-7586-1-c000.snappy.parquet\", \"/ecommerce/raw/users-historical/\", \"/ecommerce/raw/users-historical/_SUCCESS\", \"/ecommerce/raw/users-historical/_committed_531959640415905750\", \"/ecommerce/raw/users-historical/_started_531959640415905750\", \"/ecommerce/raw/users-historical/part-00000-tid-531959640415905750-948b4f2d-2d35-46e3-97eb-e6d85d2bf872-7571-1-c000.snappy.parquet\", \"/ecommerce/raw/users-historical/part-00001-tid-531959640415905750-948b4f2d-2d35-46e3-97eb-e6d85d2bf872-7572-1-c000.snappy.parquet\", \"/ecommerce/raw/users-historical/part-00002-tid-531959640415905750-948b4f2d-2d35-46e3-97eb-e6d85d2bf872-7573-1-c000.snappy.parquet\", \"/ecommerce/raw/users-historical/part-00003-tid-531959640415905750-948b4f2d-2d35-46e3-97eb-e6d85d2bf872-7574-1-c000.snappy.parquet\", \"/flights/\", \"/flights/README.md\", \"/flights/airport-codes-na.txt\", \"/flights/departuredelays.csv\", \"/healthcare/\", \"/healthcare/README.md\", \"/healthcare/patient/\", \"/healthcare/patient/patient_info.csv\", \"/healthcare/tracker-2/\", \"/healthcare/tracker-2/agg_data.csv\", \"/healthcare/tracker-2/classic_data_2020_h1.snappy.parquet\", \"/healthcare/tracker-2/health_profile_data.snappy.parquet\", \"/healthcare/tracker-2/health_tracker_data_2020_1.json\", \"/healthcare/tracker-2/health_tracker_data_2020_2.json\", \"/healthcare/tracker-2/health_tracker_data_2020_2_late.json\", \"/healthcare/tracker-2/health_tracker_data_2020_3.json\", \"/healthcare/tracker-2/health_tracker_data_2020_4.json\", \"/healthcare/tracker-2/health_tracker_data_2020_5.json\", \"/healthcare/tracker-2/user-profile-data.json\", \"/healthcare/tracker-2/user_data.csv\", \"/healthcare/tracker-2/user_profile_data.snappy.parquet\", \"/healthcare/tracker/\", \"/healthcare/tracker/agg_data.csv\", \"/healthcare/tracker/classic_data_2020_h1.snappy.parquet\", \"/healthcare/tracker/etl/\", \"/healthcare/tracker/etl/2020_01.json\", \"/healthcare/tracker/etl/2020_02-01.json\", \"/healthcare/tracker/etl/2020_02.json\", \"/healthcare/tracker/health_profile_data.snappy.parquet\", \"/healthcare/tracker/moocs/\", \"/healthcare/tracker/moocs/daily-metrics.csv\", \"/healthcare/tracker/moocs/users.csv\", \"/healthcare/tracker/raw-alternate-schemas.json/\", \"/healthcare/tracker/raw-alternate-schemas.json/health_tracker_data_2020_new_schema_col.json\", \"/healthcare/tracker/raw-alternate-schemas.json/health_tracker_data_2020_new_schema_type.json\", \"/healthcare/tracker/raw-alternate-schemas.json/health_tracker_data_2020_old_schema.json\", \"/healthcare/tracker/raw-alternate-schemas.json/opt_out.json\", \"/healthcare/tracker/raw-late.json/\", \"/healthcare/tracker/raw-late.json/health_tracker_data_2020_2_late.json\", \"/healthcare/tracker/raw-missing.json/\", \"/healthcare/tracker/raw-missing.json/health_tracker_data_2020_7_missing.json\", \"/healthcare/tracker/raw.json/\", \"/healthcare/tracker/raw.json/health_tracker_data_2020_1.json\", \"/healthcare/tracker/raw.json/health_tracker_data_2020_2.json\", \"/healthcare/tracker/raw.json/health_tracker_data_2020_3.json\", \"/healthcare/tracker/raw.json/health_tracker_data_2020_4.json\", \"/healthcare/tracker/raw.json/health_tracker_data_2020_5.json\", \"/healthcare/tracker/streaming/\", \"/healthcare/tracker/streaming/01.json\", \"/healthcare/tracker/streaming/02.json\", \"/healthcare/tracker/streaming/03.json\", \"/healthcare/tracker/streaming/04.json\", \"/healthcare/tracker/streaming/05.json\", \"/healthcare/tracker/streaming/06.json\", \"/healthcare/tracker/streaming/07.json\", \"/healthcare/tracker/streaming/08.json\", \"/healthcare/tracker/streaming/09.json\", \"/healthcare/tracker/streaming/10.json\", \"/healthcare/tracker/streaming/11.json\", \"/healthcare/tracker/streaming/12.json\", \"/healthcare/tracker/user_data/\", \"/healthcare/tracker/user_data/user_data.csv/\", \"/healthcare/tracker/user_data/user_data.csv/user_data.csv\", \"/healthcare/tracker/user_data/user_data.json/\", \"/healthcare/tracker/user_data/user_data.json/user-profile-data.json\", \"/healthcare/tracker/user_data/user_data.parquet/\", \"/healthcare/tracker/user_data/user_data.parquet/user_profile_data.snappy.parquet\", \"/nyctaxi-with-zipcodes/\", \"/nyctaxi-with-zipcodes/README.md\", \"/nyctaxi-with-zipcodes/data/\", \"/nyctaxi-with-zipcodes/data/_delta_log/\", \"/nyctaxi-with-zipcodes/data/_delta_log/.s3-optimization-0\", \"/nyctaxi-with-zipcodes/data/_delta_log/.s3-optimization-1\", \"/nyctaxi-with-zipcodes/data/_delta_log/.s3-optimization-2\", \"/nyctaxi-with-zipcodes/data/_delta_log/00000000000000000000.crc\", \"/nyctaxi-with-zipcodes/data/_delta_log/00000000000000000000.json\", \"/nyctaxi-with-zipcodes/data/part-00000-80b68cae-ce6a-41cf-87cd-2573d91b4c07-c000.snappy.parquet\", \"/nyctaxi-with-zipcodes/data/part-00001-c883942d-366f-478a-be3b-f13fd4bee0ab-c000.snappy.parquet\", \"/nyctaxi-with-zipcodes/data/part-00002-bbf9fd81-4b3a-46f3-943e-841b48ae743e-c000.snappy.parquet\", \"/nyctaxi-with-zipcodes/data/part-00003-3d80435e-15f8-4154-92c7-515307e41c1b-c000.snappy.parquet\", \"/nyctaxi-with-zipcodes/data/part-00004-0b996b45-a3ff-4339-afeb-8fc691770056-c000.snappy.parquet\", \"/nyctaxi-with-zipcodes/data/part-00005-ec9ab51b-23a3-4333-8d42-1730df56bfb6-c000.snappy.parquet\", \"/retail-org/\", \"/retail-org/README.md\", \"/retail-org/active_promotions/\", \"/retail-org/active_promotions/active_promotions.parquet\", \"/retail-org/company_employees/\", \"/retail-org/company_employees/company_employees.csv\", \"/retail-org/customers/\", \"/retail-org/customers/customers.csv\", \"/retail-org/loyalty_segments/\", \"/retail-org/loyalty_segments/loyalty_segment.csv\", \"/retail-org/products/\", \"/retail-org/products/products.csv\", \"/retail-org/promotions/\", \"/retail-org/promotions/promotions.csv\", \"/retail-org/purchase_orders/\", \"/retail-org/purchase_orders/purchase_orders.xml\", \"/retail-org/sales_orders/\", \"/retail-org/sales_orders/_SUCCESS\", \"/retail-org/sales_orders/_committed_1771549084454148016\", \"/retail-org/sales_orders/_started_1771549084454148016\", \"/retail-org/sales_orders/part-00000-tid-1771549084454148016-e2275afd-a5bb-40ed-b044-1774c0fdab2b-105592-1-c000.json\", \"/retail-org/sales_stream/\", \"/retail-org/sales_stream/sales_stream.json/\", \"/retail-org/sales_stream/sales_stream.json/12-00.json\", \"/retail-org/sales_stream/sales_stream.json/12-05.json\", \"/retail-org/sales_stream/sales_stream.json/12-10.json\", \"/retail-org/sales_stream/sales_stream.json/12-15.json\", \"/retail-org/sales_stream/sales_stream.json/12-25.json\", \"/retail-org/sales_stream/sales_stream.json/12-35.json\", \"/retail-org/sales_stream/sales_stream.json/12-40.json\", \"/retail-org/sales_stream/sales_stream.json/12-45.json\", \"/retail-org/sales_stream/sales_stream.json/13-00.json\", \"/retail-org/sales_stream/sales_stream.json/13-15.json\", \"/retail-org/sales_stream/sales_stream.json/13-20.json\", \"/retail-org/sales_stream/sales_stream.json/13-25.json\", \"/retail-org/sales_stream/sales_stream.json/13-30.json\", \"/retail-org/sales_stream/sales_stream.json/13-35.json\", \"/retail-org/sales_stream/sales_stream.json/13-40.json\", \"/retail-org/sales_stream/sales_stream.json/13-50.json\", \"/retail-org/sales_stream/sales_stream.json/13-55.json\", \"/retail-org/sales_stream/sales_stream.json/14-00.json\", \"/retail-org/sales_stream/sales_stream.json/14-05.json\", \"/retail-org/sales_stream/sales_stream.json/14-15.json\", \"/retail-org/sales_stream/sales_stream.json/14-25.json\", \"/retail-org/sales_stream/sales_stream.json/14-30.json\", \"/retail-org/sales_stream/sales_stream.json/14-35.json\", \"/retail-org/sales_stream/sales_stream.json/14-40.json\", \"/retail-org/sales_stream/sales_stream.json/14-45.json\", \"/retail-org/sales_stream/sales_stream.json/14-55.json\", \"/retail-org/sales_stream/sales_stream.json/15-00.json\", \"/retail-org/sales_stream/sales_stream.json/15-05.json\", \"/retail-org/sales_stream/sales_stream.json/15-10.json\", \"/retail-org/sales_stream/sales_stream.json/15-25.json\", \"/retail-org/sales_stream/sales_stream.json/15-30.json\", \"/retail-org/sales_stream/sales_stream.json/15-35.json\", \"/retail-org/sales_stream/sales_stream.json/15-45.json\", \"/retail-org/sales_stream/sales_stream.json/15-55.json\", \"/retail-org/sales_stream/sales_stream.json/16-00.json\", \"/retail-org/sales_stream/sales_stream.json/16-05.json\", \"/retail-org/sales_stream/sales_stream.json/16-10.json\", \"/retail-org/sales_stream/sales_stream.json/16-15.json\", \"/retail-org/sales_stream/sales_stream.json/16-20.json\", \"/retail-org/sales_stream/sales_stream.json/16-25.json\", \"/retail-org/sales_stream/sales_stream.json/16-30.json\", \"/retail-org/sales_stream/sales_stream.json/16-35.json\", \"/retail-org/sales_stream/sales_stream.json/16-40.json\", \"/retail-org/sales_stream/sales_stream.json/16-50.json\", \"/retail-org/sales_stream/sales_stream.json/16-55.json\", \"/retail-org/solutions/\", \"/retail-org/solutions/bronze/\", \"/retail-org/solutions/bronze/active_promotions/\", \"/retail-org/solutions/bronze/active_promotions/_delta_log/\", \"/retail-org/solutions/bronze/active_promotions/_delta_log/.s3-optimization-0\", \"/retail-org/solutions/bronze/active_promotions/_delta_log/.s3-optimization-1\", \"/retail-org/solutions/bronze/active_promotions/_delta_log/.s3-optimization-2\", \"/retail-org/solutions/bronze/active_promotions/_delta_log/00000000000000000000.crc\", \"/retail-org/solutions/bronze/active_promotions/_delta_log/00000000000000000000.json\", \"/retail-org/solutions/bronze/active_promotions/part-00000-66841a9d-7450-4b16-909b-094f28f245eb-c000.snappy.parquet\", \"/retail-org/solutions/bronze/customers/\", \"/retail-org/solutions/bronze/customers/_delta_log/\", \"/retail-org/solutions/bronze/customers/_delta_log/.s3-optimization-0\", \"/retail-org/solutions/bronze/customers/_delta_log/.s3-optimization-1\", \"/retail-org/solutions/bronze/customers/_delta_log/.s3-optimization-2\", \"/retail-org/solutions/bronze/customers/_delta_log/00000000000000000000.crc\", \"/retail-org/solutions/bronze/customers/_delta_log/00000000000000000000.json\", \"/retail-org/solutions/bronze/customers/part-00000-db8f8abe-4cfd-4c0d-bd99-792bdc32004a-c000.snappy.parquet\", \"/retail-org/solutions/bronze/products/\", \"/retail-org/solutions/bronze/products/_delta_log/\", \"/retail-org/solutions/bronze/products/_delta_log/.s3-optimization-0\", \"/retail-org/solutions/bronze/products/_delta_log/.s3-optimization-1\", \"/retail-org/solutions/bronze/products/_delta_log/.s3-optimization-2\", \"/retail-org/solutions/bronze/products/_delta_log/00000000000000000000.crc\", \"/retail-org/solutions/bronze/products/_delta_log/00000000000000000000.json\", \"/retail-org/solutions/bronze/products/_delta_log/00000000000000000001.crc\", \"/retail-org/solutions/bronze/products/_delta_log/00000000000000000001.json\", \"/retail-org/solutions/bronze/products/_delta_log/00000000000000000002.crc\", \"/retail-org/solutions/bronze/products/_delta_log/00000000000000000002.json\", \"/retail-org/solutions/bronze/products/_delta_log/00000000000000000003.crc\", \"/retail-org/solutions/bronze/products/_delta_log/00000000000000000003.json\", \"/retail-org/solutions/bronze/products/_delta_log/00000000000000000004.crc\", \"/retail-org/solutions/bronze/products/_delta_log/00000000000000000004.json\", \"/retail-org/solutions/bronze/products/part-00000-77826561-f91b-4ecf-87f1-ea8698344641-c000.snappy.parquet\", \"/retail-org/solutions/bronze/products/part-00000-822480e0-924b-4bce-8049-6c2885a52cfa-c000.snappy.parquet\", \"/retail-org/solutions/bronze/products/part-00000-b873b452-1219-45b7-9806-33d71b0bdb91-c000.snappy.parquet\", \"/retail-org/solutions/bronze/products/part-00000-d5bb465a-0823-489f-b707-b10c9ec0b40d-c000.snappy.parquet\", \"/retail-org/solutions/bronze/products/part-00000-f7432c2c-68fc-4c38-93c2-b3634169d1b2-c000.snappy.parquet\", \"/retail-org/solutions/bronze/purchase_orders/\", \"/retail-org/solutions/bronze/purchase_orders/_delta_log/\", \"/retail-org/solutions/bronze/purchase_orders/_delta_log/.s3-optimization-0\", \"/retail-org/solutions/bronze/purchase_orders/_delta_log/.s3-optimization-1\", \"/retail-org/solutions/bronze/purchase_orders/_delta_log/.s3-optimization-2\", \"/retail-org/solutions/bronze/purchase_orders/_delta_log/00000000000000000000.crc\", \"/retail-org/solutions/bronze/purchase_orders/_delta_log/00000000000000000000.json\", \"/retail-org/solutions/bronze/purchase_orders/part-00000-33a8a060-5811-4e95-bbc1-244c3c4b38e6-c000.snappy.parquet\", \"/retail-org/solutions/bronze/sales_orders/\", \"/retail-org/solutions/bronze/sales_orders/_delta_log/\", \"/retail-org/solutions/bronze/sales_orders/_delta_log/.s3-optimization-0\", \"/retail-org/solutions/bronze/sales_orders/_delta_log/.s3-optimization-1\", \"/retail-org/solutions/bronze/sales_orders/_delta_log/.s3-optimization-2\", \"/retail-org/solutions/bronze/sales_orders/_delta_log/00000000000000000000.crc\", \"/retail-org/solutions/bronze/sales_orders/_delta_log/00000000000000000000.json\", \"/retail-org/solutions/bronze/sales_orders/part-00000-814353fb-139e-4395-a8e4-58654bf8e591-c000.snappy.parquet\", \"/retail-org/solutions/bronze/suppliers/\", \"/retail-org/solutions/bronze/suppliers/_delta_log/\", \"/retail-org/solutions/bronze/suppliers/_delta_log/.s3-optimization-0\", \"/retail-org/solutions/bronze/suppliers/_delta_log/.s3-optimization-1\", \"/retail-org/solutions/bronze/suppliers/_delta_log/.s3-optimization-2\", \"/retail-org/solutions/bronze/suppliers/_delta_log/00000000000000000000.crc\", \"/retail-org/solutions/bronze/suppliers/_delta_log/00000000000000000000.json\", \"/retail-org/solutions/bronze/suppliers/part-00000-d143db6a-a976-4ad8-a23d-c927b26f0234-c000.snappy.parquet\", \"/retail-org/solutions/gold/\", \"/retail-org/solutions/gold/sales/\", \"/retail-org/solutions/gold/sales/_delta_log/\", \"/retail-org/solutions/gold/sales/_delta_log/.s3-optimization-0\", \"/retail-org/solutions/gold/sales/_delta_log/.s3-optimization-1\", \"/retail-org/solutions/gold/sales/_delta_log/.s3-optimization-2\", \"/retail-org/solutions/gold/sales/_delta_log/00000000000000000000.crc\", \"/retail-org/solutions/gold/sales/_delta_log/00000000000000000000.json\", \"/retail-org/solutions/gold/sales/part-00000-0defb1b8-fa7f-4fdb-a3ae-2c89ee9bb1bb-c000.snappy.parquet\", \"/retail-org/solutions/sales_stream/\", \"/retail-org/solutions/sales_stream/12-05.json\", \"/retail-org/solutions/silver/\", \"/retail-org/solutions/silver/_SUCCESS\", \"/retail-org/solutions/silver/_committed_6853939106257238539\", \"/retail-org/solutions/silver/_started_6853939106257238539\", \"/retail-org/solutions/silver/goods_receipt/\", \"/retail-org/solutions/silver/goods_receipt/_delta_log/\", \"/retail-org/solutions/silver/goods_receipt/_delta_log/.s3-optimization-0\", \"/retail-org/solutions/silver/goods_receipt/_delta_log/.s3-optimization-1\", \"/retail-org/solutions/silver/goods_receipt/_delta_log/.s3-optimization-2\", \"/retail-org/solutions/silver/goods_receipt/_delta_log/00000000000000000000.crc\", \"/retail-org/solutions/silver/goods_receipt/_delta_log/00000000000000000000.json\", \"/retail-org/solutions/silver/goods_receipt/part-00000-9c53d70f-1e9c-4a8c-a122-2dfcc52c251d-c000.snappy.parquet\", \"/retail-org/solutions/silver/part-00000-tid-6853939106257238539-8da37d6d-79cd-4a86-8aca-cd904c0bca38-12-1-c000.snappy.parquet\", \"/retail-org/solutions/silver/promo_prices/\", \"/retail-org/solutions/silver/promo_prices/_delta_log/\", \"/retail-org/solutions/silver/promo_prices/_delta_log/.s3-optimization-0\", \"/retail-org/solutions/silver/promo_prices/_delta_log/.s3-optimization-1\", \"/retail-org/solutions/silver/promo_prices/_delta_log/.s3-optimization-2\", \"/retail-org/solutions/silver/promo_prices/_delta_log/00000000000000000000.crc\", \"/retail-org/solutions/silver/promo_prices/_delta_log/00000000000000000000.json\", \"/retail-org/solutions/silver/promo_prices/part-00000-5fa6c8e3-9eee-428f-ab1b-12d15b445985-c000.snappy.parquet\", \"/retail-org/solutions/silver/purchase_orders.delta/\", \"/retail-org/solutions/silver/purchase_orders.delta/_delta_log/\", \"/retail-org/solutions/silver/purchase_orders.delta/_delta_log/.s3-optimization-0\", \"/retail-org/solutions/silver/purchase_orders.delta/_delta_log/.s3-optimization-1\", \"/retail-org/solutions/silver/purchase_orders.delta/_delta_log/.s3-optimization-2\", \"/retail-org/solutions/silver/purchase_orders.delta/_delta_log/00000000000000000000.crc\", \"/retail-org/solutions/silver/purchase_orders.delta/_delta_log/00000000000000000000.json\", \"/retail-org/solutions/silver/purchase_orders.delta/part-00000-f1da016e-a0d6-461e-b01f-8580c5a0a1a0-c000.snappy.parquet\", \"/retail-org/solutions/silver/purchase_orders_cleansed.delta/\", \"/retail-org/solutions/silver/purchase_orders_cleansed.delta/_delta_log/\", \"/retail-org/solutions/silver/purchase_orders_cleansed.delta/_delta_log/.s3-optimization-0\", \"/retail-org/solutions/silver/purchase_orders_cleansed.delta/_delta_log/.s3-optimization-1\", \"/retail-org/solutions/silver/purchase_orders_cleansed.delta/_delta_log/.s3-optimization-2\", \"/retail-org/solutions/silver/purchase_orders_cleansed.delta/_delta_log/00000000000000000000.crc\", \"/retail-org/solutions/silver/purchase_orders_cleansed.delta/_delta_log/00000000000000000000.json\", \"/retail-org/solutions/silver/purchase_orders_cleansed.delta/part-00000-5110efaf-a2b7-41fe-a1c0-63f8e90ba478-c000.snappy.parquet\", \"/retail-org/solutions/silver/purchase_orders_cleansed/\", \"/retail-org/solutions/silver/purchase_orders_cleansed/_delta_log/\", \"/retail-org/solutions/silver/purchase_orders_cleansed/_delta_log/.s3-optimization-0\", \"/retail-org/solutions/silver/purchase_orders_cleansed/_delta_log/.s3-optimization-1\", \"/retail-org/solutions/silver/purchase_orders_cleansed/_delta_log/.s3-optimization-2\", \"/retail-org/solutions/silver/purchase_orders_cleansed/_delta_log/00000000000000000000.crc\", \"/retail-org/solutions/silver/purchase_orders_cleansed/_delta_log/00000000000000000000.json\", \"/retail-org/solutions/silver/purchase_orders_cleansed/part-00000-06d5cc45-8388-41cd-b6a9-173c0a4bd498-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/\", \"/retail-org/solutions/silver/sales_orders/_delta_log/\", \"/retail-org/solutions/silver/sales_orders/_delta_log/.s3-optimization-0\", \"/retail-org/solutions/silver/sales_orders/_delta_log/.s3-optimization-1\", \"/retail-org/solutions/silver/sales_orders/_delta_log/.s3-optimization-2\", \"/retail-org/solutions/silver/sales_orders/_delta_log/00000000000000000000.crc\", \"/retail-org/solutions/silver/sales_orders/_delta_log/00000000000000000000.json\", \"/retail-org/solutions/silver/sales_orders/part-00000-596a0749-4657-4616-aaea-8a8a19de9fcd-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00001-529c1abd-0305-4172-b10c-9061ed5a71f8-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00002-ae45432b-d996-4539-a145-eff1d3ad9198-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00003-dda2dd4a-b133-42de-853a-87e8ab900ccc-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00004-1d981219-2a07-43d9-b3fa-bad542e4e38b-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00005-388e576e-5602-4140-9088-91a49ce9f01a-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00006-a1b22ccf-f3eb-42d9-a129-6501247b1af6-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00007-1a080f20-7116-414e-8299-80964795b127-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00008-5f064136-28c0-456c-8edc-4deb65a963bb-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00009-ac67b427-8320-4e5c-9657-0d27e3fb62f4-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00010-82a1b215-3279-4a21-9efc-13f470cbbdf7-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00011-3816d202-0c28-441a-83cc-501c2b9f6565-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00012-08ea3589-41f6-43b3-9e80-47fbe9b4bee8-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00013-942dab8c-12f2-43c2-8ef0-d081afe5a7d0-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00014-13189bef-0c8d-46f5-8110-ebac7b216f20-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00015-9ecc5e4b-99c7-409c-9e47-bd413e3567c0-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00016-a5ea7ae7-57e7-445e-8cd2-ad2e6dccbafd-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00017-042702b2-1b69-4afc-aeec-08ed39167975-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00018-4106d1db-d457-4ab8-a40a-a31231ccc6d3-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00019-c6bea7fa-1885-4ae0-86fe-b1447f9d4342-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00020-abfab4ef-4d41-47cc-b3cc-5013c32fdc0b-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00021-c0fce417-7e09-407c-ba48-3d9684e3b445-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00022-b5a45e49-b1d9-4ac5-8ef9-a86edf1c80a3-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00023-4a7b4460-593f-40d1-b56d-98f4d5cb216f-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00024-0b0fef9b-298f-49b6-8df6-1587904e9090-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00025-05522774-6a67-4800-8305-f3c2901e1f94-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00026-cf76dc2e-4a9b-4d21-9885-ddbe78de5011-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00027-3b9765e0-5439-4cfd-9378-cc15581080a2-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00028-f51230d4-a48d-4067-a8e1-c6408791c6d8-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00029-dccfacd2-9434-4791-8dbc-115189796bcf-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00030-d3ceae78-5963-4adf-a806-2a41a6c49de3-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00031-0300b5b3-6cd0-4c6a-85d4-41f6d1bec4d0-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00032-c85553e1-c05e-408b-b150-4b8231c5de67-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00033-d24100b2-09ae-42dc-b3ee-f26bd2f3f297-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00034-b38a0aec-dccf-4856-8703-5f814566d38c-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00035-7de5e9a2-e0f3-4941-9066-9ae9667851d3-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00036-376b199f-f275-42a9-9589-804a33232969-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00037-a689bab9-9894-45e8-bcd3-8ff57e2be0b2-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00038-aaac4e61-428d-45f9-96f1-5b82a4f44266-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00039-9a769317-15a7-4e17-a331-13dccff84222-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00040-ce2829e0-8016-44e3-97e7-026936bcf9af-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00041-3462a3a4-00d0-4064-ad61-93861a58be04-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00042-52a9b435-9e87-4805-abd5-b91be3e89504-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00043-892b294d-d45d-47f6-8854-63ea0c62c27c-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00044-681426d4-8de3-4713-9a6d-754d67a1454c-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00045-2c815267-7072-421b-9548-3fb4f7469988-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00046-5734907c-5f63-476a-b641-b12157ab89ee-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00047-d84eb521-05c8-4f5f-a33c-2e17d4abb864-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00048-f54b7228-1a40-4b57-8c03-540a87d3929a-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00049-1d56903f-3781-4ba4-b5d4-32358ff3d6b3-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00050-1dd01a83-c650-4d72-a152-86067175c816-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00051-fec877c5-de96-4be5-9417-953a654791a2-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00052-acce140a-255e-472a-92d9-1925fd338f24-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00053-8a18315e-f975-4212-be91-0cfb35fef8bd-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00054-e7887fa8-0197-4a83-89d3-4563575da378-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00055-eab718a5-5693-41d7-8dd4-6a3d07ffaecf-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00056-7c7a6c9a-4708-4cec-ad63-c6df2ff148c4-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00057-4c99d5c3-5ee9-40a7-849d-83213d8399ee-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00058-5c6605df-f583-43a5-8786-f132a875a229-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00059-54793fd3-d383-47f3-9810-b3ccdb7d4e0b-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00060-bbd56a19-e540-45bd-b7c9-07c0afe25784-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00061-1e87e4ad-7b85-436b-94c1-bc5b57011176-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00062-2e268fe7-6a78-4b3c-8457-2c6091da1fe9-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00063-b3b0fcc0-2d6c-46ae-8746-a100ea9d6d30-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00064-ea2f9a18-f32d-474d-b6ac-81bb87b719cf-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00065-ceb35584-3ec5-4d74-b281-faeb637e115f-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00066-86ae0f9e-a104-43b6-8edc-a2b296fc30bf-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00067-cb482d49-9d02-4cc0-890b-5254885cca5d-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00068-8468940a-b064-4365-8d0e-180f7a65c74e-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00069-ca218a11-028e-4811-9e57-101afcc32730-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00070-0ebbf85e-1960-4b57-ac81-4164b04fc70d-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00071-96639b25-1685-485d-acf8-f7e486d85db2-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00072-5f39948f-7e67-4aac-8328-8ecdab1aef98-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00073-2ee6c3e8-938b-4d0d-9080-d9c165d7ff5c-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00074-ae490ddd-1500-4643-a5c6-db515ba5f3e7-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00075-4b30d47e-6c27-45c9-ab2a-7087a72c0bb8-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00076-f052fbdd-2ff8-442e-9fc8-225a7bc0ff93-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00077-41944abb-f25f-4900-9ed6-e949428eed02-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00078-87f78167-71e8-44a2-9172-da294fe87b7a-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00079-96b7ba71-713c-4184-98ca-985fcc8bfdc9-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00080-dbe26f07-3d5a-42eb-b563-1698ccb39d52-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00081-267ae14f-3970-4f64-9fe0-c27ff6599808-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00082-940e4a16-e5d7-4fb8-b1e2-4fc21d17beb0-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00083-3984a1b4-46fb-40e4-bd24-3dee10fb7cdc-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00084-04ce11ae-e480-4e9d-95dd-ea2870001740-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00085-068796b6-4c07-43b2-8e3f-8f18cec96f2e-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00086-dac2fe00-c19a-4b4d-9088-f322c3b8f270-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00087-f8b391e4-4829-427a-8359-8ddd65bc517a-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00088-d4f705ec-fe04-4d5c-9638-772acf6dbe80-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00089-508bd7bb-4a98-4d8c-b07c-c2cfa22f6315-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00090-7c248042-6386-40d6-bd36-66b4fb61c484-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00091-0afb71f9-8bff-452c-9b35-cd1f2879ffc9-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00092-666e5106-1630-44af-9e69-f4136b6e9a08-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00093-1fc02b05-9dbf-416e-b6ed-193e68542425-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00094-ed71aef6-0755-459f-9362-fd394ad71f16-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00095-bc225cae-d496-418c-b60b-d56b4a94a9ff-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00096-944e0a29-44b7-4cfb-b3f1-1a2ef80c6756-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00097-6a0ccd8a-d191-4219-a0de-8ccc8ea92c72-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00098-260ecea8-e9c1-491c-994e-33e3f18666dc-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00099-46f979f6-02f9-4348-bcea-032837803e77-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00100-58869468-a9b9-4e4c-bdf2-7fb542e59fed-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00101-de84a8b4-13a8-4c26-8188-0ddadc8ce355-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00102-90b670cf-5f92-4da9-bd48-f4b2de1fce37-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00103-c698c39d-445e-48ff-8e8d-5999d3b5a7e7-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00104-2dd233a5-59c0-4a3a-99d5-dadbadad0bba-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00105-1011cfb1-d679-4648-905d-6656ab3ae93e-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00106-98710a22-27fe-4bfe-abf0-8e1d516ae8f5-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00107-df054751-939d-4720-bbd1-b81183fa6f6d-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00108-a5adbf6f-ae0f-4c9c-b947-1b1fc3f79b41-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00109-0ecc4304-a91d-4a5d-9049-d0e25f1f97b2-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00110-62cad0fa-b4c4-43a5-96b8-bc0bda9d4719-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00111-ef3602e1-6985-41bc-aa13-d9fbd9c23471-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00112-313b86d3-69b5-4ff2-a458-960a44e99828-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00113-659d37fb-4fb1-4c54-9f28-0f5827d7c49b-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00114-61f90bcb-a2ba-4e4b-bc95-f1d15b783bda-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00115-5ff9c2a6-801d-4169-9e1f-c79840904ba7-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00116-36801dc4-a520-436b-8a7f-320d5633c6df-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00117-9dc9275e-862d-4754-b50c-37e7b49b5567-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00118-ea3d7e09-4610-424a-834d-93ca82dc0826-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00119-9990d1b9-eba6-46a1-9af6-df910c3e125e-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00120-d8abfe41-4c29-4a2d-b48f-a5ff8c290450-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00121-29d7d30c-2f05-4f25-905b-3b55431b729f-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00122-1849d995-be86-4463-8e56-34812f53507a-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00123-f7d742d4-3aba-468d-980f-94b91bf78758-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00124-42c7bd70-4ac0-4c9c-be16-f71c9619e704-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00125-c5830a70-9813-4bee-816f-e9215b9e3012-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00126-a6160da4-1fe9-4db9-b993-40d4c6fb05c6-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00127-97d5ae11-ae7f-4b3c-aa51-665ec4117f6e-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00128-ab7db55a-4ac1-456d-82bf-d68c4b90ed90-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00129-0794bbc5-1133-4724-8fba-e354a39a76db-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00130-ff24c572-85f4-4088-b9de-f5ac8731cd38-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00131-f18d7633-69f1-41d1-8887-6b9cd2928cf0-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00132-572dc12f-fc6c-4fbc-b6c9-c0ee429a5e8e-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00133-8580548a-88e0-440e-bc7a-7a0cfc0f3b13-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00134-6bcd86d3-279b-4b81-921d-a616e6fcd061-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00135-4136b1ab-8089-480d-b109-1b1c97a78321-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00136-aeadac01-de0b-4025-b674-72fc42788c76-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00137-2c66d31f-d394-4838-86ed-c8f39e6deb39-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00138-b6ffa75e-a708-40e3-b8b0-a85441b4d067-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00139-a46a0fab-bbfe-49e3-af76-fd262873d83e-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00140-22597dae-37b7-4c66-adf1-b96b0eb17c66-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00141-058c330d-f3b8-438b-aaeb-44cca9526699-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00142-e7dd5d94-7f89-4358-b3cc-17ce6cc37d46-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00143-465752aa-483c-48be-b0e7-e08642524250-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00144-b4e368e2-dba6-4c29-89e6-ab82d0b2e9a0-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00145-55f4c4b0-2e94-4629-b3e1-dea7e2d6d729-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00146-fd1bb570-9e85-4752-97fc-3653d0ab9ded-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00147-f9092020-2f1a-4be6-859d-570b7f9158d6-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00148-a6c9b928-bce8-4d77-994d-c0c50cdd0a29-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00149-54bb1210-878c-4f7d-b1b7-3acb0c887af8-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00150-362094b0-3009-4e73-b145-2ef52994fd04-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00151-266aa304-0e33-4ecb-8669-29a9942bc0f9-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00152-857dee25-ac65-4b8a-93cb-eedeaed359b1-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00153-cad54416-3065-4b3f-9efe-a131f0f8fe23-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00154-0865cc09-0355-4a4a-90e6-5687459b5ed5-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00155-65ee50a7-7287-4c1b-87e0-8a0126bd100b-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00156-737c6154-2d17-4abf-80f3-e1ddb3f9fe9a-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00157-35432185-0701-4a92-a27e-39c92ef100b5-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00158-ec4d00ef-925b-4d8c-8677-22fa6cea688d-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00159-d71a2b25-f22a-4c29-b2d5-426701643a9d-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00160-defcae83-3520-4537-8c9e-c5dd80c872dc-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00161-1a7687fb-1e5a-4eaa-ae63-b8c122fa77f6-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00162-84dd7735-a137-4592-bbae-fb94793de695-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00163-edbf5892-cc69-4ad4-8d7c-7d2f0dc86bdd-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00164-aa72504d-dc80-43e2-910f-cdb7df717300-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00165-b8587eb3-9ab3-4b7d-8a56-8e49b1979bd8-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00166-bf069bfc-4b5f-4603-9b35-dbfe50f229f8-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00167-bf99945f-93c1-4ae5-901f-3a1e8f7ff972-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00168-27e0c8a5-84d0-4f16-acd4-98fdbb9b7d21-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00169-c0c52cca-4625-4e78-b658-f4d36842bb56-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00170-4161a58a-43bc-446a-95af-188e922d4ce7-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00171-ff98c8a7-499f-44f9-9237-510c558214aa-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00172-2d3ca012-3d7c-4077-84cc-f3173d42dad3-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00173-b5dd3a11-87dc-4f87-909b-85fb5101a8fe-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00174-7fb1d5d2-242b-4b53-8a6a-30e03ef1aad7-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00175-0211bf74-60ea-4355-b20c-e20e760a2573-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00176-3488dd3e-86d5-408d-a920-3f3149b3120c-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00177-b1729cd1-f15b-43da-a723-18c81218a3b0-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00178-d9a36e58-29f0-42ee-ba2f-1583223ba968-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00179-c45112a5-8628-434d-a2b8-57609d2fdb03-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00180-4a2b5785-e3ac-4216-af4c-646af5cd1c53-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00181-7e5c8f4e-7cfa-41e0-89ed-66553e22da2c-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00182-e9532c6e-7279-4285-8629-f88b1bfc07f8-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00183-8b2186e5-5886-4db6-bec0-37431c0b9954-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00184-73d5bbd9-e79d-4bf1-bc01-d5cf449079ea-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00185-c51ec718-06f9-41d9-8039-25895f6f71e8-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00186-ad248c22-d5f9-4af7-89ed-baffab0b2996-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00187-c712d96c-f84f-49b9-8281-fe7351682b11-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00188-367db1ff-4935-475d-bf49-259c14d95fb0-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00189-1e8aef86-aba2-469a-af8b-32c9ad65adbe-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00190-6f7a252c-4791-4e65-9dd7-19cfaddfaf54-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00191-6285a9e0-f32d-4835-8a99-fd19c4a614db-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00192-f1ce3d1d-19b3-4c11-87bf-2b07c08ae579-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00193-170caa6e-f26b-4a37-a29d-1714e3561343-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00194-3099336f-fcaa-46d0-9f16-2ae5edc0316d-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00195-63b870f7-96c2-4371-9dca-e7bc5d978777-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00196-4005bb53-95ce-4942-b2d0-51195d17d79b-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00197-6dd08883-e343-4d7f-a615-67d5ec5452d0-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00198-d8ddcdad-8f7d-47c2-9539-69eea426f391-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders/part-00199-b9022048-ddcd-4b5e-84af-615af135656c-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/\", \"/retail-org/solutions/silver/sales_orders_3/_delta_log/\", \"/retail-org/solutions/silver/sales_orders_3/_delta_log/.s3-optimization-0\", \"/retail-org/solutions/silver/sales_orders_3/_delta_log/.s3-optimization-1\", \"/retail-org/solutions/silver/sales_orders_3/_delta_log/.s3-optimization-2\", \"/retail-org/solutions/silver/sales_orders_3/_delta_log/00000000000000000000.crc\", \"/retail-org/solutions/silver/sales_orders_3/_delta_log/00000000000000000000.json\", \"/retail-org/solutions/silver/sales_orders_3/part-00000-c3324580-732a-492f-9391-006e87f546c2-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00001-1a8afdaa-ad03-4746-a5e9-ebb59da58727-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00002-558e343e-b6da-45b4-9bb2-86d6b2dfd77d-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00003-2ea6e2d4-0c32-41f9-965b-bfb3ee1e32d7-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00004-d338a476-9a52-41f7-b62a-de2a7f99ea24-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00005-54b82d2f-6f77-4bb5-8799-8d809c9592e7-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00006-2028d56d-3272-493a-992f-334dfcd25f7a-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00007-261a8910-aa31-42e3-a3c0-830e2a854404-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00008-e03ba0f9-1a2d-4f1d-953f-21d68da8153b-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00009-979a4d2f-4ff7-4fcc-8eed-8207c66c52e6-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00010-8ff9b50c-b6be-4918-be7d-fb3b6ed05e29-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00011-aacd2b26-8a48-4323-9088-8bcd9f53934d-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00012-afb698d3-8579-4bd2-965c-773ed573994a-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00013-966d155c-694e-4f1d-a5b1-da752fcdf40b-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00014-bc276f64-0e56-4776-8aa0-a87a233c79aa-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00015-88f423df-d7a0-4081-9e47-880a62360b28-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00016-ce78e3cb-c162-47a8-994c-b2ab47604b9c-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00017-806345fc-c234-4cdd-aba7-bcd9cd81d495-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00018-b1d455a6-c154-4fbe-ba8d-7b6bf5fa912c-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00019-73e9894c-c0aa-4f3d-b7fb-7c71077713f4-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00020-865ceae5-5f37-4ecf-bc75-ba86a8f7bebb-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00021-16b5c15b-b8b4-4478-bf57-136f61a9e2e9-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00022-266499df-871e-4975-8c14-2feb281ffff8-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00023-9077dce0-c14e-40f3-8139-e6c2b7c3e799-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00024-ac4bf056-3a2c-4d23-84c8-fb49ffbe8dbd-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00025-db11f5f0-4709-4aaf-adb4-810fa60e40f6-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00026-125ee2c7-53b8-4343-8f35-954aef604c8f-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00027-0de99332-81e6-438a-93b9-1abc1adefdc8-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00028-9984af8a-5236-4f4c-9936-3f5011215b88-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00029-13921bee-684e-4d37-a90a-12e2a96e000d-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00030-34da4dd0-a501-413b-a09d-4be59c1066f1-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00031-78431ede-f942-4db2-a828-61521f82255e-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00032-029d36dd-69eb-4c33-9ea8-a63e8be27ae0-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00033-bca5a922-109f-49c0-9318-b6ff73e6497a-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00034-167f4e8e-5f79-4afe-a134-f30c1ad3acbf-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00035-8b5924a1-70e2-4f57-888c-caf3110706c2-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00036-c91351f6-4387-438c-a515-0403e181b074-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00037-51619262-b65b-4cd1-b5cd-d2420ccf2860-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00038-d1b1f3c2-12a7-4812-b120-86ec509947a1-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00039-51322af7-d41c-490a-9d7b-3a7dcf091661-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00040-8056ec66-df98-472f-94da-dc2f98a5bce6-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00041-b0249419-1ed8-42bf-b194-d403cb64605e-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00042-a833840b-ab47-48ef-8ca4-9154a2adcb87-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00043-08da2b23-b44e-4eb5-9d6e-34c53bfa37ac-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00044-47129183-8feb-4cfc-8922-790878d19954-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00045-5702274f-6103-4724-bbf0-93a9fd3eea11-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00046-aa6b921c-e75b-4e38-ab7d-5c2d797032cc-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00047-73ba72c4-1e96-4e6b-8f54-569d6cad8050-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00048-47256b6d-257c-4a2f-ac33-dad001315dfa-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00049-2fb2d8e8-45a2-4cfb-b7d1-a5d35d6adede-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00050-0098e0e7-3ebb-4a52-a72d-2795a1235806-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00051-7846548d-8537-4f51-8dce-e981a4e090ec-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00052-9ee3934a-3d01-4422-85d7-9895fa9a4b89-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00053-95bf9b60-ae0c-4425-b034-fc4550fdb1a0-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00054-91deee9a-bc9f-4d7c-8192-117dbdad3c9f-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00055-9d9345b7-6b74-4d31-b2d3-acc58b0637a3-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00056-315b3632-b162-4d97-b462-facbf5a2946b-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00057-6ed358a4-ccdd-484e-b0ac-dfd03e90a402-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00058-3b73533e-e781-4913-a481-ab09fed46714-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00059-67dd375d-36c8-4362-bd5e-05fa5b825934-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00060-9c25ea09-89a0-4fd1-8715-871afa62dbf3-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00061-bd65e15a-4005-481b-8779-be19d6b72c09-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00062-f39e3f8a-4dfe-49b2-a299-e00dcc3da911-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00063-d85bb470-7e26-459c-a2ab-bcaa9d25a88e-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00064-7044c512-4019-4f3b-a60d-414b6511e81d-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00065-bf34dcf5-f614-48e3-8787-e157de1a14f2-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00066-bb7b9dc7-cc2e-412d-8538-47d1faa23508-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00067-dabe04c4-cbdb-4f4b-9f93-5d9f32207b3c-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00068-03573d12-8081-4e1d-970a-0c66df2bf905-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00069-d49fad70-eb03-4684-893a-6d3bd14453f5-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00070-6e0ecfc1-9c2b-4585-b7b0-5b12f63e9fce-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00071-4d967064-fc9b-4c97-9c77-0ccc737a02b5-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00072-27bc2843-8388-4933-b5ab-2f8ad06e8a2f-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00073-144b9c6c-7e3b-485c-a27e-ca6936c1b19e-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00074-f4cf83cd-1593-49bc-9dd5-a50037980118-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00075-2d4806a1-e8a9-46ff-9e90-8fa7eb309faf-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00076-159eb87d-8907-47cb-9b59-e1eb635300a5-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00077-1c22c4b5-8a04-49b3-b4e7-cbbddbf7f13f-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00078-4892189d-a4bf-4017-98e2-6a20a61c8a84-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00079-8f9e214f-7402-4f23-a3bb-5b2343540670-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00080-92caeaf2-561b-466d-873f-b3b21ef94a63-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00081-71753f7a-758d-489f-b17e-b0376639dceb-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00082-067db036-e1a9-40e5-b02a-42d0ea7e347e-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00083-e243ce16-a7b2-45be-a516-5da45dcbe2ff-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00084-959b74b5-a45a-4f81-b580-f39debc23687-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00085-9e02e94d-e8c2-4109-baed-b6513381053d-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00086-a28917f0-80d4-4856-8cc1-f419473def34-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00087-e8ab8ffd-5147-42c2-9f29-3f70996db7ec-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00088-b1322fba-0506-45d8-9d02-a8e288775a6f-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00089-f57b06fc-6c97-4686-b3fe-eec2adc7afac-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00090-82690dc7-8220-47ca-a2d7-5178144f2252-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00091-350d90de-9cd8-4e3f-a360-a547538c7255-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00092-e9c079e0-d979-4d7f-ada2-a9b180f08665-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00093-49d627b7-dc0d-4a06-9e47-6c016d233b99-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00094-ac0ebfda-e138-4ac1-aa1b-aa1676af1e3a-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00095-6c777694-59f0-4242-bdd2-8cc3fd48bdbd-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00096-30788be9-3b27-41d3-899c-c40c56e0e050-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00097-1b2ee33c-2dde-4918-9c3f-6acc33f17f57-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00098-a6fbd06d-b52a-49e4-bc9c-0a3de2f3efd9-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00099-28d73d67-62a3-43fd-ad49-6b3d3ae73a5e-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00100-d55bb6fc-628b-412e-9529-7adfd5e70ccb-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00101-175ab1c3-52ad-4897-a79d-bf8b7c0f3c0e-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00102-e9116588-6655-4831-a15d-f76feae01cb1-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00103-1ddc595e-69a1-47a3-a563-7652a7befb32-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00104-c15af9e1-59e9-4795-a054-38ddbce43bc4-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00105-070a4c09-babd-4153-aaa3-f4341796deba-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00106-044fb464-7d2d-490f-9919-89e69fc10b6c-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00107-b437f3d6-9ae3-4574-b197-0b525ca114fe-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00108-42f4e69e-34e1-43d8-a8ad-3ed91ec5ad3c-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00109-4abed381-e950-4dd1-9cd5-ad78176738d2-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00110-757fdcc3-3dd9-401c-9546-59a53084f7d6-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00111-c5579631-f52a-4170-9a7d-9000bec8e727-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00112-465ed257-819d-4362-83be-aa96644c5287-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00113-f3e1235c-8441-40ee-82af-6025d0839ca2-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00114-37313691-9fb3-40c2-848d-a5655f82f831-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00115-9cc5d976-b431-4dc5-b212-0c518e6bb9be-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00116-f3afca5a-fb38-43be-9dfc-6983ba255e55-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00117-426ac214-f2b8-40b7-ab33-3cd4f97d987d-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00118-5838772a-737b-404f-be42-ce21ff4139b9-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00119-9a285b3b-bcfa-409a-b79f-b731f15b53b1-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00120-6ae75ecf-de21-4908-857d-50f87750a8c8-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00121-983c4199-e747-42cf-8189-85bb450a2954-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00122-2aa25dd7-0f4e-492b-a6a1-aae063151103-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00123-9d6d8d5e-4ef5-40a1-af76-40dac08c5871-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00124-cdea225f-81de-4472-a23d-1fe22d53485d-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00125-ccf17e70-612f-427a-ba18-c80b4c449ba2-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00126-ef47cc73-b46d-4927-82ce-077efb7ddff7-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00127-6d30bab4-2047-43ce-a4f8-8323a42f11f8-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00128-f2c31b04-e102-4d07-9c43-4de9d77acca2-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00129-0f5766b2-7ef8-44ca-acdd-c6deb7af3b87-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00130-97f97b86-6a5e-48fc-bdf1-be19625252f5-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00131-30d19a40-41a9-4566-a1a6-858a03cdfae4-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00132-ffa1bfe6-6d4f-4d6c-a3b0-b6549cfc68ad-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00133-4853417c-9f7b-4231-aa7a-79d246768198-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00134-a22432ba-59ce-4c24-a40e-bfdd2bf988fa-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00135-aa5bed65-c3ff-4e2d-8663-2bd54965c56e-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00136-1069dfc5-7190-4ec1-bbe8-3a87506583fc-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00137-55bcafbe-7327-44de-a207-5093d6a40780-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00138-b50badfa-a775-4924-b33e-9e8be1059402-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00139-9e4be8da-2cae-49e5-810e-633d8af61063-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00140-dd782fcf-ace8-4b0a-8171-3b106b91bbb7-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00141-21808489-cba7-4568-adad-06ebe7c18164-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00142-577dff09-ff1d-4e3b-9481-64f6a9bc1c8a-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00143-f41e281d-dc61-4192-82ec-eeeff7778cb3-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00144-19835963-f8ed-43d3-a661-f83c0a3ba760-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00145-1515da5a-8e7c-4335-b658-d2bea3e712dc-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00146-2ff33ed7-c826-4d49-bb64-7e305d529551-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00147-c0cb2041-5d1d-4d03-a1db-8ffad9cb8f60-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00148-c1e4d7e4-c38b-4c2a-b167-500671de0306-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00149-a620ea2f-d2b0-4f29-ae52-ba29c1e29a54-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00150-830da1f4-cd7a-4907-8295-6116e9a14301-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00151-97e976fa-b92a-460f-bdd6-6b5f9500e0a0-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00152-8eaaae90-3a8e-4ced-8ad7-a39a535b9879-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00153-21505690-c565-415d-b7a7-19500cf3365a-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00154-d8214a2c-e6d7-4b74-9528-1b27cecda676-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00155-b66c95da-4c99-4198-a868-edf30ca34d0f-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00156-a51999a7-fafc-4499-ad63-7957660ea286-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00157-7347da92-f343-4988-8ab5-3397fb53cd54-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00158-6e0c30ae-e90e-4e4d-abb1-7a78c28a0104-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00159-6ca94da9-8618-4911-a8b0-bb599cae0caf-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00160-7ee577fa-791f-48ac-a565-8fb34f077c4f-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00161-8d4b3423-bb9c-4fc4-a025-86fdc976faac-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00162-0b3c273d-8b3b-4472-8303-d631ed222172-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00163-94a34f35-31e7-4acc-b576-c81587b91fa7-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00164-c8680a10-c48e-44d6-a84f-22170fc0484f-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00165-71ad62c6-7ada-40af-83a8-cb583ba2115c-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00166-3b6602cd-65a1-47f7-a432-2308fcbd9ad9-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00167-7ae529fa-a4de-44d0-8c51-f281aee76b97-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00168-808164ac-4310-43cd-93b8-575f14a765c6-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00169-8939d87b-83ef-488f-91b9-311b7d54d57f-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00170-d6af9db5-6569-4d69-94c4-78f3c9c2ce0c-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00171-59226ce0-a751-4f6c-8640-aecc16ff6ef8-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00172-3cdd868b-f763-44e4-a1a2-38d38ad7a5aa-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00173-836d7e57-8c3a-49b0-a47f-3df51d4293e0-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00174-87087099-40d2-4e80-88c3-2754782c0fde-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00175-b9b35665-8384-4053-8f6e-599725a8b461-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00176-9997d62a-459b-44c8-8408-621ae9973427-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00177-c5c07f43-6d4e-4042-aa1a-93c1b77973e2-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00178-bca28289-ad8b-4bc0-b128-6f7381e4cc53-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00179-9279d2f8-b141-4422-9cab-bfe8c1d0f82b-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00180-98be1e52-b5cf-43f2-8cc8-38c2f2529fa4-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00181-467814eb-b8ca-479b-8995-173fec602e6d-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00182-133f9531-10c7-4d38-b756-72310b7d3dc4-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00183-a8d866e7-5eaa-488a-bc4d-edcabe46c474-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00184-e55cd1d3-7ccf-4a88-984c-e7044446c4a1-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00185-d8e043e4-8fe4-46f9-9606-89bec84618d2-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00186-eeca21f1-265e-4cb3-aa4e-78811c8024c8-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00187-1d38dd7f-b718-4c37-a91d-6beb9ea600bf-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00188-5fdb4a83-7120-45a4-91eb-4e2b47befd72-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00189-ca4aada7-cd00-4773-a1aa-9c0172ca5588-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00190-41067a53-c35f-4550-9550-d56d8009b5cd-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00191-63f4ad6a-2d4c-4f53-a995-fdc7de0de5e5-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00192-2aeda72e-dbde-44d8-ab47-5d67bcf79560-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00193-789cfe99-c6f3-415f-afed-57856f8d6e39-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00194-477d6bc3-72b6-44eb-ae6b-751a759e7461-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00195-9909da6d-0f63-44ef-b58c-b690aff52c30-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00196-90b82c9d-0364-44ed-b147-5d80662b5a61-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00197-56d0e7a6-77d2-4e41-8d34-5cc0f4e5c798-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00198-b7dec7f0-d792-43e6-8843-7cb384d072e5-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_3/part-00199-402c5953-ab35-4f38-a6c2-6a6166769968-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_verified/\", \"/retail-org/solutions/silver/sales_orders_verified/_delta_log/\", \"/retail-org/solutions/silver/sales_orders_verified/_delta_log/.s3-optimization-0\", \"/retail-org/solutions/silver/sales_orders_verified/_delta_log/.s3-optimization-1\", \"/retail-org/solutions/silver/sales_orders_verified/_delta_log/.s3-optimization-2\", \"/retail-org/solutions/silver/sales_orders_verified/_delta_log/00000000000000000000.crc\", \"/retail-org/solutions/silver/sales_orders_verified/_delta_log/00000000000000000000.json\", \"/retail-org/solutions/silver/sales_orders_verified/part-00000-102d5b73-9be0-4412-9544-a80ca2b6d189-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_verified/part-00001-00d98295-9ec6-4852-87a8-21cdde66740e-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_verified/part-00002-47405cba-2139-49da-bd12-d4b34ff2f258-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_verified/part-00003-63d3c002-8297-4e32-abda-53aaa9e32e83-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_verified/part-00004-9ad89585-58c9-4d3d-adb3-942ed50d0094-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_verified/part-00005-ba292f18-42d8-4a2c-b2d6-24b7c7ffe0c7-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_verified/part-00006-924cce28-746d-418d-a593-c8d2df6eedf2-c000.snappy.parquet\", \"/retail-org/solutions/silver/sales_orders_verified/part-00007-0a1671cf-5b74-467d-8bca-7860819a5376-c000.snappy.parquet\", \"/retail-org/solutions/silver/suppliers/\", \"/retail-org/solutions/silver/suppliers/_delta_log/\", \"/retail-org/solutions/silver/suppliers/_delta_log/.s3-optimization-0\", \"/retail-org/solutions/silver/suppliers/_delta_log/.s3-optimization-1\", \"/retail-org/solutions/silver/suppliers/_delta_log/.s3-optimization-2\", \"/retail-org/solutions/silver/suppliers/_delta_log/00000000000000000000.crc\", \"/retail-org/solutions/silver/suppliers/_delta_log/00000000000000000000.json\", \"/retail-org/solutions/silver/suppliers/part-00000-7502d2de-29a5-4960-b7c9-ac26fcfa20e8-c000.snappy.parquet\", \"/retail-org/suppliers/\", \"/retail-org/suppliers/suppliers.csv\", \"/weather/\", \"/weather/README.md\", \"/weather/StationData-parquet/\", \"/weather/StationData-parquet/_SUCCESS\", \"/weather/StationData-parquet/_committed_6522815070831625438\", \"/weather/StationData-parquet/_started_6522815070831625438\", \"/weather/StationData-parquet/part-00000-tid-6522815070831625438-4358ade9-a419-4a61-a2ea-9f64c9c3c0fc-3504-c000.snappy.parquet\", \"/weather/StationData-parquet/part-00001-tid-6522815070831625438-4358ade9-a419-4a61-a2ea-9f64c9c3c0fc-3505-c000.snappy.parquet\", \"/weather/StationData-parquet/part-00002-tid-6522815070831625438-4358ade9-a419-4a61-a2ea-9f64c9c3c0fc-3506-c000.snappy.parquet\", \"/weather/StationData-parquet/part-00003-tid-6522815070831625438-4358ade9-a419-4a61-a2ea-9f64c9c3c0fc-3507-c000.snappy.parquet\"]\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/_utility-methods')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "14728bdf-a374-4142-9a86-b9bfff13f904"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%pip install \\\n",
							"git+https://github.com/databricks-academy/dbacademy-gems@e8183eed9481624f25b34436810cf6666b4438c0 \\\n",
							"git+https://github.com/databricks-academy/dbacademy-rest@bc48bdb21810c4fd69d27154bfff2076cc4d02cc \\\n",
							"git+https://github.com/databricks-academy/dbacademy-helper@e0e819d661e6bf972c6fb1872c1ae1a7d2a74b23 \\\n",
							"--quiet --disable-pip-version-check"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"%run ./_remote_files"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"import time\n",
							"from dbacademy_gems import dbgems\n",
							"from dbacademy_helper import DBAcademyHelper, Paths\n",
							"\n",
							"helper_arguments = {\n",
							"    \"course_code\" : \"dewd\",            # The abreviated version of the course\n",
							"    \"course_name\" : \"data-engineering-with-databricks\",      # The full name of the course, hyphenated\n",
							"    \"data_source_name\" : \"data-engineering-with-databricks\", # Should be the same as the course\n",
							"    \"data_source_version\" : \"v02\",     # New courses would start with 01\n",
							"    \"enable_streaming_support\": True,  # This couse uses stream and thus needs checkpoint directories\n",
							"    \"install_min_time\" : \"5 min\",      # The minimum amount of time to install the datasets (e.g. from Oregon)\n",
							"    \"install_max_time\" : \"15 min\",     # The maximum amount of time to install the datasets (e.g. from India)\n",
							"    \"remote_files\": remote_files,      # The enumerated list of files in the datasets\n",
							"}\n",
							"# Start a timer so we can \n",
							"# benchmark execution duration.\n",
							"setup_start = int(time.time())"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def update_cluster_params(self, params: dict, task_indexes: list):\n",
							"\n",
							"    if not self.is_smoke_test():\n",
							"        return params\n",
							"    \n",
							"    for task_index in task_indexes:\n",
							"        # Need to modify the parameters to run run as a smoke-test.\n",
							"        task = params.get(\"tasks\")[task_index]\n",
							"        del task[\"existing_cluster_id\"]\n",
							"\n",
							"        cluster_params =         {\n",
							"            \"num_workers\": \"0\",\n",
							"            \"spark_version\": self.client.clusters().get_current_spark_version(),\n",
							"            \"spark_conf\": {\n",
							"              \"spark.master\": \"local[*]\"\n",
							"            },\n",
							"        }\n",
							"\n",
							"        instance_pool_id = self.client.clusters().get_current_instance_pool_id()\n",
							"        if instance_pool_id is not None: cluster_params[\"instance_pool_id\"] = self.client.clusters().get_current_instance_pool_id()\n",
							"        else:                            cluster_params[\"node_type_id\"] = self.client.clusters().get_current_node_type_id()\n",
							"\n",
							"        task[\"new_cluster\"] = cluster_params\n",
							"        \n",
							"    return params\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def clone_source_table(self, table_name, source_path, source_name=None):\n",
							"    import time\n",
							"    start = int(time.time())\n",
							"\n",
							"    source_name = table_name if source_name is None else source_name\n",
							"    print(f\"Cloning the {table_name} table from {source_path}/{source_name}\", end=\"...\")\n",
							"    \n",
							"    spark.sql(f\"\"\"\n",
							"        CREATE OR REPLACE TABLE {table_name}\n",
							"        SHALLOW CLONE delta.`{source_path}/{source_name}`\n",
							"        \"\"\")\n",
							"\n",
							"    total = spark.read.table(table_name).count()\n",
							"    print(f\"({int(time.time())-start} seconds / {total:,} records)\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"class DltDataFactory:\n",
							"    def __init__(self, stream_path):\n",
							"        self.stream_path = stream_path\n",
							"        self.source = f\"{DA.paths.datasets}/healthcare/tracker/streaming\"\n",
							"        try:\n",
							"            self.curr_mo = 1 + int(max([x[1].split(\".\")[0] for x in dbutils.fs.ls(self.stream_path)]))\n",
							"        except:\n",
							"            self.curr_mo = 1\n",
							"    \n",
							"    def load(self, continuous=False):\n",
							"        if self.curr_mo > 12:\n",
							"            print(\"Data source exhausted\\n\")\n",
							"        elif continuous == True:\n",
							"            while self.curr_mo <= 12:\n",
							"                curr_file = f\"{self.curr_mo:02}.json\"\n",
							"                target_dir = f\"{self.stream_path}/{curr_file}\"\n",
							"                print(f\"Loading the file {curr_file} to the {target_dir}\")\n",
							"                dbutils.fs.cp(f\"{self.source}/{curr_file}\", target_dir)\n",
							"                self.curr_mo += 1\n",
							"        else:\n",
							"            curr_file = f\"{str(self.curr_mo).zfill(2)}.json\"\n",
							"            target_dir = f\"{self.stream_path}/{curr_file}\"\n",
							"            print(f\"Loading the file {curr_file} to the {target_dir}\")\n",
							"\n",
							"            dbutils.fs.cp(f\"{self.source}/{curr_file}\", target_dir)\n",
							"            self.curr_mo += 1"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@DBAcademyHelper.monkey_patch\n",
							"def setup_completed(self):\n",
							"    print(f\"\\nSetup completed in {int(time.time())-setup_start} seconds\")\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/example-setup')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "01 - Synapse Workspace and Services/ExampleSetupFolder"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "LakehouseTest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "fb4ea673-12dd-4bbb-8b7c-8ae73532fda1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/8aa90780-bc26-461d-9db6-6df705873871/resourceGroups/domo-demo/providers/Microsoft.Synapse/workspaces/domodemo/bigDataPools/LakehouseTest",
						"name": "LakehouseTest",
						"type": "Spark",
						"endpoint": "https://domodemo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/LakehouseTest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run ../../Includes/_utility-methods"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DA = DBAcademyHelper(**helper_arguments) # Create the DA object\n",
							"DA.reset_environment()                   # Reset by removing databases and files from other lessons\n",
							"DA.init(install_datasets=True,           # Initialize, install and validate the datasets\n",
							"        create_db=False)                 # Continue initialization, create the user-db\n",
							"DA.conclude_setup()                      # Conclude setup by advertising environmental changes"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# TODO\n",
							"my_name = None"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"example_df = spark.range(16)\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/setup-entities')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Includes"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "623cef3e-7974-47c9-bf9d-4c5beffb559a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import sys, subprocess, os\n",
							"subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"git+https://github.com/databricks-academy/user-setup\"])\n",
							"\n",
							"from dbacademy import LessonConfig\n",
							"LessonConfig.configure(course_name=\"Databases Tables and Views on Databricks\", use_db=False)\n",
							"LessonConfig.install_datasets(silent=True)\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LakehouseTest')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.2",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westeurope"
		}
	]
}